<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>
<head>
	<title>OMR-Research</title>
	<link rel=stylesheet type="text/css" href="css/OMR-Research.css">
</head>
<body>

<section class="page-header">
	<h1>Bibliography on Optical Music Recognition</h1>
	<p>Last updated: 01.12.2024</p>
	<a href="https://github.com/OMR-Research/omr-research.github.io" class="btn">View on GitHub</a>		
	<table class="page-header-table" id="navigation-table">
		<tr>
			<td><a href="index.html" class="btn-light">Sorted by Year</a></td>					
			<td><a href="omr-research-compact.html" class="btn-light">Sortey by Year (Compact)</a></td>
			<td><a href="omr-research-sorted-by-key.html" class="btn-light">Sorted by Key</a>	</td>		
			<td><a href="omr-related-research.html" class="btn-light">Related research</a></td>
			<td><a href="omr-research-unverified.html" class="btn-light">Unverified research</a></td>
		</tr>
	</table>		
</section>

<!-- This document was automatically generated with bibtex2html 1.96
     (see http://www.lri.fr/~filliatr/bibtex2html/),
     with the following command:
     BibTeX2HTML/OSX_x86_64/bibtex2html -s omr-style --use-keys --no-keywords --nodoc -o OMR-Related OMR-Related-Research.bib  -->


<table>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Agarwal2008">Agarwal2008</a>]
</td>
<td class="bibtexitem">
Abhaya Agarwal and Alon Lavie.
 METEOR, M-BLEU and M-TER: Evaluation Metrics for
  High-correlation with Human Rankings of Machine Translation Output.
 In <em>Third Workshop on Statistical Machine Translation</em>, pages
  115-118, Stroudsburg, PA, USA, 2008. Association for Computational
  Linguistics.
 ISBN 978-1-932432-09-1.
[&nbsp;<a href="OMR-Related_bib.html#Agarwal2008">bib</a>&nbsp;| 
<a href="http://dl.acm.org/citation.cfm?id=1626394.1626406">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Akiyama1990">Akiyama1990</a>]
</td>
<td class="bibtexitem">
Teruo Akiyama and Norihiro Hagita.
 Automated entry system for printed documents.
 <em>Pattern Recognition</em>, 23 (11): 1141-1154, 1990.
 ISSN 0031-3203.
[&nbsp;<a href="OMR-Related_bib.html#Akiyama1990">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/0031-3203(90)90112-X">DOI</a>&nbsp;| 
<a href="http://www.sciencedirect.com/science/article/pii/003132039090112X">http</a>&nbsp;]
<blockquote><font size="-1">
This paper proposes a system for automatically reading either Japanese or English documents that have complex layout structures that include graphics. First, document image segmentation and character segmentation are carried out using three basic features and the knowledge of document layout rules. Next, multi-font character recognition is performed based on feature vector matching. Recognition experiments with a prototype system for a variety of complex printed documents shows that the proposed system is capable of reading different types of printed documents at an accuracy rate of 94.8â€“97.2%.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Alvaro2016">Alvaro2016</a>]
</td>
<td class="bibtexitem">
Francisco &Aacute;lvaro, Joan-Andreu S&aacute;nchez, and Jos&eacute;-Miguel Bened&iacute;.
 An integrated grammar-based approach for mathematical expression
  recognition.
 <em>Pattern Recognition</em>, 51: 135-147, 2016.
[&nbsp;<a href="OMR-Related_bib.html#Alvaro2016">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.patcog.2015.09.013">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Andre2014">Andre2014</a>]
</td>
<td class="bibtexitem">
Ga&euml;tan Andr&eacute;, Viviane Kostrubiec, Jean-Christophe Buisson,
  Jean-Michel Albaret, and Pier-Giorgio Zanone.
 A parsimonious oscillatory model of handwriting.
 <em>Biological Cybernetics</em>, 108 (3): 321-336, 2014.
[&nbsp;<a href="OMR-Related_bib.html#Andre2014">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s00422-014-0600-z">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Ann2010">Ann2010</a>]
</td>
<td class="bibtexitem">
Hsing-Yen Ann, Chang-Biau Yang, Yung-Hsing Peng, and Bern-Cherng Liaw.
 Efficient algorithms for the block edit problems.
 <em>Information and Computation</em>, 208 (3): 221-229, 2010.
[&nbsp;<a href="OMR-Related_bib.html#Ann2010">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.ic.2009.12.001">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Bahdanau2014">Bahdanau2014</a>]
</td>
<td class="bibtexitem">
Dzmitry Bahdanau, Cho&nbsp;Kyung Hyun, and Yoshua Bengio.
 Neural Machine Translation by Jointly Learning to Align and
  Translate.
 <em>Computing Research Repository</em>, abs/1409.0473, 2014.
[&nbsp;<a href="OMR-Related_bib.html#Bahdanau2014">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1409.0473">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Balke2019">Balke2019</a>]
</td>
<td class="bibtexitem">
Stefan Balke, Matthias Dorfer, Luis Carvalho, Andreas Arzt, and Gerhard Widmer.
 Learning Soft-Attention Models for Tempo-Invariant Audio-Sheet Music
  Retrieval.
 In <em>20th International Society for Music Information Retrieval
  Conference</em>, pages 75-82, 2019.
[&nbsp;<a href="OMR-Related_bib.html#Balke2019">bib</a>&nbsp;| 
<a href="http://archives.ismir.net/ismir2019/paper/000024.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Barate2018">Barate2018</a>]
</td>
<td class="bibtexitem">
Adriano Barat&egrave;, Goffredo Haus, and Luca&nbsp;A. Ludovico.
 Advanced Experience of Music through 5G Technologies.
 <em>IOP Conference Series: Materials Science and Engineering</em>, 364
  (1): 012021, 2018.
[&nbsp;<a href="OMR-Related_bib.html#Barate2018">bib</a>&nbsp;| 
<a href="http://stacks.iop.org/1757-899X/364/i=1/a=012021">http</a>&nbsp;]
<blockquote><font size="-1">
This paper focuses on new models to enjoy music that will be implementable in a near future thanks to 5G technology. In the last two decades, our research mainly focused on the comprehensive description of music information, where multiple aspects are integrated to provide the user with an advanced multi-layer environment to experience music content. In recent times, the advancements in network technologies allowed a web implementation of this approach through W3C-compliant languages. The last obstacle to the use of personal devices is currently posed by the characteristics of mobile networks, concerning bandwidth, reliability, and the density of devices in an area. Designed to meet the requirements of future technological challenges, such as the Internet of Things and self-driving vehicles, the advent of 5G networks will solve these problems, thus paving the way also for new music-oriented applications. The possibilities described in this work range from bringing archive materials and music cultural heritage to a new life to the implementation of immersive environments for live-show remote experience.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Bay2009">Bay2009</a>]
</td>
<td class="bibtexitem">
Mert Bay, Andreas&nbsp;F. Ehmann, and J.&nbsp;Stephen Downie.
 Evaluation of Multiple-F0 Estimation and Tracking Systems.
 In <em>10th International Society for Music Information Retrieval
  Conference</em>, pages 315-320, Kobe, Japan, 2009.
[&nbsp;<a href="OMR-Related_bib.html#Bay2009">bib</a>&nbsp;| 
<a href="http://ismir2009.ismir.net/proceedings/PS2-21.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Bellini2005">Bellini2005</a>]
</td>
<td class="bibtexitem">
Pierfrancesco Bellini, Paolo Nesi, and Giorgio Zoia.
 Symbolic Music Representation in MPEG.
 <em>IEEE MultiMedia</em>, 12 (4): 42-49, 2005.
[&nbsp;<a href="OMR-Related_bib.html#Bellini2005">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/MMUL.2005.82">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Benetos2013">Benetos2013</a>]
</td>
<td class="bibtexitem">
Emmanouil Benetos, Simon Dixon, Dimitrios Giannoulis, Holger Kirchhoff, and
  Anssi Klapuri.
 Automatic music transcription: challenges and future directions.
 <em>Journal of Intelligent Information Systems</em>, 41 (3): 407-434,
  2013.
 ISSN 1573-7675.
[&nbsp;<a href="OMR-Related_bib.html#Benetos2013">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s10844-013-0258-3">DOI</a>&nbsp;]
<blockquote><font size="-1">
Automatic music transcription is considered by many to be a key enabling technology in music signal processing. However, the performance of transcription systems is still significantly below that of a human expert, and accuracies reported in recent years seem to have reached a limit, although the field is still very active. In this paper we analyse limitations of current methods and identify promising directions for future research. Current transcription methods use general purpose models which are unable to capture the rich diversity found in music signals. One way to overcome the limited performance of transcription systems is to tailor algorithms to specific use-cases. Semi-automatic approaches are another way of achieving a more reliable transcription. Also, the wealth of musical scores and corresponding audio data now available are a rich potential source of training data, via forced alignment of audio to scores, but large scale utilisation of such data has yet to be attempted. Other promising approaches include the integration of information from multiple algorithms and different musical aspects.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Bezine2004">Bezine2004</a>]
</td>
<td class="bibtexitem">
Hala Bezine, Adel&nbsp;M. Alimi, and Nasser Sherkat.
 Generation and Analysis of Handwriting Script with the Beta-Elliptic
  Model.
 In <em>Ninth International Workshop on Frontiers in Handwriting
  Recognition</em>. Institute of Electrical &amp; Electronics Engineers (IEEE),
  2004a.
[&nbsp;<a href="OMR-Related_bib.html#Bezine2004">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/iwfhr.2004.45">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Bezine2004a">Bezine2004a</a>]
</td>
<td class="bibtexitem">
Hala Bezine, Adel&nbsp;M. Alimi, and Nasser Sherkat.
 Generation and Analysis of Handwriting Script With the Beta-Elliptic
  Model.
 <em>International Journal of Simulation</em>, 8 (2): 45-65, 2004b.
 ISSN 1473-8031.
[&nbsp;<a href="OMR-Related_bib.html#Bezine2004a">bib</a>&nbsp;| 
<a href="http://ijssst.info/Vol-08/No-2/paper6.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Bojar2011">Bojar2011</a>]
</td>
<td class="bibtexitem">
Ond&#X0159;ej Bojar, Milo&scaron; Ercegov&#x010D;evi&#x107;, Martin Popel, and
  Omar&nbsp;F. Zaidan.
 A Grain of Salt for the WMT Manual Evaluation.
 In <em>Sixth Workshop on Statistical Machine Translation</em>, pages
  1-11, Edinburgh, Scotland, 2011. Association for Computational Linguistics.
 ISBN 978-1-937284-12-1.
[&nbsp;<a href="OMR-Related_bib.html#Bojar2011">bib</a>&nbsp;| 
<a href="http://dl.acm.org/citation.cfm?id=2132960.2132962">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Bresler2015">Bresler2015</a>]
</td>
<td class="bibtexitem">
Martin Bresler, Daniel Pr&#367;&scaron;a, and V&aacute;clav Hlav&aacute;&#x010D;.
 Using Agglomerative Clustering of Strokes to Perform Symbols
  Over-segmentation within a Diagram Recognition System.
 In Vincent&nbsp;Lepetit Paul&nbsp;Wohlhart, editor, <em>20th Computer Vision
  Winter Workshop</em>, pages 67-74, Seggau, Austria, 2015. Graz University of
  Technology.
 ISBN 978-3-85125-388-7.
[&nbsp;<a href="OMR-Related_bib.html#Bresler2015">bib</a>&nbsp;| 
<a href="http://cmp.felk.cvut.cz/ftp/articles/bresler/Bresler-Prusa-Hlavac-CVWW-2015.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Breuel2013">Breuel2013</a>]
</td>
<td class="bibtexitem">
Thomas&nbsp;M. Breuel, Adnan Ul-Hasan, Mayce Al&nbsp;Azawi, and Faisal Shafait.
 High-Performance OCR for Printed English and Fraktur Using LSTM
  Networks.
 In <em>2013 12th International Conference on Document Analysis and
  Recognition</em>, pages 683-687, 2013.
[&nbsp;<a href="OMR-Related_bib.html#Breuel2013">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICDAR.2013.140">DOI</a>&nbsp;]
<blockquote><font size="-1">
Long Short-Term Memory (LSTM) networks have yielded excellent results
	on handwriting recognition. This paper describes an application of
	bidirectional LSTM networks to the problem of machine-printed Latin
	and Fraktur recognition. Latin and Fraktur recognition differs significantly
	from handwriting recognition in both the statistical properties of
	the data, as well as in the required, much higher levels of accuracy.
	Applications of LSTM networks to handwriting recognition use two-dimensional
	recurrent networks, since the exact position and baseline of handwritten
	characters is variable. In contrast, for printed OCR, we used a one-dimensional
	recurrent network combined with a novel algorithm for baseline and
	x-height normalization. A number of databases were used for training
	and testing, including the UW3 database, artificially generated and
	degraded Fraktur text and scanned pages from a book digitization
	project. The LSTM architecture achieved 0.6	error on English text. When the artificially degraded Fraktur data
	set is divided into training and test sets, the system achieves an
	error rate of 1.64	of the training set), the system achieves error rates of 0.15	and 1.47	without using any language modelling or any other post-processing
	techniques.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Byrd2006a">Byrd2006a</a>]
</td>
<td class="bibtexitem">
Donald Byrd.
 Music Notation and Music Representation.
 Technical report, Indiana University, School of informatics, 2006.
[&nbsp;<a href="OMR-Related_bib.html#Byrd2006a">bib</a>&nbsp;| 
<a href="http://music.informatics.indiana.edu/don_notation.html">.html</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Callison-Burch2007">Callison-Burch2007</a>]
</td>
<td class="bibtexitem">
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh
  Schroeder.
 (Meta-) Evaluation of Machine Translation.
 In <em>Second Workshop on Statistical Machine Translation</em>, pages
  136-158, Stroudsburg, PA, USA, 2007. Association for Computational
  Linguistics.
[&nbsp;<a href="OMR-Related_bib.html#Callison-Burch2007">bib</a>&nbsp;| 
<a href="http://dl.acm.org/citation.cfm?id=1626355.1626373">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Callison-Burch2008">Callison-Burch2008</a>]
</td>
<td class="bibtexitem">
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh
  Schroeder.
 Further Meta-evaluation of Machine Translation.
 In <em>Third Workshop on Statistical Machine Translation</em>, pages
  70-106, Stroudsburg, PA, USA, 2008. Association for Computational
  Linguistics.
 ISBN 978-1-932432-09-1.
[&nbsp;<a href="OMR-Related_bib.html#Callison-Burch2008">bib</a>&nbsp;| 
<a href="http://dl.acm.org/citation.cfm?id=1626394.1626403">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Callison-Burch2010">Callison-Burch2010</a>]
</td>
<td class="bibtexitem">
Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Peterson, Mark
  Przybocki, and Omar&nbsp;F. Zaidan.
 Findings of the 2010 Joint Workshop on Statistical Machine
  Translation and Metrics for Machine Translation.
 In <em>Joint Fifth Workshop on Statistical Machine Translation and
  MetricsMATR</em>, pages 17-53, Stroudsburg, PA, USA, 2010. Association for
  Computational Linguistics.
 ISBN 978-1-932432-71-8.
[&nbsp;<a href="OMR-Related_bib.html#Callison-Burch2010">bib</a>&nbsp;| 
<a href="http://dl.acm.org/citation.cfm?id=1868850.1868853">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Calvo-Zaragoza2016a">Calvo-Zaragoza2016a</a>]
</td>
<td class="bibtexitem">
Jorge Calvo-Zaragoza, Jose&nbsp;J. Valero-Mas, and Juan&nbsp;R. Rico-Juan.
 Prototype generation on structural data using dissimilarity space
  representation.
 <em>Neural Computing and Applications</em>, pages 1-10, 2016.
 ISSN 1433-3058.
[&nbsp;<a href="OMR-Related_bib.html#Calvo-Zaragoza2016a">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s00521-016-2278-8">DOI</a>&nbsp;]
<blockquote><font size="-1">
Data reduction techniques play a key role in instance-based classification to lower the amount of data to be processed. Among the different existing approaches, prototype selection (PS) and prototype generation (PG) are the most representative ones. These two families differ in the way the reduced set is obtained from the initial one: While the former aims at selecting the most representative elements from the set, the latter creates new data out of it. Although PG is considered to delimit more efficiently decision boundaries, the operations required are not so well defined in scenarios involving structural data such as strings, trees, or graphs. This work studies the possibility of using dissimilarity space (DS) methods as an intermediate process for mapping the initial structural representation to a statistical one, thereby allowing the use of PG methods. A comparative experiment over string data is carried out in which our proposal is faced to PS methods on the original space. Results show that the proposed strategy is able to achieve significantly similar results to PS in the initial space, thus standing as a clear alternative to the classic approach, with some additional advantages derived from the DS representation.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Calvo-Zaragoza2016b">Calvo-Zaragoza2016b</a>]
</td>
<td class="bibtexitem">
Jorge Calvo-Zaragoza and Jose Oncina.
 An efficient approach for Interactive Sequential Pattern Recognition.
 <em>Pattern Recognition</em>, 64: 295-304, 2016.
 ISSN 0031-3203.
[&nbsp;<a href="OMR-Related_bib.html#Calvo-Zaragoza2016b">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.patcog.2016.11.006">DOI</a>&nbsp;| 
<a href="http://www.sciencedirect.com/science/article/pii/S0031320316303569">http</a>&nbsp;]
<blockquote><font size="-1">
Abstract Interactive Pattern Recognition (IPR) is an emergent framework in which the user is involved actively in the recognition process by giving feedback to the system when an error is detected. Although this framework is expected to reduce the number of errors to correct, it may increase the time required to complete the task since the machine needs to recompute its proposal after each interaction. Therefore, a fast computation is required to make the interactive system profitable and user-friendly. This work presents an efficient approach to deal with IPR tasks when data has a sequential nature. Our approach includes some computation at the very beginning of the task but it then achieves a linear complexity after user corrections. We also show how these tasks can be effectively carried out if the solution space is defined with a Regular Language. This fact has indeed proven to be the most relevant factor to improve the efficiency of the approach. Several experiments are carried out in which our proposal is faced against a classical search. Results show a reduction in time in all experiments considered, solving efficiently some complex IPR tasks thanks to our proposals.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Cancino-Chacon2018">Cancino-Chacon2018</a>]
</td>
<td class="bibtexitem">
Carlos&nbsp;E. Cancino-Chac&oacute;n, Maarten Grachten, Werner Goebl, and Gerhard
  Widmer.
 Computational Models of Expressive Music Performance: A Comprehensive
  and Critical Review.
 <em>Frontiers in Digital Humanities</em>, 5: 25, 2018.
 ISSN 2297-2668.
[&nbsp;<a href="OMR-Related_bib.html#Cancino-Chacon2018">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.3389/fdigh.2018.00025">DOI</a>&nbsp;]
<blockquote><font size="-1">
Expressive performance is an indispensable part of music making.
When playing a piece, expert performers shape various parameters (tempo,
timing, dynamics, intonation, articulation, etc.) in ways that are not
prescribed by the notated score, in this way producing an expressive
rendition that brings out dramatic, affective, and emotional
qualities that may engage and affect the listeners.
Given the central importance of this skill for many kinds of music, expressive
performance has become an important research topic for disciplines like
musicology, music psychology, etc. 
This paper focuses on a specific thread of research: work on
computational music performance models. Computational models
are attempts at codifying hypotheses about expressive performance in terms of
mathematical formulas or computer programs, so that they can be evaluated in
systematic and quantitative ways. Such models can serve at least two main purposes:
they permit us to systematically study certain hypotheses regarding performance;
and they can be used as tools to generate automated or semi-automated performances,
in artistic or educational contexts.
The present article presents an up-to-date overview of the state of the art
in this domain. We explore recent trends in the field, such as a strong focus on
data-driven (machine learning); a growing interest in interactive expressive systems,
such as conductor simulators and automatic accompaniment systems; and an increased
interest in exploring cognitively plausible features and models.
We provide an in-depth discussion of several important design choices in
such computer models, and discuss a crucial (and still largely unsolved)
problem that is hindering systematic progress: the question of how to
evaluate such models in scientifically and musically meaningful ways.
From all this, we finally derive some research directions that should be pursued
with priority, in order to advance the field and our understanding
of expressive music performance.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Chen2017a">Chen2017a</a>]
</td>
<td class="bibtexitem">
Liang-Chieh Chen, Alexander Hermans, George Papandreou, Florian Schroff, Peng
  Wang, and Hartwig Adam.
 MaskLab: Instance Segmentation by Refining Object Detection with
  Semantic and Direction Features.
 <em>CoRR</em>, abs/1712.04837, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Chen2017a">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.04837">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Chiu2018">Chiu2018</a>]
</td>
<td class="bibtexitem">
Chung-Cheng Chiu, Tara&nbsp;N. Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick
  Nguyen, Zhifeng Chen, Anjuli Kannan, Ron&nbsp;J. Weiss, Kanishka Rao, Ekaterina
  Gonina, Navdeep Jaitly, Bo&nbsp;Li, Jan Chorowski, and Michiel Bacchiani.
 State-of-the-Art Speech Recognition with Sequence-to-Sequence Models.
 In <em>2018 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)</em>, pages 4774-4778, 2018.
[&nbsp;<a href="OMR-Related_bib.html#Chiu2018">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP.2018.8462105">DOI</a>&nbsp;]
<blockquote><font size="-1">
Attention-based encoder-decoder architectures such as Listen, Attend, and Spell (LAS), subsume the acoustic, pronunciation and language model components of a traditional automatic speech recognition (ASR) system into a single neural network. In previous work, we have shown that such architectures are comparable to state-of-the-art ASR systems on dictation tasks, but it was not clear if such architectures would be practical for more challenging tasks such as voice search. In this work, we explore a variety of structural and optimization improvements to our LAS model which significantly improve performance. On the structural side, we show that word piece models can be used instead of graphemes. We also introduce a multi-head attention architecture, which offers improvements over the commonly-used single-head attention. On the optimization side, we explore synchronous training, scheduled sampling, label smoothing, and minimum word error rate optimization, which are all shown to improve accuracy. We present results with a unidirectional LSTM encoder for streaming recognition. On a 12, 500 hour voice search task, we find that the proposed changes improve the WER from 9.2% to 5.6%, while the best conventional system achieves 6.7%; on a dictation task our model achieves a WER of 4.1% compared to 5% for the conventional system.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Chollet2017">Chollet2017</a>]
</td>
<td class="bibtexitem">
Fran&ccedil;ois Chollet.
 Keras.
 <a href="https://github.com/fchollet/keras">https://github.com/fchollet/keras</a>, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Chollet2017">bib</a>&nbsp;| 
<a href="https://github.com/keras-team/keras">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Chowdhury2018">Chowdhury2018</a>]
</td>
<td class="bibtexitem">
Arindam Chowdhury and Lovekesh Vig.
 An Efficient End-to-End Neural Model for Handwritten Text
  Recognition.
 In <em>29th British Machine Vision Conference</em>, 2018.
[&nbsp;<a href="OMR-Related_bib.html#Chowdhury2018">bib</a>&nbsp;| 
<a href="http://bmvc2018.org/contents/papers/0606.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Clausner2011">Clausner2011</a>]
</td>
<td class="bibtexitem">
Christian Clausner, Stefan Pletschacher, and Apostolos Antonacopoulos.
 Aletheia - An Advanced Document Layout and Text Ground-Truthing
  System for Production Environments.
 In <em>2011 International Conference on Document Analysis and
  Recognition, ICDAR</em>, pages 48-52, Beijing, China, 2011. IEEE Computer
  Society.
[&nbsp;<a href="OMR-Related_bib.html#Clausner2011">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICDAR.2011.19">DOI</a>&nbsp;| 
<a href="http://www.prima.cse.salford.ac.uk:8080/www/assets/papers/ICDAR2011_Clausner_Aletheia.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Clausner2014">Clausner2014</a>]
</td>
<td class="bibtexitem">
Christian Clausner, Stefan Pletschacher, and Apostolos Antonacopoulos.
 Efficient OCR Training Data Generation with Aletheia.
 In <em>Short Paper Booklet of the 11th International Association
  for Pattern Recognition (IAPR) Workshop on Document Analysis Systems (DAS)</em>,
  2014.
[&nbsp;<a href="OMR-Related_bib.html#Clausner2014">bib</a>&nbsp;| 
<a href="http://www.primaresearch.org/www/assets/papers/DAS2014_Clausner_OCRTrainingDataGeneration.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Cont2007">Cont2007</a>]
</td>
<td class="bibtexitem">
Arshia Cont, Diemo Schwarz, Norbert Schnell, and Christopher Raphael.
 Evaluation of Real-Time Audio-to-Score Alignment.
 In <em>8th International Conference on Music Information
  Retrieval</em>, Vienna, Austria, 2007.
[&nbsp;<a href="OMR-Related_bib.html#Cont2007">bib</a>&nbsp;| 
<a href="https://hal.inria.fr/hal-00839068">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Cont2010">Cont2010</a>]
</td>
<td class="bibtexitem">
Arshia Cont.
 A Coupled Duration-Focused Architecture for Real-Time Music-to-Score
  Alignment.
 <em>IEEE Transactions on Pattern Analysis and Machine
  Intelligence</em>, 32 (6): 974-987, 2010.
[&nbsp;<a href="OMR-Related_bib.html#Cont2010">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/TPAMI.2009.106">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Cordella2000">Cordella2000</a>]
</td>
<td class="bibtexitem">
L.&nbsp;P. Cordella and M.&nbsp;Vento.
 Symbol and Shape Recognition.
 In Atul&nbsp;K. Chhabra and Dov Dori, editors, <em>Graphics Recognition
  Recent Advances</em>, pages 167-182, Berlin, Heidelberg, 2000. Springer Berlin
  Heidelberg.
 ISBN 978-3-540-40953-3.
[&nbsp;<a href="OMR-Related_bib.html#Cordella2000">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/3-540-40953-X_14">DOI</a>&nbsp;]
<blockquote><font size="-1">
The different aspects of a process for recognizing symbols in documents are considered and the techniques that have been most commonly used during the last ten years, in the different application fields, are reviewed. Methods used in the representation, description and classification phases are shortly discussed and the main recognition strategies are mentioned. Some of the problems that appear still open are proposed to the attention of the reader.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="CPDL">CPDL</a>]
</td>
<td class="bibtexitem">
Rafael Ornes.
 Choral Public Domain Library.
 <a href="http://cpdl.org">http://cpdl.org</a>, 1998.
[&nbsp;<a href="OMR-Related_bib.html#CPDL">bib</a>&nbsp;| 
<a href="http://cpdl.org">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Cutter2015">Cutter2015</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;P. Cutter and Roberto Manduchi.
 Towards Mobile OCR.
 In <em>2015 ACM Symposium on Document Engineering - DocEng'15</em>.
  ACM Press, 2015.
[&nbsp;<a href="OMR-Related_bib.html#Cutter2015">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/2682571.2797066">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Dawid1979">Dawid1979</a>]
</td>
<td class="bibtexitem">
Alexander&nbsp;Philip Dawid and Allan&nbsp;M. Skene.
 Maximum likelihood estimation of observer error-rates using the EM
  algorithm.
 <em>Applied statistics</em>, pages 20-28, 1979.
[&nbsp;<a href="OMR-Related_bib.html#Dawid1979">bib</a>&nbsp;| 
<a href="https://www.jstor.org/stable/2346806">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Delakis2008">Delakis2008</a>]
</td>
<td class="bibtexitem">
Manolis Delakis and Christophe Garcia.
 Text detection with convolutional neural networks.
 In <em>International Conference on Computer Vision Theory and
  Application</em>, pages 290-294, 2008.
[&nbsp;<a href="OMR-Related_bib.html#Delakis2008">bib</a>&nbsp;| 
<a href="https://www.researchgate.net/profile/Christophe_Garcia2/publication/221415287_text_Detection_with_Convolutional_Neural_Networks/links/545251a30cf2bccc49087299/text-Detection-with-Convolutional-Neural-Networks.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="DIAMM">DIAMM</a>]
</td>
<td class="bibtexitem">
Margaret Bent and Andrew Wathey.
 Digital Image Archive of Medieval Music.
 <a href="https://www.diamm.ac.uk">https://www.diamm.ac.uk</a>, 1998.
[&nbsp;<a href="OMR-Related_bib.html#DIAMM">bib</a>&nbsp;| 
<a href="https://www.diamm.ac.uk">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Downie2008">Downie2008</a>]
</td>
<td class="bibtexitem">
J.&nbsp;Stephen Downie.
 The music information retrieval evaluation exchange (2005-2007): A
  window into music information retrieval research.
 <em>Acoust. Sci. &amp; Tech.</em>, 29 (4): 247-255, 2008.
[&nbsp;<a href="OMR-Related_bib.html#Downie2008">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1250/ast.29.247">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Downie2010">Downie2010</a>]
</td>
<td class="bibtexitem">
J.&nbsp;Stephen Downie, Andreas&nbsp;F. Ehmann, Mert Bay, and M.&nbsp;Cameron Jones.
 The Music Information Retrieval Evaluation eXchange: Some
  Observations and Insights.
 In <em>Advances in Music Information Retrieval</em>, pages 93-115.
  Springer Berlin Heidelberg, Berlin, Heidelberg, 2010.
 ISBN 978-3-642-11674-2.
[&nbsp;<a href="OMR-Related_bib.html#Downie2010">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-642-11674-2_5">DOI</a>&nbsp;| 
<a href="https://doi.org/10.1007/978-3-642-11674-2_5">http</a>&nbsp;]
<blockquote><font size="-1">
Advances in the science and technology of Music Information Retrieval (MIR) systems and algorithms are dependent on the development of rigorous measures of accuracy and performance such that meaningful comparisons among current and novel approaches can be made. This is the motivating principle driving the efforts of the International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL) and the annual Music Information Retrieval Evaluation eXchange (MIREX). Since it started in 2005, MIREX has fostered great advancements not only in many specific areas of MIR, but also in our general understanding of how MIR systems and algorithms are to be evaluated. This chapter outlines some of the major highlights of the past four years of MIREX evaluations, including its organizing principles, the selection of evaluation metrics, and the evolution of evaluation tasks. The chapter concludes with a brief introduction of how MIREX plans to expand into the future using a suite of Web 2.0 technologies to automated MIREX evaluations.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Droettboom2003">Droettboom2003</a>]
</td>
<td class="bibtexitem">
Michael Droettboom.
 Correcting broken characters in the recognition of historical printed
  documents.
 In <em>Joint Conference on Digital Libraries</em>, pages 364-366,
  2003.
[&nbsp;<a href="OMR-Related_bib.html#Droettboom2003">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/JCDL.2003.1204889">DOI</a>&nbsp;]
<blockquote><font size="-1">
We present a new technique for dealing with broken characters, one of the major challenges in the optical character recognition (OCR) of degraded historical printed documents. A technique based on graph combinatorics is used to rejoin the appropriate connected components. It has been applied to real data with successful results.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Erickson1975">Erickson1975</a>]
</td>
<td class="bibtexitem">
Raymond&nbsp;F. Erickson.
 &ldquo;The Darms project&rdquo;: A status report.
 <em>Computers and the Humanities</em>, 9 (6): 291-298, 1975.
 ISSN 1572-8412.
[&nbsp;<a href="OMR-Related_bib.html#Erickson1975">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/BF02396292">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Eskenazi2017">Eskenazi2017</a>]
</td>
<td class="bibtexitem">
S&eacute;bastien Eskenazi, Petra Gomez-Kr&auml;mer, and Jean-Marc Ogier.
 A comprehensive survey of mostly textual document segmentation
  algorithms since 2008.
 <em>Pattern Recognition</em>, 64: 1-14, 2017.
 ISSN 0031-3203.
[&nbsp;<a href="OMR-Related_bib.html#Eskenazi2017">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.patcog.2016.10.023">DOI</a>&nbsp;]
<blockquote><font size="-1">
In document image analysis, segmentation is the task that identifies the regions of a document. The increasing number of applications of document analysis requires a good knowledge of the available technologies. This survey highlights the variety of the approaches that have been proposed for document image segmentation since 2008. It provides a clear typology of documents and of document image segmentation algorithms. We also discuss the technical limitations of these algorithms, the way they are evaluated and the general trends of the community. Â© 2016 Elsevier Ltd
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Everingham2015">Everingham2015</a>]
</td>
<td class="bibtexitem">
Mark Everingham, S.&nbsp;M.&nbsp;Ali Eslami, Luc Van&nbsp;Gool, Christopher K.&nbsp;I. Williams,
  John Winn, and Andrew Zisserman.
 The Pascal Visual Object Classes Challenge: A Retrospective.
 <em>International Journal of Computer Vision</em>, 111 (1): 98-136,
  2015.
[&nbsp;<a href="OMR-Related_bib.html#Everingham2015">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s11263-014-0733-5">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Ewert2014">Ewert2014</a>]
</td>
<td class="bibtexitem">
Sebastian Ewert, Bryan Pardo, Meinard M&uuml;ller, and Mark&nbsp;D. Plumbley.
 Score-Informed Source Separation for Musical Audio Recordings: An
  overview.
 <em>IEEE Signal Process. Mag.</em>, 31 (3): 116-124, 2014.
[&nbsp;<a href="OMR-Related_bib.html#Ewert2014">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/MSP.2013.2296076">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Fahmy1992">Fahmy1992</a>]
</td>
<td class="bibtexitem">
Hoda Fahmy and Dorothea Blostein.
 A survey of graph grammars: Theory and applications.
 In <em>11th IAPR International Conference on Pattern Recognition</em>,
  pages 294-298. IEEE, 1992.
[&nbsp;<a href="OMR-Related_bib.html#Fahmy1992">bib</a>&nbsp;| 
<a href="https://www.researchgate.net/publication/3513859_A_survey_of_graph_grammars_theory_and_applications">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Fasanaro1990">Fasanaro1990</a>]
</td>
<td class="bibtexitem">
A.&nbsp;M. Fasanaro, D.&nbsp;L.&nbsp;A. Spitaleri, R.&nbsp;Valiani, and D.&nbsp;Grossi.
 Dissociation in Musical Reading: A Musician Affected by Alexia
  without Agraphia.
 <em>Music Perception: An Interdisciplinary Journal</em>, 7 (3):
  259-272, 1990.
 ISSN 0730-7829.
[&nbsp;<a href="OMR-Related_bib.html#Fasanaro1990">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.2307/40285464">DOI</a>&nbsp;| 
<a href="http://mp.ucpress.edu/content/7/3/259">http</a>&nbsp;]
<blockquote><font size="-1">
Previous works have postulated a similarity between music reading
	and text reading. Therefore it is interesting to evaluate both of
	these functions in an alexic subject. The patient investigated is
	a professional musician who had an ischemic lesion in the left temporoparieto-occipital
	region. Text reading showed pure alexia in which both the phonological
	and global routes were damaged. His ability to read correctly via
	matching tests showed that the word-form system was preserved. The
	reading of musical scores was damaged too and showed a dissociation
	between the reading of ideograms and rhythms (preserved) and the
	reading of notes (impaired). The results of note reading were analogous
	to those of word reading. Furthermore, the patient could read notes
	correctly via matching tests. On the basis of these findings, we
	propose a model of music reading where the reading of notes is based
	on a representational system analogous to that of words (the so-called
	internal language) whereas reading of ideograms and rhythms occurs
	via an internal representation unrelated to linguistic functions.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Feist2017">Feist2017</a>]
</td>
<td class="bibtexitem">
Jonathan Feist.
 <em>Berklee Contemporary Music Notation</em>.
 Berklee Press, 2017.
 ISBN 978-0876391785.
[&nbsp;<a href="OMR-Related_bib.html#Feist2017">bib</a>&nbsp;]
<blockquote><font size="-1">
Learn the nuances of music notation, and create professional looking scores. This reference presents a comprehensive look at contemporary music notation. You will learn the meaning and stylistic practices for many types of notation that are currently in common use, from traditional staffs to lead sheets to guitar tablature. It discusses hundreds of notation symbols, as well as general guidelines for writing music. Berklee College of Music brings together teachers and students from all over the world, and we use notation in a great variety of ways. This book presents our perspectives on notation: what we have found to be the most commonly used practices in today's music industry, and what seems to be serving our community best. It includes a foreword by Matthew Nicholl, who was a long-time chair of Berklee's Contemporary Writing and Production Department. Whether you find yourself in a Nashville recording studio, Hollywood sound stage, grand concert hall, worship choir loft, or elementary school auditorium, this book will help you to create readable, professional, publication-quality notation. Beyond understanding the standard rules and definitions, you will learn to make appropriate choices for your own work, and generally how to achieve clarity and consistency in your notation so that it best serves your music.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Ferlaino2018">Ferlaino2018</a>]
</td>
<td class="bibtexitem">
Michael Ferlaino, Craig&nbsp;A. Glastonbury, Carolina Motta-Mejia, Manu Vatish,
  Ingrid Granne, Stephen Kennedy, Cecilia&nbsp;M. Lindgren, and Christoffer
  Nell&aring;ker.
 Towards Deep Cellular Phenotyping in Placental Histology.
 <em>ArXiv e-prints</em>, 2018.
[&nbsp;<a href="OMR-Related_bib.html#Ferlaino2018">bib</a>&nbsp;| 
<a href="https://openreview.net/pdf?id=HJq5OGKsz">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Fletcher1988">Fletcher1988</a>]
</td>
<td class="bibtexitem">
Lloyd&nbsp;Alan Fletcher and Rangachar Kasturi.
 A Robust Algorithm for Text String Separation from Mixed
  Text/Graphics Images.
 <em>IEEE Transactions on Pattern Analysis and Machine
  Intelligence</em>, 10 (6): 910-918, 1988.
[&nbsp;<a href="OMR-Related_bib.html#Fletcher1988">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/34.9112">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Gao2003">Gao2003</a>]
</td>
<td class="bibtexitem">
Sheng Gao, Namunu&nbsp;Chinthaka Maddage, and Chin-Hui Lee.
 A hidden Markov model based approach to music segmentation and
  identification.
 In <em>Joint Fourth International Conference on Information,
  Communications and Signal Processing and the Fourth Pacific Rim Conference on
  Multimedia</em>, pages 1576-1580 vol.3, 2003.
[&nbsp;<a href="OMR-Related_bib.html#Gao2003">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICICS.2003.1292732">DOI</a>&nbsp;]
<blockquote><font size="-1">
Classification of musical segments is an interesting problem. It is a key technology in the development of content-based audio document indexing and retrieval. In this paper, we apply the feature extraction and modeling techniques commonly used in automatic speech recognition to solving the problem of segmentation and instrument identification of musical passages. The correlation among the different components in the feature space and the auto-correlation of each component are analyzed to demonstrate feasibility in musical signal analysis and instrument class modeling. Our experimental results are first evaluated on 3 instrument categories, i.e. vocal music, instrumental music, and their combinations. Furthermore each category is split into two individual cases to give a 6-class problem. Our results show that good performance could be obtained with simple features, such as mel-frequency cepstral coefficients and cepstral coefficients derived from linear prediction signal analysis. Even with a limited amount of training data, we could give an accuracy of 90.60% in the case of three categories. A slightly worse accuracy of 90.38% is obtained when we double the number of categories to six classes.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Garfinkle2017">Garfinkle2017</a>]
</td>
<td class="bibtexitem">
David Garfinkle, Claire Arthur, Peter Schubert, Julie Cumming, and Ichiro
  Fujinaga.
 PatternFinder: Content-Based Music Retrieval with Music21.
 In <em>4th International Workshop on Digital Libraries for
  Musicology</em>, pages 5-8, New York, NY, USA, 2017. ACM.
 ISBN 978-1-4503-5347-2.
[&nbsp;<a href="OMR-Related_bib.html#Garfinkle2017">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3144749.3144751">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Gatos2004">Gatos2004</a>]
</td>
<td class="bibtexitem">
Basilios Gatos, Ioannis Pratikakis, and Stavros&nbsp;J. Perantonis.
 An Adaptive Binarization Technique for Low Quality Historical
  Documents.
 In Simone Marinai and Andreas&nbsp;R. Dengel, editors, <em>Document
  Analysis Systems VI</em>, pages 102-113, Berlin, Heidelberg, 2004. Springer
  Berlin Heidelberg.
 ISBN 978-3-540-28640-0.
[&nbsp;<a href="OMR-Related_bib.html#Gatos2004">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-540-28640-0_10">DOI</a>&nbsp;]
<blockquote><font size="-1">
Historical document collections are a valuable resource for human history. This paper proposes a novel digital image binarization scheme for low quality historical documents allowing further content exploitation in an efficient way. The proposed scheme consists of five distinct steps: a pre-processing procedure using a low-pass Wiener filter, a rough estimation of foreground regions using Niblack's approach, a background surface calculation by interpolating neighboring background intensities, a thresholding by combining the calculated background surface with the original image and finally a post-processing step in order to improve the quality of text regions and preserve stroke connectivity. The proposed methodology works with great success even in cases of historical manuscripts with poor quality, shadows, nonuniform illumination, low contrast, large signal- dependent noise, smear and strain. After testing the proposed method on numerous low quality historical manuscripts, it has turned out that our methodology performs better compared to current state-of-the-art adaptive thresholding techniques.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Gatos2006">Gatos2006</a>]
</td>
<td class="bibtexitem">
Basilios Gatos, Ioannis Pratikakis, and Stavros&nbsp;J. Perantonis.
 Adaptive degraded document image binarization.
 <em>Pattern Recognition</em>, 39 (3): 317-327, 2006.
 ISSN 0031-3203.
[&nbsp;<a href="OMR-Related_bib.html#Gatos2006">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.patcog.2005.09.010">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Gatos2009">Gatos2009</a>]
</td>
<td class="bibtexitem">
Basilios Gatos, K.&nbsp;Ntirogiannis, and Stavros&nbsp;J. Perantonis.
 ICDAR 2009 Document Image Binarization Contest (DIBCO 2009).
 In <em>2009 10th International Conference on Document Analysis and
  Recognition</em>, pages 1375-1382, 2009.
[&nbsp;<a href="OMR-Related_bib.html#Gatos2009">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICDAR.2009.246">DOI</a>&nbsp;]
<blockquote><font size="-1">
DIBCO 2009 is the first International Document Image Binarization
	Contest organized in the context of ICDAR 2009 conference. The general
	objective of the contest is to identify current advances in document
	image binarization using established evaluation performance measures.
	This paper describes the contest details including the evaluation
	measures used as well as the performance of the 43 submitted methods
	along with a short description of each method.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="George2014">George2014</a>]
</td>
<td class="bibtexitem">
Joe George and Lior Shamir.
 Computer analysis of similarities between albums in popular music.
 <em>Pattern Recognition Letters</em>, 45: 78-84, 2014.
[&nbsp;<a href="OMR-Related_bib.html#George2014">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.patrec.2014.02.021">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Gerou2009">Gerou2009</a>]
</td>
<td class="bibtexitem">
Tom Gerou and Linda Lusk.
 <em>Essential Dicionary of Music Notation</em>.
 Alfred Publishing Co., Inc., 2009.
 ISBN 0-8828284-768-6.
[&nbsp;<a href="OMR-Related_bib.html#Gerou2009">bib</a>&nbsp;| 
<a href="http://www.amazon.com/Essentials-Music-Notation-Alfred-Publishing/dp/073906083X">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Giotis2017">Giotis2017</a>]
</td>
<td class="bibtexitem">
Angelos&nbsp;P. Giotis, Giorgos Sfikas, Basilis Gatos, and Christophoros Nikou.
 A survey of document image word spotting techniques.
 <em>Pattern Recognition</em>, 68: 310-332, 2017.
 ISSN 0031-3203.
[&nbsp;<a href="OMR-Related_bib.html#Giotis2017">bib</a>&nbsp;| 
<a href="https://doi.org/10.1016/j.patcog.2017.02.023">DOI</a>&nbsp;| 
<a href="http://www.sciencedirect.com/science/article/pii/S0031320317300870">http</a>&nbsp;]
<blockquote><font size="-1">
Vast collections of documents available in image format need to be indexed for information retrieval purposes. In this framework, word spotting is an alternative solution to optical character recognition (OCR), which is rather inefficient for recognizing text of degraded quality and unknown fonts usually appearing in printed text, or writing style variations in handwritten documents. Over the past decade there has been a growing interest in addressing document indexing using word spotting which is reflected by the continuously increasing number of approaches. However, there exist very few comprehensive studies which analyze the various aspects of a word spotting system. This work aims to review the recent approaches as well as fill the gaps in several topics with respect to the related works. The nature of texts and inherent challenges addressed by word spotting methods are thoroughly examined. After presenting the core steps which compose a word spotting system, we investigate the use of retrieval enhancement techniques based on relevance feedback which improve the retrieved results. Finally, we present the datasets which are widely used for word spotting, we describe the evaluation standards and measures applied for performance assessment and discuss the results achieved by the state of the art.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Girshick2014">Girshick2014</a>]
</td>
<td class="bibtexitem">
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.
 Rich feature hierarchies for accurate object detection and semantic
  segmentation.
 In <em>IEEE Conference On Computer Vision and Pattern Recognition</em>,
  pages 580-587, 2014.
[&nbsp;<a href="OMR-Related_bib.html#Girshick2014">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/CVPR.2014.81">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Girshick2015">Girshick2015</a>]
</td>
<td class="bibtexitem">
Ross Girshick.
 Fast R-CNN.
 In <em>2015 IEEE International Conference on Computer Vision
  (ICCV)</em>, pages 1440-1448, 2015.
[&nbsp;<a href="OMR-Related_bib.html#Girshick2015">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICCV.2015.169">DOI</a>&nbsp;]
<blockquote><font size="-1">
This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Girshick2016">Girshick2016</a>]
</td>
<td class="bibtexitem">
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.
 Region-Based Convolutional Networks for Accurate Object Detection and
  Segmentation.
 <em>IEEE Transactions on Pattern Analysis and Machine
  Intelligence</em>, 38 (1): 142-158, 2016.
 ISSN 0162-8828.
[&nbsp;<a href="OMR-Related_bib.html#Girshick2016">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/TPAMI.2015.2437384">DOI</a>&nbsp;]
<blockquote><font size="-1">
Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition. The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50 percent relative to the previous best result on VOC 2012-achieving a mAP of 62.4 percent. Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly. Since we combine region proposals with CNNs, we call the resulting model an R-CNN or Region-based Convolutional Network. Source code for the complete system is available at http://www.cs.berkeley.edu/&nbsp;rbg/rcnn.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Good2001">Good2001</a>]
</td>
<td class="bibtexitem">
Michael Good.
 MusicXML: An Internet-Friendly Format for Sheet Music.
 Technical report, Recordare LLC, 2001.
[&nbsp;<a href="OMR-Related_bib.html#Good2001">bib</a>&nbsp;| 
<a href="https://pdfs.semanticscholar.org/5617/972667ff794da79a4cbb6b985e85f8487ddd.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Good2003">Good2003</a>]
</td>
<td class="bibtexitem">
Michael Good and Geri Actor.
 Using MusicXML for File Interchange.
 In <em>Third International Conference on WEB Delivering of Music</em>,
  page 153, 2003.
[&nbsp;<a href="OMR-Related_bib.html#Good2003">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/WDM.2003.1233890">DOI</a>&nbsp;]
<blockquote><font size="-1">
The MusicXML format is designed to be a universal translator for programs that understand common Western musical notation. We have made significant progress towards this goal, with over a dozen programs supporting MusicXML as of June 2003. We describe some of the ways that MusicXML has been used for file interchange, and will demonstrate several scenarios.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Goodfellow2013">Goodfellow2013</a>]
</td>
<td class="bibtexitem">
Ian&nbsp;J. Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, and Vinay
  Shet.
 Multi-digit Number Recognition from Street View Imagery using Deep
  Convolutional Neural Networks.
 <em>Computing Research Repository</em>, abs/1312.6: 1-13, 2013.
[&nbsp;<a href="OMR-Related_bib.html#Goodfellow2013">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1312.6082">http</a>&nbsp;]
<blockquote><font size="-1">
Recognizing arbitrary multi-character text in unconstrained natural photographs is a hard problem. In this paper, we address an equally hard sub-problem in this domain viz. recognizing arbitrary multi-digit numbers from Street View imagery. Traditional approaches to solve this problem typically separate out the localization, segmentation, and recognition steps. In this paper we propose a unified approach that integrates these three steps via the use of a deep convolutional neural network that operates directly on the image pixels. We employ the DistBelief implementation of deep neural networks in order to train large, distributed neural networks on high quality images. We find that the performance of this approach increases with the depth of the convolutional network, with the best performance occurring in the deepest architecture we trained, with eleven hidden layers. We evaluate this approach on the publicly available SVHN dataset and achieve over 96% accuracy in recognizing complete street numbers. We show that on a per-digit recognition task, we improve upon the state-of-the-art, achieving 97.84% accuracy. We also evaluate this approach on an even more challenging dataset generated from Street View imagery containing several tens of millions of street number annotations and achieve over 90% accuracy. To further explore the applicability of the proposed system to broader text recognition tasks, we apply it to synthetic distorted text from reCAPTCHA. reCAPTCHA is one of the most secure reverse turing tests that uses distorted text to distinguish humans from bots. We report a 99.8% accuracy on the hardest category of reCAPTCHA. Our evaluations on both tasks indicate that at specific operating thresholds, the performance of the proposed system is comparable to, and in some cases exceeds, that of human operators.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Goolsby1994">Goolsby1994</a>]
</td>
<td class="bibtexitem">
Thomas&nbsp;W. Goolsby.
 Eye Movement in Music Reading: Effects of Reading Ability, Notational
  Complexity, and Encounters.
 <em>Music Perception: An Interdisciplinary Journal</em>, 12 (1):
  77-96, 1994a.
 ISSN 0730-7829.
[&nbsp;<a href="OMR-Related_bib.html#Goolsby1994">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.2307/40285756">DOI</a>&nbsp;| 
<a href="http://mp.ucpress.edu/content/12/1/77">http</a>&nbsp;]
<blockquote><font size="-1">
Six types of eye movement were measured and recorded with an SRI Eyetracker: number of progressive and regressive fixations, durations of progressive and regressive fixations and lengths of progressive and regressive saccades. Twenty-four graduate music students were selected as skilled and less- skilled music readers. Eye position was measured every millisecond with a high degree of accuracy. The factorial design was 2 Groups x 4 Melodies x 3 Encounters (including a practice period). Results indicated that patterns of eye movement in the two groups were similar across melodies and encounters, but differed with notational complexity. Eye movement was reduced when performing melodies with more-concentrated visual information than when performing melodies with less- concentrated visual information. The main effect of encounters indicated that music readers used fewer but longer fixations after practicing the melodies. Results suggest that skilled music readers look farther ahead in the notation, and then back to the point of performance, when sightreading.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Goolsby1994a">Goolsby1994a</a>]
</td>
<td class="bibtexitem">
Thomas&nbsp;W. Goolsby.
 Profiles of Processing: Eye Movements during Sightreading.
 <em>Music Perception: An Interdisciplinary Journal</em>, 12 (1):
  97-123, 1994b.
 ISSN 0730-7829.
[&nbsp;<a href="OMR-Related_bib.html#Goolsby1994a">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.2307/40285757">DOI</a>&nbsp;| 
<a href="http://mp.ucpress.edu/content/12/1/97">http</a>&nbsp;]
<blockquote><font size="-1">
Temporal and sequential components of the eye movement used by a skilled and a less-skilled sightreader were used to construct six profiles of processing. Each subject read three melodies of varying levels of concentration of visual detail. The profiles indicates the order, duration, and location of each fixation while the subjects sightread the melodies. Results indicate that music readers do not fixate on note stems or the bar lines that connect eighth notes when sightreading. The less-skilled music reader progressed through the melody virtually note-by-note using long fixations, whereas the skilled sightreader directed fixations to all areas of the notation (using more regressions than the less-skilled reader) to perform the music accurately. Results support earlier findings that skilled sightreaders look farther ahead in the notation, then back to the point of performance (Goolsby, 1994), and have a larger perceptual span than less-skilled sightreaders. Findings support Slobodans (1984) contention that music reading (i. e., sightreading) is indeed music perception, because music notation is processed before performance. Support was found for Sloboda's (1977, 1984, 1985, 1988) hypotheses on the effects of physical and structural boundaries on visual musical perception. The profiles indicate a number of differences between music perception from processing visual notation and perception resulting from language reading. These differences include: (1) opposite trends in the control of eye movement (i. e., the better music reader fixates in blank areas of the visual stimuli and not directly on each item of the information that was performed), (2) a perceptual span that is vertical as well as horizontal, (3) more eye movement associated with the better reader, and (4) greater attention used for processing language than for music, although the latter task requires an "exact realization."
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Gould2011">Gould2011</a>]
</td>
<td class="bibtexitem">
Elaine Gould.
 <em>Behind Bars</em>.
 Faber Music, 2011.
 ISBN 0-571-51456-1.
[&nbsp;<a href="OMR-Related_bib.html#Gould2011">bib</a>&nbsp;| 
<a href="http://behindbarsnotation.co.uk/">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Graves2007">Graves2007</a>]
</td>
<td class="bibtexitem">
Alex Graves, Santiago Fern&aacute;ndez, Marcus Liwicki, Horst Bunke, and
  J&uuml;rgen Schmidhuber.
 Unconstrained Online Handwriting Recognition with Recurrent Neural
  Networks.
 In <em>20th International Conference on Neural Information
  Processing Systems</em>, pages 577-584, USA, 2007. Curran Associates Inc.
 ISBN 978-1-60560-352-0.
[&nbsp;<a href="OMR-Related_bib.html#Graves2007">bib</a>&nbsp;| 
<a href="http://dl.acm.org/citation.cfm?id=2981562.2981635">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Graves2009">Graves2009</a>]
</td>
<td class="bibtexitem">
Alex Graves, Marcus Liwicki, Santiago Fern&aacute;ndez, Roman Bertolami, Horst
  Bunke, and J&uuml;rgen Schmidhuber.
 A novel connectionist system for unconstrained handwriting
  recognition.
 <em>Pattern Analysis and Machine Intelligence, IEEE Transactions
  on</em>, 31 (5): 855-868, 2009.
[&nbsp;<a href="OMR-Related_bib.html#Graves2009">bib</a>&nbsp;| 
<a href="http://www.cs.toronto.edu/~graves/tpami_2009.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Graves2013">Graves2013</a>]
</td>
<td class="bibtexitem">
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton.
 Speech recognition with deep recurrent neural networks.
 In <em>2013 IEEE International Conference on Acoustics, Speech and
  Signal Processing</em>, pages 6645-6649, 2013.
[&nbsp;<a href="OMR-Related_bib.html#Graves2013">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP.2013.6638947">DOI</a>&nbsp;]
<blockquote><font size="-1">
Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Graves2014">Graves2014</a>]
</td>
<td class="bibtexitem">
Alex Graves and Navdeep Jaitly.
 Towards End-to-end Speech Recognition with Recurrent Neural Networks.
 In <em>31st International Conference on International Conference on
  Machine Learning</em>, pages 1764-1772, Beijing, China, 2014. JMLR.org.
[&nbsp;<a href="OMR-Related_bib.html#Graves2014">bib</a>&nbsp;| 
<a href="http://dl.acm.org/citation.cfm?id=3044805.3045089">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Grefenstette2015">Grefenstette2015</a>]
</td>
<td class="bibtexitem">
Edward Grefenstette, Karl&nbsp;Moritz Hermann, Mustafa Suleyman, and Phil Blunsom.
 Learning to Transduce with Unbounded Memory.
 <em>Computing Research Repository</em>, abs/1506.02516, 2015.
[&nbsp;<a href="OMR-Related_bib.html#Grefenstette2015">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1506.02516">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Gregor2015">Gregor2015</a>]
</td>
<td class="bibtexitem">
Karol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra.
 DRAW: A Recurrent Neural Network For Image Generation.
 <em>Computing Research Repository</em>, abs/1502.04623, 2015.
[&nbsp;<a href="OMR-Related_bib.html#Gregor2015">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1502.04623">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Hankinson2011">Hankinson2011</a>]
</td>
<td class="bibtexitem">
Andrew Hankinson, Perry Roland, and Ichiro Fujinaga.
 The Music Encoding Initiative as a Document-Encoding Framework.
 In <em>12th International Society for Music Information Retrieval
  Conference</em>, pages 293-298, 2011.
[&nbsp;<a href="OMR-Related_bib.html#Hankinson2011">bib</a>&nbsp;| 
<a href="https://ismir2011.ismir.net/papers/OS3-1.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Harman2011">Harman2011</a>]
</td>
<td class="bibtexitem">
Donna Harman.
 <em>Information Retrieval Evaluation</em>.
 Morgan &amp; Claypool Publishers, 1st edition, 2011.
 ISBN 1598299719, 9781598299717.
[&nbsp;<a href="OMR-Related_bib.html#Harman2011">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="He2016">He2016</a>]
</td>
<td class="bibtexitem">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
 Deep residual learning for image recognition.
 In <em>The IEEE Conference on Computer Vision and Pattern
  Recogntiion (CVPR)</em>, pages 770-778, 2016.
[&nbsp;<a href="OMR-Related_bib.html#He2016">bib</a>&nbsp;| 
<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="He2017">He2017</a>]
</td>
<td class="bibtexitem">
Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick.
 Mask R-CNN.
 In <em>The IEEE International Conference on Computer Vision
  (ICCV)</em>, Oct 2017.
[&nbsp;<a href="OMR-Related_bib.html#He2017">bib</a>&nbsp;| 
<a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Heussenstamm1987">Heussenstamm1987</a>]
</td>
<td class="bibtexitem">
George Heussenstamm.
 <em>The Norton Manual of Music Notation</em>.
 W. W. Norton &amp; Company, 1987.
 ISBN 9780393955262.
[&nbsp;<a href="OMR-Related_bib.html#Heussenstamm1987">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Hosang2016">Hosang2016</a>]
</td>
<td class="bibtexitem">
Jan Hosang, Rodrigo Benenson, Piotr Doll&aacute;r, and Bernt Schiele.
 What Makes for Effective Detection Proposals?
 <em>IEEE Transactions on Pattern Analysis and Machine
  Intelligence</em>, 38 (4): 814-830, 2016.
 ISSN 0162-8828.
[&nbsp;<a href="OMR-Related_bib.html#Hosang2016">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/TPAMI.2015.2465908">DOI</a>&nbsp;]
<blockquote><font size="-1">
Current top performing object detectors employ detection proposals
	to guide the search for objects, thereby avoiding exhaustive sliding
	window search across images. Despite the popularity and widespread
	use of detection proposals, it is unclear which trade-offs are made
	when using them during object detection. We provide an in-depth analysis
	of twelve proposal methods along with four baselines regarding proposal
	repeatability, ground truth annotation recall on PASCAL, ImageNet,
	and MS COCO, and their impact on DPM, R-CNN, and Fast R-CNN detection
	performance. Our analysis shows that for object detection improving
	proposal localisation accuracy is as important as improving recall.
	We introduce a novel metric, the average recall (AR), which rewards
	both high recall and good localisation and correlates surprisingly
	well with detection performance. Our findings show common strengths
	and weaknesses of existing methods, and provide insights and metrics
	for selecting and tuning proposal methods.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Hu2017">Hu2017</a>]
</td>
<td class="bibtexitem">
Ronghang Hu, Piotr Doll&aacute;r, Kaiming He, Trevor Darrell, and Ross Girshick.
 Learning to Segment Every Thing.
 <em>CoRR</em>, abs/1711.10370, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Hu2017">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.10370">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Huang2016">Huang2016</a>]
</td>
<td class="bibtexitem">
Allen Huang and Raymond Wu.
 Deep Learning for Music.
 Technical report, Stanford University, 2016.
[&nbsp;<a href="OMR-Related_bib.html#Huang2016">bib</a>&nbsp;| 
<a href="https://arxiv.org/abs/1606.04930">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Huang2017">Huang2017</a>]
</td>
<td class="bibtexitem">
Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara,
  Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, and
  Kevin Murphy.
 Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors.
 In <em>The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)</em>, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Huang2017">bib</a>&nbsp;| 
<a href="http://openaccess.thecvf.com/content_cvpr_2017/html/Huang_SpeedAccuracy_Trade-Offs_for_CVPR_2017_paper.html">.html</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="IMSLP">IMSLP</a>]
</td>
<td class="bibtexitem">
Project Petrucci LLC.
 International Music Score Library Project.
 <a href="http://imslp.org">http://imslp.org</a>, 2006.
[&nbsp;<a href="OMR-Related_bib.html#IMSLP">bib</a>&nbsp;| 
<a href="http://imslp.org">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Journet2017">Journet2017</a>]
</td>
<td class="bibtexitem">
Nicholas Journet, Muriel Visani, Boris Mansencal, Kieu Van-Cuong, and Antoine
  Billy.
 DocCreator: A New Software for Creating Synthetic Ground-Truthed
  Document Images.
 <em>Journal of Imaging</em>, 3 (4): 62, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Journet2017">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.3390/jimaging3040062">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Kasturi1988">Kasturi1988</a>]
</td>
<td class="bibtexitem">
Rangachar Kasturi and Lloyd&nbsp;Alan Fletcher.
 A Robust Algorithm for Text String Separation from Mixed
  Text/Graphics Images.
 <em>IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</em>,
  10: 910-918, 1988.
 ISSN 0162-8828.
[&nbsp;<a href="OMR-Related_bib.html#Kasturi1988">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/34.9112">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Katayose1989">Katayose1989</a>]
</td>
<td class="bibtexitem">
Haruhiro Katayose, H.&nbsp;Kato, M.&nbsp;Imai, and Inokuchi S.
 An approach to an artificial music expert.
 In <em>International Computer Music Conference</em>, pages 139-146,
  1989.
[&nbsp;<a href="OMR-Related_bib.html#Katayose1989">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Katayose1989a">Katayose1989a</a>]
</td>
<td class="bibtexitem">
Haruhiro Katayose and Seiji Inokuchi.
 The Kansei Music System.
 <em>Computer Music Journal</em>, 13 (4): 72-77, 1989.
 ISSN 01489267, 15315169.
[&nbsp;<a href="OMR-Related_bib.html#Katayose1989a">bib</a>&nbsp;| 
<a href="http://www.jstor.org/stable/3679555">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Keil2017">Keil2017</a>]
</td>
<td class="bibtexitem">
Klaus Keil and Jennifer&nbsp;A. Ward.
 Applications of RISM data in digital libraries and digital
  musicology.
 <em>International Journal on Digital Libraries</em>, 2017.
 ISSN 1432-1300.
[&nbsp;<a href="OMR-Related_bib.html#Keil2017">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s00799-016-0205-3">DOI</a>&nbsp;| 
<a href="https://doi.org/10.1007/s00799-016-0205-3">http</a>&nbsp;]
<blockquote><font size="-1">
Information about manuscripts and printed music indexed in RISM (R&eacute;pertoire International des Sources Musicales), a large, international project that records and describes musical sources, was for decades available solely through book publications, CD-ROMs, or subscription services. Recent initiatives to make the data available on a wider scale have resulted in, most significantly, a freely accessible online database and the availability of its data as open data and linked open data. Previously, the task of increasing the amount of data was primarily carried out by RISM national groups and the Zentralredaktion (Central Office). The current opportunities available by linking to other freely accessible databases and importing data from other resources open new perspectives and prospects. This paper describes the RISM data and their applications for digital libraries and digital musicological projects. We discuss the possibilities and challenges in making available a large and growing quantity of data and how the data have been utilized in external library and musicological projects. Interactive functions in the RISM OPAC are planned for the future, as is closer collaboration with the projects that use RISM data. Ultimately, RISM would like to arrange a &ldquo;take and give&rdquo; system in which the RISM data are used in external projects, enhanced by the project participants, and then delivered back to the RISM Zentralredaktion.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Kingma2014">Kingma2014</a>]
</td>
<td class="bibtexitem">
Diederik&nbsp;P. Kingma and Jimmy&nbsp;Lei Ba.
 Adam: A Method for Stochastic Optimization.
 <em>Computing Research Repository</em>, abs/1412.6980, 2014.
[&nbsp;<a href="OMR-Related_bib.html#Kingma2014">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1412.6980">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Koehn2005">Koehn2005</a>]
</td>
<td class="bibtexitem">
Philipp Koehn.
 Europarl: A parallel corpus for statistical machine translation.
 In <em>MT summit</em>, pages 79-86, 2005.
[&nbsp;<a href="OMR-Related_bib.html#Koehn2005">bib</a>&nbsp;| 
<a href="http://courses.washington.edu/ling473/Project5.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Kurth2007">Kurth2007</a>]
</td>
<td class="bibtexitem">
Frank Kurth, Meinard M&uuml;ller, Christian Fremerey, Yoon-ha Chang, and
  Michael Clausen.
 Automated synchronization of scanned sheet music with audio
  recordings.
 In <em>8th International Conference on Music Information
  Retrieval</em>, pages 261-266, Vienna, Austria, 2007.
 ISBN 978-3-85403-218.
[&nbsp;<a href="OMR-Related_bib.html#Kurth2007">bib</a>&nbsp;| 
<a href="http://ismir2007.ismir.net/proceedings/ISMIR2007_p261_kurth.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Lake2013">Lake2013</a>]
</td>
<td class="bibtexitem">
Brenden&nbsp;M. Lake, Ruslan Salakhutdinov, and Joshua&nbsp;B. Tenenbaum.
 One-shot learning by inverting a compositional causal process.
 In C.J.C. Burges, L.&nbsp;Bottou, M.&nbsp;Welling, Z.&nbsp;Ghahramani, and K.Q.
  Weinberger, editors, <em>Advances in Neural Information Processing Systems
  26</em>, pages 2526-2534. Curran Associates, Inc., 2013.
[&nbsp;<a href="OMR-Related_bib.html#Lake2013">bib</a>&nbsp;| 
<a href="http://papers.nips.cc/paper/5128-one-shot-learning-by-inverting-a-compositional-causal-process.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Lavie2007">Lavie2007</a>]
</td>
<td class="bibtexitem">
Alon Lavie and Abhaya Agarwal.
 Meteor: An Automatic Metric for MT Evaluation with High Levels of
  Correlation with Human Judgments.
 In <em>Second Workshop on Statistical Machine Translation</em>, pages
  228-231, Stroudsburg, PA, USA, 2007. Association for Computational
  Linguistics.
[&nbsp;<a href="OMR-Related_bib.html#Lavie2007">bib</a>&nbsp;| 
<a href="http://dl.acm.org/citation.cfm?id=1626355.1626389">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="LeCun1998">LeCun1998</a>]
</td>
<td class="bibtexitem">
Yann LeCun, L&eacute;on Bottou, Yoshua Bengio, and Patrick Haffner.
 Gradient-based learning applied to document recognition.
 <em>Proceedings of the IEEE</em>, 86 (11): 2278-2324, 1998.
 ISSN 0018-9219.
[&nbsp;<a href="OMR-Related_bib.html#LeCun1998">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/5.726791">DOI</a>&nbsp;]
<blockquote><font size="-1">
Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="LeCun2015">LeCun2015</a>]
</td>
<td class="bibtexitem">
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
 Deep learning.
 <em>Nature</em>, 521 (7553): 436-444, 2015.
 ISSN 1476-4687.
[&nbsp;<a href="OMR-Related_bib.html#LeCun2015">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1038/nature14539">DOI</a>&nbsp;| 
<a href="https://doi.org/10.1038/nature14539">http</a>&nbsp;]
<blockquote><font size="-1">
Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Lee2016">Lee2016</a>]
</td>
<td class="bibtexitem">
Chungkeun Lee, H.&nbsp;Jin Kim, and Kyeong&nbsp;Won Oh.
 Comparison of faster R-CNN models for object detection.
 In <em>2016 16th International Conference on Control, Automation
  and Systems (ICCAS)</em>, pages 107-110, 2016.
[&nbsp;<a href="OMR-Related_bib.html#Lee2016">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICCAS.2016.7832305">DOI</a>&nbsp;]
<blockquote><font size="-1">
Object detection is one of the important problems for autonomous robots.
	Faster R-CNN, one of the state-of-the-art object detection methods,
	approaches real time application; nevertheless, computational time
	lies borderline of real time application, i.e. 5fps with VGG16 model
	in K40 GPU system in [1]. Moreover, computation time depends on model
	and image crop size, but precision is also affected; usually, time
	and precision have trade-off relation. By adjusting input image size
	in spite of downgrading performance, computation time meets criteria
	for one model. Therefore, selection of a model is one of the important
	problems when faster R-CNN based object detection system for an autonomous
	robot is constructed. In this paper, we convert several state-of-the-art
	models from convolution neural network (CNN) for image classification.
	Then, we compare converted models with several image crop size in
	terms of computation time and detection precision. We will utilize
	those comparison data for selecting a proper detection model in case
	a robot needs to perform an object detection task.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Lewis2004">Lewis2004</a>]
</td>
<td class="bibtexitem">
David&nbsp;D. Lewis, Yiming Yang, Tony&nbsp;G. Rose, and Fan Li.
 Rcv1: A new benchmark collection for text categorization research.
 <em>The Journal of Machine Learning Research</em>, 5: 361-397, 2004.
[&nbsp;<a href="OMR-Related_bib.html#Lewis2004">bib</a>&nbsp;| 
<a href="http://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Lilypond2014">Lilypond2014</a>]
</td>
<td class="bibtexitem">
The LilyPond Developement Team.
 LilyPond - Essay on automated music engraving, 2014.
[&nbsp;<a href="OMR-Related_bib.html#Lilypond2014">bib</a>&nbsp;| 
<a href="http://www.lilypond.org/">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Lin2014">Lin2014</a>]
</td>
<td class="bibtexitem">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll&aacute;r, and C.&nbsp;Lawrence Zitnick.
 Microsoft COCO: Common Objects in Context.
 In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars,
  editors, <em>Computer Vision - ECCV 2014</em>, pages 740-755, Cham, 2014.
  Springer International Publishing.
 ISBN 978-3-319-10602-1.
[&nbsp;<a href="OMR-Related_bib.html#Lin2014">bib</a>&nbsp;| 
<a href="https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48">http</a>&nbsp;]
<blockquote><font size="-1">
We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Lin2017">Lin2017</a>]
</td>
<td class="bibtexitem">
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll&aacute;r.
 Focal Loss for Dense Object Detection.
 <em>CoRR</em>, abs/1708.02002, 2017b.
[&nbsp;<a href="OMR-Related_bib.html#Lin2017">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1708.02002">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Lin2017a">Lin2017a</a>]
</td>
<td class="bibtexitem">
Tsung-Yi Lin, Piotr Doll&aacute;r, Ross Girshick, Kaiming He, Bharath Hariharan,
  and Serge Belongie.
 Feature Pyramid Networks for Object Detection.
 In <em>The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)</em>, 2017a.
[&nbsp;<a href="OMR-Related_bib.html#Lin2017a">bib</a>&nbsp;| 
<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Liu2016">Liu2016</a>]
</td>
<td class="bibtexitem">
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
  Cheng-Yang Fu, and Alexander&nbsp;C. Berg.
 SSD: Single Shot MultiBox Detector.
 In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors,
  <em>Computer Vision - ECCV 2016</em>, pages 21-37, Cham, 2016. Springer
  International Publishing.
 ISBN 978-3-319-46448-0.
[&nbsp;<a href="OMR-Related_bib.html#Liu2016">bib</a>&nbsp;| 
<a href="https://link.springer.com/chapter/10.1007%2F978-3-319-46448-0_2">http</a>&nbsp;]
<blockquote><font size="-1">
We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300x300 input, SSD achieves 74.3% mAP on VOC2007 test at 59Â FPS on a Nvidia Titan X and for 512x512 input, SSD achieves 76.9% mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Lopez2019">Lopez2019</a>]
</td>
<td class="bibtexitem">
N&eacute;stor&nbsp;N&aacute;poles L&oacute;pez, Gabriel Vigliensoni, and Ichiro
  Fujinaga.
 The effects of translation between symbolic music formats: A case
  study with Humdrum, Lilypond, MEI, and MusicXML.
 In <em>Music Encoding Conference 2019</em>, Vienna, Austria, 2019.
[&nbsp;<a href="OMR-Related_bib.html#Lopez2019">bib</a>&nbsp;| 
<a href="https://music-encoding.org/conference/2019/abstracts_mec2019/The%20effects%20of%20translation%20between%20the%20Humdrum%20%20Lilypond%20%20MEI%20%20and%20MusicXML.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Machacek2014">Machacek2014</a>]
</td>
<td class="bibtexitem">
Matou&scaron; Mach&aacute;&#x010D;ek and Ond&#X0159;ej Bojar.
 Results of the WMT14 Metrics Shared Task.
 <em>Ninth Workshop on Statistical Machine Translation</em>, pages
  293-301, 2014.
[&nbsp;<a href="OMR-Related_bib.html#Machacek2014">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Machacek2015">Machacek2015</a>]
</td>
<td class="bibtexitem">
Matou&scaron; Mach&aacute;&#x010D;ek and Ond&#X0159;ej Bojar.
 Evaluating Machine Translation Quality Using Short Segments
  Annotations.
 <em>The Prague Bulletin of Mathematical Linguistics</em>, 103 (1),
  2015.
[&nbsp;<a href="OMR-Related_bib.html#Machacek2015">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1515/pralin-2015-0005">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Mandt2017">Mandt2017</a>]
</td>
<td class="bibtexitem">
Stephan Mandt, Matthew&nbsp;D. Hoffman, and David&nbsp;M. Blei.
 Stochastic Gradient Descent as Approximate Bayesian Inference.
 <em>ArXiv e-prints</em>, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Mandt2017">bib</a>&nbsp;| 
<a href="https://arxiv.org/abs/1704.04289">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Manning2008">Manning2008</a>]
</td>
<td class="bibtexitem">
Chirstopher&nbsp;D. Manning, Prabhakar Raghavan, and Hinrich Sch&uuml;tze.
 <em>Introduction to Information Retrieval</em>.
 Cambridge University Press, 2008.
 ISBN 978-0-521-86571-5.
[&nbsp;<a href="OMR-Related_bib.html#Manning2008">bib</a>&nbsp;| 
<a href="https://nlp.stanford.edu/IR-book/">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Marr2010">Marr2010</a>]
</td>
<td class="bibtexitem">
David Marr.
 <em>Vision</em>.
 W.H. Freeman and Company San Francisco, 2010.
[&nbsp;<a href="OMR-Related_bib.html#Marr2010">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.7551/mitpress/9780262514620.001.0001">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Marsden2012">Marsden2012</a>]
</td>
<td class="bibtexitem">
Alan Marsden.
 Interrogating Melodic Similarity: A Definitive Phenomenon or the
  Product of Interpretation?
 <em>Journal of New Music Research</em>, 41 (4): 323-335, 2012.
[&nbsp;<a href="OMR-Related_bib.html#Marsden2012">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Mattheson1739">Mattheson1739</a>]
</td>
<td class="bibtexitem">
Johann Mattheson.
 <em>Der vollkommene Capellmeister</em>.
 Herold, Christian, Hamburg, 1739.
 ISBN 9783761814130.
[&nbsp;<a href="OMR-Related_bib.html#Mattheson1739">bib</a>&nbsp;| 
<a href="https://imslp.org/wiki/Der_vollkommene_Capellmeister_(Mattheson,_Johann)">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="McKay2004">McKay2004</a>]
</td>
<td class="bibtexitem">
Cory McKay and Ichiro Fujinaga.
 Automatic genre classification using large high-level musical feature
  sets.
 In <em>5th International Conference on Music Information
  Retrieval</em>, Barcelona, Spain, 2004.
[&nbsp;<a href="OMR-Related_bib.html#McKay2004">bib</a>&nbsp;| 
<a href="http://ismir2004.ismir.net/proceedings/p095-page-525-paper240.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="McKay2006">McKay2006</a>]
</td>
<td class="bibtexitem">
Cory McKay and Ichiro Fujinaga.
 jSymbolic: A feature extractor for MIDI files.
 In <em>International Computer Music Conference</em>, pages 302-305,
  New Orleans, LA, 2006.
[&nbsp;<a href="OMR-Related_bib.html#McKay2006">bib</a>&nbsp;| 
<a href="https://www.music.mcgill.ca/~cmckay/papers/musictech/McKay_ICMC_06_jSymbolic.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="MerchanSanchez-Jara2015">MerchanSanchez-Jara2015</a>]
</td>
<td class="bibtexitem">
Javier Merch&aacute;n S&aacute;nchez-Jara.
 e-Score; Impact, Perception and Uses in Music Educational
  Institutions.
 In Alves&nbsp;G.R. Felgueiras&nbsp;M.C., editor, <em>3rd International
  Conference on Technological Ecosystems for Enhancing Multiculturality</em>, pages
  449-454. Association for Computing Machinery, 2015.
 ISBN 9781450334426.
[&nbsp;<a href="OMR-Related_bib.html#MerchanSanchez-Jara2015">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/2808580.2808647">DOI</a>&nbsp;]
<blockquote><font size="-1">
Parallel to the very first music expressions as a human activity,
	developed under some kind of guidelines and precepts, there have
	arisen relentless efforts and attempts to achieve the long-lasting
	and stable representation of the musical products being created,
	in a way that would allow to recreate them as closely as posible
	to the original event.from the very beginning until the present days,
	have been many notation systems, formats, and expressions developed
	with this purpose in mind. With the incursion and implementation
	of the new digital technologies in the musical activity spheres,
	a new paradigm emerge in the representation and transmission of the
	musical work, culminating nowadays with the born of a new music document
	typology tha may be called digital music score or e-Score. The aim
	of this paper is lay down and approach to this new type of object
	for music texts reading, editing and transmission through a quantitive
	research, being the scope of the study to analyze and understand
	how electronic music scores (e-Score) are received, used and perceived
	by music students and teachers in any of the music learning centers
	in our country. The main purpose is to provide scientific evidence,
	through statistical methods, of the level of implementation and use
	of these new support type for music knowledge transmission, as well
	as the most influential aspects implied in perception of therm amongst
	musical community in Spain.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Mirex2019ScoreFollowing">Mirex2019ScoreFollowing</a>]
</td>
<td class="bibtexitem">
Yun Hao.
 Real-time Audio to Score Alignment (a.k.a Score Following).
 
  <a href="https://www.music-ir.org/mirex/wiki/2019:Real-time_Audio_to_Score_Alignment_(a.k.a_Score_Following)">https://www.music-ir.org/mirex/wiki/2019:Real-time_Audio_to_Score_Alignment_(a.k.a_Score_Following)</a>,
  2019.
[&nbsp;<a href="OMR-Related_bib.html#Mirex2019ScoreFollowing">bib</a>&nbsp;| 
<a href="https://www.music-ir.org/mirex/wiki/2019:Real-time_Audio_to_Score_Alignment_(a.k.a_Score_Following)">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Mnih2014">Mnih2014</a>]
</td>
<td class="bibtexitem">
Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu.
 Recurrent Models of Visual Attention.
 <em>Computing Research Repository</em>, abs/1406.6247, 2014.
[&nbsp;<a href="OMR-Related_bib.html#Mnih2014">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1406.6247">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Muge2000">Muge2000</a>]
</td>
<td class="bibtexitem">
F.&nbsp;Muge, I.&nbsp;Granado, M.&nbsp;Mengucci, P.&nbsp;Pina, V.&nbsp;Ramos, N.&nbsp;Sirakov, J.&nbsp;R.
  Caldas&nbsp;Pinto, A.&nbsp;Marcolino, M&aacute;rio Ramalho, P.&nbsp;Vieira, and A.&nbsp;Maia&nbsp;do
  Amaral.
 Automatic Feature Extraction and Recognition for Digital Access of
  Books of the Renaissance.
 In Jos&eacute; Borbinha and Thomas Baker, editors, <em>Research and
  Advanced Technology for Digital Libraries</em>, pages 1-13, Berlin, Heidelberg,
  2000. Springer Berlin Heidelberg.
 ISBN 978-3-540-45268-3.
[&nbsp;<a href="OMR-Related_bib.html#Muge2000">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/3-540-45268-0_1">DOI</a>&nbsp;]
<blockquote><font size="-1">
Antique printed books constitute a heritage that should be preserved and used. With novel digitising techniques is now possible to have these books stored in digital format and accessible to a wider public. However it remains the problem of how to use them. DEBORA (Digital accEss to BOoks of the RenAissance) is a European project that aims to develop a system to interact with these books through world-wide networks. The main issue is to build a database accessible through client computers. That will require to built accompanying metadata that should characterise different components of the books as illuminated letters, banners, figures and key words in order to simplify and speed up the remote access. To solve these problems, digital image analysis algorithms regarding filtering, segmentation, separation of text from non-text, lines and word segmentation and word recognition were developed. Some novel ideas are presented and illustrated through examples.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Ng2008">Ng2008</a>]
</td>
<td class="bibtexitem">
Kia Ng and Paolo Nesi.
 <em>Interactive Multimedia Music Technologies</em>.
 IGI Global, 2008.
 ISBN 1599041529.
[&nbsp;<a href="OMR-Related_bib.html#Ng2008">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.4018/978-1-59904-150-6">DOI</a>&nbsp;]
<blockquote><font size="-1">
Many multimedia music content owners and distributors are converting their archives of music scores from paper into digital images, and to machine readable symbolic notation in order to survive in the business world. Interactive Multimedia Music Technologies discusses relevant state-of-the-art technologies and consists of analysis, knowledge, and application scenarios as surveyed, analyzed, and evaluated by industry professionals. Interactive Multimedia Music Technologies exemplifies the newest functionalities of multimedia interactive music to be used for valorizing cultural heritage, content and archives that are not currently distributed due to lack of safety, suitable coding models, and conversion technologies. Interactive Multimedia Music Technologies explains new and innovative methods of promoting music and products for entertainment, distance teaching, valorizing archives, and commercial and non-commercial purposes, and provides new services for those connected via personal computers, mobile and other devices, for both sighted and print-impaired consumers.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Nielsen2009">Nielsen2009</a>]
</td>
<td class="bibtexitem">
Johan Nielsen.
 Statistical Analysis of Music Corpora.
 Technical report, Dept. of Computer Science, University of
  Copenhagen, Copenhagen, Denmark, 2009.
[&nbsp;<a href="OMR-Related_bib.html#Nielsen2009">bib</a>&nbsp;| 
<a href="https://johanbrinch.com/static/papers/brinchj-2008_music.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Nivre2005">Nivre2005</a>]
</td>
<td class="bibtexitem">
Joakim Nivre and Jens Nilsson.
 Pseudo-projective Dependency Parsing.
 In <em>43rd Annual Meeting on Association for Computational
  Linguistics</em>, pages 99-106, Stroudsburg, PA, USA, 2005. Association for
  Computational Linguistics.
[&nbsp;<a href="OMR-Related_bib.html#Nivre2005">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.3115/1219840.1219853">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Ntirogiannis2013">Ntirogiannis2013</a>]
</td>
<td class="bibtexitem">
Konstantinos Ntirogiannis, Basilis Gatos, and Ioannis Pratikakis.
 Performance evaluation methodology for historical document image
  binarization.
 <em>IEEE Transactions on Image Processing</em>, 22 (2): 595-609, 2013.
 ISSN 1057-7149.
[&nbsp;<a href="OMR-Related_bib.html#Ntirogiannis2013">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/TIP.2012.2219550">DOI</a>&nbsp;]
<blockquote><font size="-1">
Document image binarization is of great importance in the document image analysis and recognition pipeline since it affects further stages of the recognition process. The evaluation of a binarization method aids in studying its algorithmic behavior, as well as verifying its effectiveness, by providing qualitative and quantitative indication of its performance. This paper addresses a pixel-based binarization evaluation methodology for historical handwritten/machine-printed document images. In the proposed evaluation scheme, the recall and precision evaluation measures are properly modified using a weighting scheme that diminishes any potential evaluation bias. Additional performance metrics of the proposed evaluation scheme consist of the percentage rates of broken and missed text, false alarms, background noise, character enlargement, and merging. Several experiments conducted in comparison with other pixel-based evaluation measures demonstrate the validity of the proposed evaluation scheme.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Palmer1999">Palmer1999</a>]
</td>
<td class="bibtexitem">
Stephen&nbsp;E. Palmer.
 <em>Vision science: Photons to phenomenology</em>.
 MIT press, 1999.
[&nbsp;<a href="OMR-Related_bib.html#Palmer1999">bib</a>&nbsp;| 
<a href="https://mitpress.mit.edu/books/vision-science">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Papineni2002">Papineni2002</a>]
</td>
<td class="bibtexitem">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
 BLEU: A Method for Automatic Evaluation of Machine Translation.
 In <em>40th Annual Meeting on Association for Computational
  Linguistics</em>, pages 311-318, Stroudsburg, PA, USA, 2002. Association for
  Computational Linguistics.
[&nbsp;<a href="OMR-Related_bib.html#Papineni2002">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.3115/1073083.1073135">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Phon-Amnuaisuk2009">Phon-Amnuaisuk2009</a>]
</td>
<td class="bibtexitem">
Somnuk Phon-Amnuaisuk.
 Estimating HMM Parameters Using Particle Swarm Optimisation.
 In Mario Giacobini, Anthony Brabazon, Stefano Cagnoni, Gianni&nbsp;A.
  Di&nbsp;Caro, Anik&oacute; Ek&aacute;rt, Anna&nbsp;Isabel Esparcia-Alc&aacute;zar, Muddassar
  Farooq, Andreas Fink, and Penousal Machado, editors, <em>Applications of
  Evolutionary Computing</em>, pages 625-634, Berlin, Heidelberg, 2009. Springer
  Berlin Heidelberg.
 ISBN 978-3-642-01129-0.
[&nbsp;<a href="OMR-Related_bib.html#Phon-Amnuaisuk2009">bib</a>&nbsp;]
<blockquote><font size="-1">
A Hidden Markov Model (HMM) is a powerful model in describing temporal sequences. The HMM parameters are usually estimated using Baum-Welch algorithm. However, it is well known that the Baum-Welch algorithm tends to arrive at local optimal points. In this report, we investigate the potential of the Particle Swarm Optimisation (PSO) as an alternative method for HMM parameters estimation. The domain in this study is the recognition of handwritten music notations. Three observables: (i) sequence of ink patterns, (ii) stroke information and (iii) spatial information associated with eight musical symbols were recorded. Sixteen HMM models were built from the data. Eight HMM models for eight musical symbols were built from the parameters estimated using the Baum-Welch algorithm and the other eight models were built from the parameters estimated using PSO. The experiment shows that the performances of HMM models, using parameters estimated from PSO and Baum-Welch approach, are comparable. We suggest that PSO or a combination of PSO and Baum-Welch algorithm could be alternative approaches for the HMM parameters estimation.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Presgurvic2005">Presgurvic2005</a>]
</td>
<td class="bibtexitem">
G&eacute;rard Presgurvic.
 Songbook Romeo &amp; Julia, 2005.
[&nbsp;<a href="OMR-Related_bib.html#Presgurvic2005">bib</a>&nbsp;| 
<a href="https://www.musicalvienna.at/de/souvenirs/12/ANDERE-MUSICALS/10/Songbook-Romeo-und-Julia">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Rashid2017">Rashid2017</a>]
</td>
<td class="bibtexitem">
Sheikh&nbsp;Faisal Rashid, Abdullah Akmal, Muhammad Adnan, Ali&nbsp;Adnan Aslam, and
  Andreas Dengel.
 Table Recognition in Heterogeneous Documents Using Machine Learning.
 In <em>2017 14th IAPR International Conference on Document Analysis
  and Recognition (ICDAR)</em>, pages 777-782, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Rashid2017">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICDAR.2017.132">DOI</a>&nbsp;]
<blockquote><font size="-1">
Tables are an easy way to represent information in a structural form. Table recognition is important for the extraction of such information from document images. Usually, modern OCR systems provide textual information coming from tables without recognizing actual table structure. However, recognition of table structure is important to get the contextual meaning of the contents. Table structure recognition in heterogeneous documents is challenging due to a variety of table layouts. It becomes harder where no physical rulings are present in a table. This work proposes a novel learning based methodology for the recognition of table contents in heterogeneous document images. Textual contents of documents are classified as table or non-table elements using a pre-trained neural network model. The output of the neural network is further enhanced by applying a contextual post processing on each element to correct the classifications errors if any. The system is trained using a subset of UNLV and UW3 document images and depicted more than 97% accuracy on a test set in detection of table and non-table elements.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Redmon2017">Redmon2017</a>]
</td>
<td class="bibtexitem">
Joseph Redmon and Ali Farhadi.
 YOLO9000: Better, Faster, Stronger.
 In <em>2017 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)</em>, pages 6517-6525, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Redmon2017">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/CVPR.2017.690">DOI</a>&nbsp;]
<blockquote><font size="-1">
We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that dont have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Ren2015">Ren2015</a>]
</td>
<td class="bibtexitem">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
 Faster R-CNN: Towards Real-Time Object Detection with Region
  Proposal Networks.
 In C.&nbsp;Cortes, N.&nbsp;D. Lawrence, D.&nbsp;D. Lee, M.&nbsp;Sugiyama, and R.&nbsp;Garnett,
  editors, <em>Advances in Neural Information Processing Systems 28</em>, pages
  91-99. Curran Associates, Inc., 2015.
[&nbsp;<a href="OMR-Related_bib.html#Ren2015">bib</a>&nbsp;| 
<a href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Rettinghaus2021">Rettinghaus2021</a>]
</td>
<td class="bibtexitem">
Klaus Rettinghaus and Alexander Pacha.
 Building a Comprehensive Sheet Music Library Application.
 In Stefan M&uuml;nnich and David Rizo, editors, <em>Music Encoding
  Conference Proceedings 2021</em>, University of Alicante, Spain, 2021.
[&nbsp;<a href="OMR-Related_bib.html#Rettinghaus2021">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.17613/fc1c-mx52">DOI</a>&nbsp;| 
<a href="https://hcommons.org/deposits/item/hc:45961/">http</a>&nbsp;]
<blockquote><font size="-1">
Digital symbolic music scores offer many benefits compared to paper-based scores, such as a flexible dynamic layout that allows adjustments of size and style, intelligent navigation features, automatic page-turning, onthe-fly modifications of the score including transposition into a different key, and rule-based annotations that can save hours of manual work by automatically highlighting relevant aspects in the score. However, most musicians still rely on paper because they donâ€™t have access to a digital version of their sheet music, or their digital solution does not provide a satisfying experience. To bring digital scores to millions of musicians, we at Enote are building a mobile application that offers a comprehensive digital library of sheet music. These scores are obtained by a large-scale Optical Music Recognition process, combined with metadata collection and curation. Our material is stored in the MEI format and we rely on Verovio as a central component of our app to present scores and parts dynamically on mobile devices. This combination of the expressiveness of MEI with the beautiful engraving of Verovio allows us to create a flexible, mobile solution that we believe to be a powerful and true alternative to paper scores with practical features like smart annotations or instant transpositions. We also invest heavily into the open-source development of Verovio to make it the gold standard for rendering beautiful digital sheet music.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Roman2018">Roman2018</a>]
</td>
<td class="bibtexitem">
Miguel&nbsp;A. Rom&aacute;n, Antonio Pertusa, and Jorge Calvo-Zaragoza.
 And End-To-End Framework for Audio-to-Score Music Transcription on
  Monophonic Excerpts.
 In <em>19th International Society for Music Information Retrieval
  Conference</em>, pages 34-41, Paris, France, 2018.
 ISBN 978-2-9540351-2-3.
[&nbsp;<a href="OMR-Related_bib.html#Roman2018">bib</a>&nbsp;| 
<a href="http://ismir2018.ircam.fr/doc/pdfs/87_Paper.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Ronneberger2015">Ronneberger2015</a>]
</td>
<td class="bibtexitem">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
 U-Net: Convolutional Networks for Biomedical Image Segmentation.
 In Nassir Navab, Joachim Hornegger, William&nbsp;M. Wells, and
  Alejandro&nbsp;F. Frangi, editors, <em>18th International Conference on Medical
  Image Computing and Computer-Assisted Intervention (MICCAI)</em>, pages 234-241,
  Munich, Germany, 2015. Springer International Publishing.
 ISBN 978-3-319-24574-4.
[&nbsp;<a href="OMR-Related_bib.html#Ronneberger2015">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-319-24574-4_28">DOI</a>&nbsp;]
<blockquote><font size="-1">
There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Sampson1985">Sampson1985</a>]
</td>
<td class="bibtexitem">
Geoffrey Sampson.
 <em>Writing Systems: A Linguistic Introduction</em>.
 Stanford University Press, 1985.
 ISBN 9780804717564.
[&nbsp;<a href="OMR-Related_bib.html#Sampson1985">bib</a>&nbsp;| 
<a href="https://books.google.cz/books?id=tVcdNRvwoDkC">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Sanchez2018">Sanchez2018</a>]
</td>
<td class="bibtexitem">
Joan&nbsp;Andreu S&aacute;nchez, Ver&oacute;nica Romero, Alejandro&nbsp;H. Toselli, and
  Enrique Vidal.
 Handwritten Text Recognition Competitions With the tranScriptorium
  Dataset.
 In <em>Document Analysis and Text Recognition</em>, chapter Chapter 8,
  pages 213-239. World Scientific, 2018.
[&nbsp;<a href="OMR-Related_bib.html#Sanchez2018">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1142/9789813229273_0008">DOI</a>&nbsp;]
<blockquote><font size="-1">
The following sections are included: Introduction The Bentham Datasets Used in the HTR Contests Contest Protocol and Evaluation Methodology Existing State-of-the-Art Approaches HTR Approaches Employed in the Contests Presentation and Analysis of HTR Contest Results Remarks and Conclusions References
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Sauvola2000">Sauvola2000</a>]
</td>
<td class="bibtexitem">
J.&nbsp;Sauvola and M.&nbsp;Pietik&auml;inen.
 Adaptive document image binarization.
 <em>Pattern Recognition</em>, 33 (2): 225-236, 2000.
 ISSN 0031-3203.
[&nbsp;<a href="OMR-Related_bib.html#Sauvola2000">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/S0031-3203(99)00055-2">DOI</a>&nbsp;]
<blockquote><font size="-1">
A new method is presented for adaptive document image binarization, where the page is considered as a collection of subcomponents such as text, background and picture. The problems caused by noise, illumination and many source type-related degradations are addressed. Two new algorithms are applied to determine a local threshold for each pixel. The performance evaluation of the algorithm utilizes test images with ground-truth, evaluation metrics for binarization of textual and synthetic images, and a weight-based ranking procedure for the final result presentation. The proposed algorithms were tested with images including different types of document components and degradations. The results were compared with a number of known techniques in the literature. The benchmarking results show that the method adapts and performs well in each case qualitatively and quantitatively.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Saxena2017">Saxena2017</a>]
</td>
<td class="bibtexitem">
Lalit&nbsp;Prakash Saxena.
 Niblack's binarization method and its modifications to real-time
  applications: a review.
 <em>Artificial Intelligence Review</em>, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Saxena2017">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s10462-017-9574-2">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Schmidhuber2015">Schmidhuber2015</a>]
</td>
<td class="bibtexitem">
J&uuml;rgen Schmidhuber.
 Deep learning in neural networks: An overview.
 <em>Neural networks</em>, 61: 85-117, 2015.
[&nbsp;<a href="OMR-Related_bib.html#Schmidhuber2015">bib</a>&nbsp;| 
<a href="https://arxiv.org/pdf/1404.7828.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Schobrun2005">Schobrun2005</a>]
</td>
<td class="bibtexitem">
Marc Schobrun.
 <em>The Everything Reading Music Book: A Step-By-Step Introduction
  to Understanding Music Notation And Theory</em>.
 Everything series. Adams Media, 2005.
 ISBN 1-59337-324-4.
[&nbsp;<a href="OMR-Related_bib.html#Schobrun2005">bib</a>&nbsp;]
<blockquote><font size="-1">
This easy-to-follow guide helps you master the fundamentals of music notation and theory in no time.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Selfridge-Field1997">Selfridge-Field1997</a>]
</td>
<td class="bibtexitem">
Eleanor Selfridge-Field.
 <em>Beyond MIDI: The Handbook of Musical Codes</em>.
 MIT Press, Cambridge, MA, USA, 1997.
 ISBN 0-262-19394-9.
[&nbsp;<a href="OMR-Related_bib.html#Selfridge-Field1997">bib</a>&nbsp;| 
<a href="https://mitpress.mit.edu/books/beyond-midi">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Sermanet2013">Sermanet2013</a>]
</td>
<td class="bibtexitem">
Pierre Sermanet, David Eigen, Xiang Zhang, Micha&euml;l Mathieu, Rob Fergus, and
  Yann LeCun.
 Overfeat: Integrated recognition, localization and detection using
  convolutional networks.
 <em>CoRR</em>, abs/1312.6229, 2013.
[&nbsp;<a href="OMR-Related_bib.html#Sermanet2013">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1312.6229">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Serra2017">Serra2017</a>]
</td>
<td class="bibtexitem">
Xavier Serra.
 The computational study of a musical culture through its digital
  traces.
 <em>Acta Musicologica. 2017; 89 (1): 24-44.</em>, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Serra2017">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Shahab2010">Shahab2010</a>]
</td>
<td class="bibtexitem">
Asif Shahab, Faisal Shafait, Thomas Kieninger, and Andreas Dengel.
 An Open Approach Towards the Benchmarking of Table Structure
  Recognition Systems.
 In <em>9th International Workshop on Document Analysis Systems</em>,
  pages 113-120, Boston, Massachusetts, USA, 2010. ACM.
 ISBN 978-1-60558-773-8.
[&nbsp;<a href="OMR-Related_bib.html#Shahab2010">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/1815330.1815345">DOI</a>&nbsp;| 
<a href="http://doi.acm.org/10.1145/1815330.1815345">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Siagian2007">Siagian2007</a>]
</td>
<td class="bibtexitem">
Christian Siagian and Laurent Itti.
 Rapid Biologically-Inspired Scene Classification Using Features
  Shared with Visual Attention.
 <em>IEEE Transactions on Pattern Analysis and Machine
  Intelligence</em>, 29 (2): 300-312, 2007.
 ISSN 0162-8828.
[&nbsp;<a href="OMR-Related_bib.html#Siagian2007">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/TPAMI.2007.40">DOI</a>&nbsp;]
<blockquote><font size="-1">
We describe and validate a simple context-based scene recognition
	algorithm for mobile robotics applications. The system can differentiate
	outdoor scenes from various sites on a college campus using a multiscale
	set of early-visual features, which capture the "gist" of the scene
	into a low-dimensional signature vector. Distinct from previous approaches,
	the algorithm presents the advantage of being biologically plausible
	and of having low-computational complexity, sharing its low-level
	features with a model for visual attention that may operate concurrently
	on a robot. We compare classification accuracy using scenes filmed
	at three outdoor sites on campus (13,965 to 34,711 frames per site).
	Dividing each site into nine segments, we obtain segment classification
	rates between 84.21 percent and 88.62 percent. Combining scenes from
	all sites (75,073 frames in total) yields 86.45 percent correct classification,
	demonstrating the generalization and scalability of the approach
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Sigtia2015">Sigtia2015</a>]
</td>
<td class="bibtexitem">
Siddharth Sigtia, Emmanouil Benetos, and Simon Dixon.
 An End-to-End Neural Network for Polyphonic Piano Music
  Transcription.
 <em>Computing Research Repository</em>, abs/1508.01774, 2015.
[&nbsp;<a href="OMR-Related_bib.html#Sigtia2015">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1508.01774">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Simard2003">Simard2003</a>]
</td>
<td class="bibtexitem">
Patrice&nbsp;Y. Simard, Dave Steinkraus, and John&nbsp;C. Platt.
 Best practices for convolutional neural networks applied to visual
  document analysis.
 <em>Seventh International Conference on Document Analysis and
  Recognition</em>, pages 958-963, 2003.
[&nbsp;<a href="OMR-Related_bib.html#Simard2003">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICDAR.2003.1227801">DOI</a>&nbsp;]
<blockquote><font size="-1">
Not Available
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Simonyan2014">Simonyan2014</a>]
</td>
<td class="bibtexitem">
Karen Simonyan and Andrew Zisserman.
 Very Deep Convolutional Networks for Large-Scale Image Recognition.
 <em>Computing Research Repository</em>, abs/1409.1556, 2014.
[&nbsp;<a href="OMR-Related_bib.html#Simonyan2014">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1409.1556">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Sloboda2005">Sloboda2005</a>]
</td>
<td class="bibtexitem">
John Sloboda.
 <em>Exploring the musical mind</em>.
 Oxford University Press, 2005.
[&nbsp;<a href="OMR-Related_bib.html#Sloboda2005">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1093/acprof:oso/9780198530121.001.0001">DOI</a>&nbsp;| 
<a href="https://books.google.at/books?id=xNK6LTIW3D8C&printsec=frontcover#v=onepage&q&f=false">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="SLUB">SLUB</a>]
</td>
<td class="bibtexitem">
S&auml;chsische Landesbibliothek.
 Staats- und Universit&auml;tsbibliothek Dresden.
 <a href="https://www.slub-dresden.de">https://www.slub-dresden.de</a>, 2007.
[&nbsp;<a href="OMR-Related_bib.html#SLUB">bib</a>&nbsp;| 
<a href="https://www.slub-dresden.de">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Smith1987">Smith1987</a>]
</td>
<td class="bibtexitem">
Leland Smith.
 SCORE, 1987.
[&nbsp;<a href="OMR-Related_bib.html#Smith1987">bib</a>&nbsp;| 
<a href="https://en.wikipedia.org/wiki/SCORE_(software)">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Socher2011">Socher2011</a>]
</td>
<td class="bibtexitem">
Richard Socher, Cliff Chiung-Yu Lin, Andrew&nbsp;Y. Ng, and Christopher&nbsp;D. Manning.
 Parsing Natural Scenes and Natural Language with Recursive Neural
  Networks.
 In <em>28th International Conference on Machine Learning</em>, 2011.
[&nbsp;<a href="OMR-Related_bib.html#Socher2011">bib</a>&nbsp;| 
<a href="http://ai.stanford.edu/~ang/papers/icml11-ParsingWithRecursiveNeuralNetworks.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Sordo2015">Sordo2015</a>]
</td>
<td class="bibtexitem">
Mohamed Sordo, Mitsunori Ogihara, and Stefan Wuchty.
 Analysis of the Evolution of Research Groups and Topics in the
  ISMIR Conference.
 In <em>16th International Society for Music Information Retrieval
  Conference</em>, 2015.
 ISBN 978-84-606-8853-2.
[&nbsp;<a href="OMR-Related_bib.html#Sordo2015">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Spreadbury2015">Spreadbury2015</a>]
</td>
<td class="bibtexitem">
Daniel Spreadbury and Robert Pi&eacute;chaud.
 Standard Music Font Layout (SMuFL).
 In Marc Battier, Jean Bresson, Pierre Couprie, C&eacute;cile
  Davy-Rigaux, Dominique Fober, Yann Geslin, Hugues Genevois, Fran&ccedil;ois
  Picard, and Alice Tacaille, editors, <em>First International Conference on
  Technologies for Music Notation and Representation - TENOR2015</em>, pages
  146-153, Paris, France, 2015. Institut de Recherche en Musicologie.
 ISBN 978-2-9552905-0-7.
[&nbsp;<a href="OMR-Related_bib.html#Spreadbury2015">bib</a>&nbsp;| 
<a href="http://tenor2015.tenor-conference.org/papers/24-Spreadbury-SMuFL.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Stadelmann2018">Stadelmann2018</a>]
</td>
<td class="bibtexitem">
Thilo Stadelmann, Mohammadreza Amirian, Ismail Arabaci, Marek Arnold,
  Gilbert&nbsp;Fran&ccedil;ois Duivesteijn, Isamil Elezi, Melanie Geiger, Stefan
  L&ouml;rwald, Benjamin&nbsp;Bruno Meier, Katharina Rombach, and Lukas Tuggener.
 Deep Learning in the Wild.
 <em>ArXiv e-prints</em>, 2018.
[&nbsp;<a href="OMR-Related_bib.html#Stadelmann2018">bib</a>&nbsp;| 
<a href="https://arxiv.org/abs/1807.04950">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Stamatopoulos2013">Stamatopoulos2013</a>]
</td>
<td class="bibtexitem">
Nikolaos Stamatopoulos, Basilis Gatos, Georgios Louloudis, Umpada Pal, and
  Alireza Alaei.
 ICDAR2013 Handwriting Segmentation Contest.
 In <em>2013 12th International Conference on Document Analysis and
  Recognition</em>, 2013.
[&nbsp;<a href="OMR-Related_bib.html#Stamatopoulos2013">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICDAR.2013.283">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Stauffer2018">Stauffer2018</a>]
</td>
<td class="bibtexitem">
Michael Stauffer, Andreas Fischer, and Kaspar Riesen.
 Keyword spotting in historical handwritten documents based on graph
  matching.
 <em>Pattern Recognition</em>, 81: 240-253, 2018.
 ISSN 0031-3203.
[&nbsp;<a href="OMR-Related_bib.html#Stauffer2018">bib</a>&nbsp;| 
<a href="https://doi.org/10.1016/j.patcog.2018.04.001">DOI</a>&nbsp;| 
<a href="http://www.sciencedirect.com/science/article/pii/S0031320318301274">http</a>&nbsp;]
<blockquote><font size="-1">
In the last decades historical handwritten documents have become increasingly available in digital form. Yet, the accessibility to these documents with respect to browsing and searching remained limited as full automatic transcription is often not possible or not sufficiently accurate. This paper proposes a novel reliable approach for template-based keyword spotting in historical handwritten documents. In particular, our framework makes use of different graph representations for segmented word images and a sophisticated matching procedure. Moreover, we extend our method to a spotting ensemble. In an exhaustive experimental evaluation on four widely used benchmark datasets we show that the proposed approach is able to keep up or even outperform several state-of-the-art methods for template- and learning-based keyword spotting.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Stone1996">Stone1996</a>]
</td>
<td class="bibtexitem">
Kurt Stone.
 <em>Music Notation in the Twentieth Century, A Practical
  Guidebook</em>.
 W. W. Norton &amp; Company, 1996.
[&nbsp;<a href="OMR-Related_bib.html#Stone1996">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Su2013">Su2013</a>]
</td>
<td class="bibtexitem">
Bolan Su, Shijian Lu, and Chew&nbsp;Lim Tan.
 Robust document image binarization technique for degraded document
  images.
 <em>IEEE Transactions on Image Processing</em>, 22 (4): 1408-1417,
  2013.
 ISSN 1057-7149.
[&nbsp;<a href="OMR-Related_bib.html#Su2013">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/TIP.2012.2231089">DOI</a>&nbsp;]
<blockquote><font size="-1">
Segmentation of text from badly degraded document images is a very challenging task due to the high inter/intra-variation between the document background and the foreground text of different document images. In this paper, we propose a novel document image binarization technique that addresses these issues by using adaptive image contrast. The adaptive image contrast is a combination of the local image contrast and the local image gradient that is tolerant to text and background variation caused by different types of document degradations. In the proposed technique, an adaptive contrast map is first constructed for an input degraded document image. The contrast map is then binarized and combined with Canny's edge map to identify the text stroke edge pixels. The document text is further segmented by a local threshold that is estimated based on the intensities of detected text stroke edge pixels within a local window. The proposed method is simple, robust, and involves minimum parameter tuning. It has been tested on three public datasets that are used in the recent document image binarization contest (DIBCO) 2009 &amp; 2011 and handwritten-DIBCO 2010 and achieves accuracies of 93.5%, 87.8%, and 92.03%, respectively, that are significantly higher than or close to that of the best-performing methods reported in the three contests. Experiments on the Bickley diary dataset that consists of several challenging bad quality document images also show the superior performance of our proposed method, compared with other techniques.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Szegedy2017">Szegedy2017</a>]
</td>
<td class="bibtexitem">
Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander&nbsp;A. Alemi.
 Inception-v4, Inception-ResNet and the Impact of Residual Connections
  on Learning.
 In <em>Thirty-First AAAI Conference on Artificial Intelligence
  (AAAI-17)</em>, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Szegedy2017">bib</a>&nbsp;| 
<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewPaper/14806">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Szeto2018">Szeto2018</a>]
</td>
<td class="bibtexitem">
Kimmy Szeto.
 The Roles of Academic Libraries in Shaping Music Publishing in the
  Digital Age.
 <em>Johns Hopkins University Press</em>, 67 (2): 303-318, 2018.
[&nbsp;<a href="OMR-Related_bib.html#Szeto2018">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1353/lib.2018.0038">DOI</a>&nbsp;]
<blockquote><font size="-1">
Libraries are positioned at the nexus of creative production, music publishing, performance, and research. The academic library community has the potential to play an influential leadership role in shaping the music publishing life cycle, making scores more readily discoverable and accessible, and establishing itself as a force that empowers a wide range of creativity and scholarship. Yet the music publishing industry has been slow to capitalize on the digital market, and academic libraries have been slow to integrate electronic music scores into their collections. In this paper, I will discuss the historical, technical, and human factors that have contributed to this moment, and the critical next steps the academic library community can take in response to the booming digital music publishing market to make a lasting impact through setting technological standards and best practices, developing education in these technologies and related intellectual property issues, and becoming an active partner in digital music publishing and in innovative research and creative possibilities.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Toiviainen2016">Toiviainen2016</a>]
</td>
<td class="bibtexitem">
P.&nbsp;Toiviainen and T.&nbsp;Eerola.
 MIDI toolbox 1.1.
 https://github.com/miditoolbox/, 2016.
[&nbsp;<a href="OMR-Related_bib.html#Toiviainen2016">bib</a>&nbsp;| 
<a href="https://github.com/miditoolbox/">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Trier1995">Trier1995</a>]
</td>
<td class="bibtexitem">
O.&nbsp;D. Trier and A.&nbsp;K. Jain.
 Goal-directed evaluation of binarization methods.
 <em>IEEE Transactions on Pattern Analysis and Machine
  Intelligence</em>, 17 (12): 1191-1201, 1995.
 ISSN 0162-8828.
[&nbsp;<a href="OMR-Related_bib.html#Trier1995">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/34.476511">DOI</a>&nbsp;]
<blockquote><font size="-1">
This paper presents a methodology for evaluation of low-level image analysis methods, using binarization (two-level thresholding) as an example. Binarization of scanned gray scale images is the first step in most document image analysis systems. Selection of an appropriate binarization method for an input image domain is a difficult problem. Typically, a human expert evaluates the binarized images according to his/her visual criteria. However, to conduct an objective evaluation, one needs to investigate how well the subsequent image analysis steps will perform on the binarized image. We call this approach goal-directed evaluation, and it can be used to evaluate other low-level image processing methods as well. Our evaluation of binarization methods is in the context of digit recognition, so we define the performance of the character recognition module as the objective measure. Eleven different locally adaptive binarization methods were evaluated, and Niblack's method gave the best performance.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Trier1995a">Trier1995a</a>]
</td>
<td class="bibtexitem">
&Oslash;ivind&nbsp;Due Trier and Torfinn Taxt.
 Evaluation of binarization methods for utility map images.
 <em>1st International Conference on Image Processing</em>, 2: 31-36,
  1995.
 ISSN 0162-8828.
[&nbsp;<a href="OMR-Related_bib.html#Trier1995a">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICIP.1994.413515">DOI</a>&nbsp;]
<blockquote><font size="-1">
This paper presents an evaluation of locally adaptive binarization methods for gray scale images with low contrast, variable background intensity and noise. Such low quality occurs frequently in utility maps and excludes the use of global binarization methods. Only robust locally adaptive binarization methods with no need for on-line tuning of the parameters were considered since the gray scale images of utility maps often consist of a billion (10&lt;sup&gt;9&lt;/sup&gt;) pixels or more. Eight locally adaptive binarization methods were tested on five different images. The postprocessing step (PS) of Yanowitz and Bruckstein's (1989) method improved all the other best binarization methods. Niblack's (1986) method with PS gave the best performance. Eikvil, Taxt and Moen's (1991) method with PS, and Yanowitz and Bruckstein's method did almost as well. Comparison was also made on the CPU requirement
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Uijlings2013">Uijlings2013</a>]
</td>
<td class="bibtexitem">
J.&nbsp;R.&nbsp;R. Uijlings, K.&nbsp;E.&nbsp;A. van&nbsp;de Sande, T.&nbsp;Gevers, and A.&nbsp;W.&nbsp;M. Smeulders.
 Selective Search for Object Recognition.
 <em>International Journal of Computer Vision</em>, 104 (2): 154-171,
  2013.
 ISSN 1573-1405.
[&nbsp;<a href="OMR-Related_bib.html#Uijlings2013">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s11263-013-0620-5">DOI</a>&nbsp;| 
<a href="https://doi.org/10.1007/s11263-013-0620-5">http</a>&nbsp;]
<blockquote><font size="-1">
This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99Â % recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software:  http://disi.unitn.it/&nbsp;uijlings/SelectiveSearch.html ).
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Urbano2010">Urbano2010</a>]
</td>
<td class="bibtexitem">
Juli&aacute;n Urbano, Juan Llor&eacute;ns, Jorge Morato, and Sonia
  S&aacute;nchez-Cuadrado.
 MIREX 2010 Symbolic Melodic Similarity: Local Alignment with
  Geometric Representations.
 Technical report, Music Information Retrieval Evaluation eXchange,
  2010.
[&nbsp;<a href="OMR-Related_bib.html#Urbano2010">bib</a>&nbsp;| 
<a href="https://julian-urbano.info/files/publications/013-mirex-2010-symbolic-melodic-similarity-local-alignment-geometric-representations.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Urbano2011">Urbano2011</a>]
</td>
<td class="bibtexitem">
Juli&aacute;n Urbano, Juan Llor&eacute;ns, Jorge Morato, and Sonia
  S&aacute;nchez-Cuadrado.
 MIREX 2011 Symbolic Melodic Similarity: Sequence Alignment with
  Geometric Representations.
 Technical report, Music Information Retrieval Evaluation eXchange,
  2011.
[&nbsp;<a href="OMR-Related_bib.html#Urbano2011">bib</a>&nbsp;| 
<a href="https://julian-urbano.info/files/publications/034-mirex-2011-symbolic-melodic-similarity-sequence-alignment-geometric-representations.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Urbano2012">Urbano2012</a>]
</td>
<td class="bibtexitem">
Juli&aacute;n Urbano, Juan Llor&eacute;ns, Jorge Morato, and Sonia
  S&aacute;nchez-Cuadrado.
 MIREX 2012 Symbolic Melodic Similarity: Hybrid Sequence Alignment
  with Geometric Representations.
 Technical report, Music Information Retrieval Evaluation eXchange,
  2012.
[&nbsp;<a href="OMR-Related_bib.html#Urbano2012">bib</a>&nbsp;| 
<a href="https://julian-urbano.info/files/publications/049-mirex-2012-symbolic-melodic-similarity-hybrid-sequence-alignment-geometric-representations.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Urbano2013">Urbano2013</a>]
</td>
<td class="bibtexitem">
Juli&aacute;n Urbano.
 MIREX 2013 Symbolic Melodic Similarity: A Geometric Model supported
  with Hybrid Sequence Alignment.
 Technical report, Music Information Retrieval Evaluation eXchange,
  2013.
[&nbsp;<a href="OMR-Related_bib.html#Urbano2013">bib</a>&nbsp;| 
<a href="http://hdl.handle.net/10230/27527">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Vajda2015">Vajda2015</a>]
</td>
<td class="bibtexitem">
Szil&aacute;rd Vajda, Yves Rangoni, and Hubert Cecotti.
 Semi-automatic ground truth generation using unsupervised clustering
  and limited manual labeling: Application to handwritten character
  recognition.
 <em>Pattern Recognition Letters</em>, 58: 23-28, 2015.
[&nbsp;<a href="OMR-Related_bib.html#Vajda2015">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.patrec.2015.02.001">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Villegas2015">Villegas2015</a>]
</td>
<td class="bibtexitem">
Mauricio Villegas, Joan&nbsp;Andreu S&aacute;nchez, and Enrique Vidal.
 Optical modelling and language modelling trade-off for Handwritten
  Text Recognition.
 In <em>2015 13th International Conference on Document Analysis and
  Recognition (ICDAR)</em>, pages 831-835, 2015.
[&nbsp;<a href="OMR-Related_bib.html#Villegas2015">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICDAR.2015.7333878">DOI</a>&nbsp;]
<blockquote><font size="-1">
Training the models needed for Automatic Handwritten Text Recognition of historical documents generally requires a significant amount of human effort. This is mainly due to the great differences that often exist between collections and to the lack of linguistic resources from the period when the documents were written, which results in a need of manual data labelling effort. This paper presents a study on the reuse of models trained with data from a different collection, focusing on the contribution that the language model and the optical models have on the performance. An empirical evaluation is performed using data from Jeremy Bentham manuscripts with the aim of recognising a manuscript about a very different topic written by Jane Austen.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Vonikakis2011">Vonikakis2011</a>]
</td>
<td class="bibtexitem">
V.&nbsp;Vonikakis, I.&nbsp;Andreadis, and N.&nbsp;Papamarkos.
 Robust document binarization with OFF center-surround cells.
 <em>Pattern Analysis and Applications</em>, 14 (3): 219-234, 2011.
 ISSN 1433-7541.
[&nbsp;<a href="OMR-Related_bib.html#Vonikakis2011">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s10044-011-0214-1">DOI</a>&nbsp;]
<blockquote><font size="-1">
This paper presents a new method for degraded-document binarization, inspired by the attributes of the Human Visual System (HVS). It can deal with various types of degradations, such as uneven illumination, shadows, low contrast, smears, and heavy noise densities. The proposed algorithm combines the characteristics of the OFF center-surround cells of the HVS with the classic Otsu binarization technique. Cells of two different scales are combined, increasing the efficiency of the algorithm and reducing the extracted noise in the final output. A new response function, which regulates the output of the cell according to the local contrast and the local lighting conditions is also introduced. The Otsu technique is used to binarize the outputs of the OFF center-surround cells. Quantitative experiments performed on a set of various computer-generated degradations, such as noise, shadow, and low contrast demonstrate the superior performance of the proposed method against six other well-established techniques. Qualitative and OCR comparisons also confirm these results.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Vos1989">Vos1989</a>]
</td>
<td class="bibtexitem">
Piet&nbsp;G. Vos and Jim&nbsp;M. Troost.
 Ascending and Descending Melodic Intervals: Statistical Findings and
  their Perceptual Relevance.
 <em>Music Perception</em>, 6 (4): 383-396, 1989.
[&nbsp;<a href="OMR-Related_bib.html#Vos1989">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.2307/40285439">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Wauthier2013">Wauthier2013</a>]
</td>
<td class="bibtexitem">
Fabian Wauthier, Michael Jordan, and Nebojsa Jojic.
 Efficient ranking from pairwise comparisons.
 In <em>30th International Conference on Machine Learning</em>, pages
  109-117, 2013.
[&nbsp;<a href="OMR-Related_bib.html#Wauthier2013">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Weigl2020">Weigl2020</a>]
</td>
<td class="bibtexitem">
David&nbsp;M. Weigl, Werner Goebl, Alex Hofmann, Tim Crawford, Federico Zubani,
  Cynthia C.&nbsp;S. Liem, and Alastair Porter.
 Read/Write Digital Libraries for Musicology.
 In <em>7th International Conference on Digital Libraries for
  Musicology</em>, DLfM 2020, pages 48-52, New York, NY, USA, 2020. Association
  for Computing Machinery.
 ISBN 9781450387606.
[&nbsp;<a href="OMR-Related_bib.html#Weigl2020">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3424911.3425519">DOI</a>&nbsp;| 
<a href="https://doi.org/10.1145/3424911.3425519">http</a>&nbsp;]
<blockquote><font size="-1">
The Web and other digital technologies have democratised music creation, reception, and analysis, putting music in the hands, ears, and minds of billions of users. Music digital libraries typically focus on an essential subset of this delugeâ€”commercial and academic publications, and historical materialsâ€”but neglect to incorporate contributions by scholars, performers, and enthusiasts, such as annotations or performed interpretations of these artifacts, despite their potential utility for many types of users. In this paper we consider means by which digital libraries for musicology may incorporate such contributions into their collections, adhering to principles of FAIR data management and respecting contributor rights as outlined in the EUâ€™s General Data Protection Regulation. We present an overview of centralised and decentralised approaches to this problem, and propose hybrid solutions in which contributions reside in a) user-controlled personal online datastores, b) decentralised file storage, and c) are published and aggregated into digital library collections. We outline the implementation of these ideas using Solid, a Web decentralisation project building on W3C standard technologies to facilitate publication and control over Linked Data. We demonstrate the feasibility of this approach by implementing prototypes supporting two types of contribution: Web Annotations describing or analysing musical elements in score encodings and music recordings; and, music performances and associated metadata supporting performance analyses across many renditions of a given piece. Finally, we situate these ideas within a wider conception of enriched, decentralised, and interconnected online music repositories.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Wilson2003">Wilson2003</a>]
</td>
<td class="bibtexitem">
D.&nbsp;Randall Wilson and Tony&nbsp;R. Martinez.
 The general inefficiency of batch training for gradient descent
  learning.
 <em>Neural Networks</em>, 16 (10): 1429-1451, 2003.
 ISSN 0893-6080.
[&nbsp;<a href="OMR-Related_bib.html#Wilson2003">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/S0893-6080(03)00138-2">DOI</a>&nbsp;| 
<a href="http://www.sciencedirect.com/science/article/pii/S0893608003001382">http</a>&nbsp;]
<blockquote><font size="-1">
Abstract Gradient descent training of neural networks can be done
	in either a batch or on-line manner. A widely held myth in the neural
	network community is that batch training is as fast or faster and/or
	more â€˜correctâ€™ than on-line training because it supposedly uses a
	better approximation of the true gradient for its weight updates.
	This paper explains why batch training is almost always slower than
	on-line trainingâ€”often orders of magnitude slowerâ€”especially on large
	training sets. The main reason is due to the ability of on-line training
	to follow curves in the error surface throughout each epoch, which
	allows it to safely use a larger learning rate and thus converge
	with less iterations through the training data. Empirical results
	on a large (20,000-instance) speech recognition task and on 26 other
	learning tasks demonstrate that convergence can be reached significantly
	faster using on-line training than batch training, with no apparent
	difference in accuracy.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Wittlich1978">Wittlich1978</a>]
</td>
<td class="bibtexitem">
Gary Wittlich, Donald Byrd, and Rosalee Nerheim.
 A system for interactive encoding of music scores under computer
  control.
 <em>Computers and the Humanities</em>, 12 (4): 309-319, 1978.
 ISSN 1572-8412.
[&nbsp;<a href="OMR-Related_bib.html#Wittlich1978">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/BF02400103">DOI</a>&nbsp;| 
<a href="https://doi.org/10.1007/BF02400103">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Wu2019">Wu2019</a>]
</td>
<td class="bibtexitem">
Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick.
 Detectron2.
 <a href="https://github.com/facebookresearch/detectron2">https://github.com/facebookresearch/detectron2</a>, 2019.
[&nbsp;<a href="OMR-Related_bib.html#Wu2019">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Xia2017">Xia2017</a>]
</td>
<td class="bibtexitem">
Gus&nbsp;G. Xia and Roger&nbsp;B. Dannenberg.
 Improvised Duet Interaction: Learning Improvisation Techniques for
  Automatic Accompaniment.
 In Cumhur Erkut, editor, <em>New Interfaces for Musical
  Expression</em>, Aalborg University Copenhagen, Denmark, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Xia2017">bib</a>&nbsp;| 
<a href="http://homes.create.aau.dk/dano/nime17/papers/0022/paper0022.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Zhai2010">Zhai2010</a>]
</td>
<td class="bibtexitem">
Yun Zhai.
 <em>Non-Numerical Ranking Based on Pairwise Comparisons</em>.
 PhD thesis, McMaster University, Hamilton, Ontario, 2010.
[&nbsp;<a href="OMR-Related_bib.html#Zhai2010">bib</a>&nbsp;| 
<a href="http://hdl.handle.net/11375/19476">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Zhai2012">Zhai2012</a>]
</td>
<td class="bibtexitem">
Ke&nbsp;Zhai, Yuening Hu, Jordan&nbsp;L. Boyd-Graber, and Sinead Williamson.
 Modeling Images using Transformed Indian Buffet Processes.
 In <em>29th International Conference on Machine Learning</em>. icml.cc
  / Omnipress, 2012.
[&nbsp;<a href="OMR-Related_bib.html#Zhai2012">bib</a>&nbsp;| 
<a href="http://icml.cc/2012/papers/750.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Zhang2016">Zhang2016</a>]
</td>
<td class="bibtexitem">
Xingxing Zhang, Liang Lu, and Mirella Lapata.
 Top-down Tree Long Short-Term Memory Networks.
 In <em>2016 Conference of the North American Chapter of the
  Association for Computational Linguistics: Human Language Technologies</em>,
  pages 310-320, San Diego, California, 2016. Association for Computational
  Linguistics.
[&nbsp;<a href="OMR-Related_bib.html#Zhang2016">bib</a>&nbsp;| 
<a href="http://www.aclweb.org/anthology/N16-1035">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Zhang2017">Zhang2017</a>]
</td>
<td class="bibtexitem">
Jianshu Zhang, Jun Du, Shiliang Zhang, Dan Liu, Yulong Hu, Jinshui Hu, Si&nbsp;Wei,
  and Lirong Dai.
 Watch, Attend and Parse: An End-to-end Neural Network Based Approach
  to Handwritten Mathematical Expression Recognition.
 <em>Pattern Recognition</em>, 2017.
 ISSN 0031-3203.
[&nbsp;<a href="OMR-Related_bib.html#Zhang2017">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.patcog.2017.06.017">DOI</a>&nbsp;| 
<a href="http://www.sciencedirect.com/science/article/pii/S0031320317302376">http</a>&nbsp;]
<blockquote><font size="-1">
Abstract Machine recognition of a handwritten mathematical expression (HME) is challenging due to the ambiguities of handwritten symbols and the two-dimensional structure of mathematical expressions. Inspired by recent work in deep learning, we present Watch, Attend and Parse (WAP), a novel end-to-end approach based on neural network that learns to recognize {HMEs} in a two-dimensional layout and outputs them as one-dimensional character sequences in LaTeX format. Inherently unlike traditional methods, our proposed model avoids problems that stem from symbol segmentation, and it does not require a predefined expression grammar. Meanwhile, the problems of symbol recognition and structural analysis are handled, respectively, using a watcher and a parser. We employ a convolutional neural network encoder that takes {HME} images as input as the watcher and employ a recurrent neural network decoder equipped with an attention mechanism as the parser to generate LaTeX sequences. Moreover, the correspondence between the input expressions and the output LaTeX sequences is learned automatically by the attention mechanism. We validate the proposed approach on a benchmark published by the {CROHME} international competition. Using the official training dataset, {WAP} significantly outperformed the state-of-the-art method with an expression recognition accuracy of 46.55% on {CROHME} 2014 and 44.55% on {CROHME} 2016.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Zitnick2014">Zitnick2014</a>]
</td>
<td class="bibtexitem">
Larry Zitnick and Piotr Dollar.
 Edge Boxes: Locating Object Proposals from Edges.
 In <em>ECCV</em>. European Conference on Computer Vision, 2014.
[&nbsp;<a href="OMR-Related_bib.html#Zitnick2014">bib</a>&nbsp;| 
<a href="https://www.microsoft.com/en-us/research/publication/edge-boxes-locating-object-proposals-from-edges/">http</a>&nbsp;]
<blockquote><font size="-1">
The use of object proposals is an effective recent approach for increasing
	the computational efficiency of object detection. We propose a novel
	method for generating object bounding box proposals using edges.
	Edges provide a sparse yet informative representation of an image.
	Our main observation is that the number of contours that are wholly
	contained in a bounding box is indicative of the likelihood of the
	box containing an object. We propose a simple box objectness score
	that measures the number of edges that exist in the box minus those
	that are members of contours that overlap the box's boundary. Using
	efficient data structures, millions of candidate boxes can be evaluated
	in a fraction of a second, returning a ranked set of a few thousand
	top-scoring proposals. Using standard metrics, we show results that
	are significantly more accurate than the current state-of-the-art
	while being faster to compute. In particular, given just 1000 proposals
	we achieve over 96	over 75	runs in 0.25 seconds and we additionally demonstrate a near real-time
	variant with only minor loss in accuracy.
</font></blockquote>
<p>
</td>
</tr>
</table><hr><p><em>This file was generated by
<a href="http://www.lri.fr/~filliatr/bibtex2html/">bibtex2html</a> 1.96.</em></p>
</body>
</html>
