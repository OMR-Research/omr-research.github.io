<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>
<head>
	<title>OMR-Research</title>
	<link rel=stylesheet type="text/css" href="css/OMR-Research.css">
</head>
<body>

<section class="page-header">
	<h1>Bibliography on Optical Music Recognition</h1>
	<p>Last updated: 10.12.2018</p>
	<a href="https://github.com/OMR-Research/omr-research.github.io" class="btn">View on GitHub</a>		
	<table class="page-header-table" id="navigation-table">
		<tr>
			<td><a href="index.html" class="btn-light">Sort by Key</a></td>		
			<td><a href="omr-research-sorted-by-year.html" class="btn-light">Sort by Year</a>	</td>			
			<td><a href="omr-research-compact.html" class="btn-light">Compact</a></td>		
			<td><a href="omr-related-research.html" class="btn-light">Related research</a></td>
			<td><a href="omr-research-unverified.html" class="btn-light">Unverified research</a></td>
		</tr>
	</table>		
</section>

<!-- This document was automatically generated with bibtex2html 1.98
     (see http://www.lri.fr/~filliatr/bibtex2html/),
     with the following command:
     BibTeX2HTML\Windows\bibtex2html.exe --use-keys --no-keywords --nodoc -a -o OMR-Related sources\OMR-Related-Research.bib  -->


<table>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Agarwal2008">Agarwal2008</a>]
</td>
<td class="bibtexitem">
Abhaya Agarwal and Alon Lavie.
 METEOR, M-BLEU and M-TER: Evaluation Metrics for
  High-correlation with Human Rankings of Machine Translation
  Output.
 In <em>Third Workshop on Statistical Machine Translation</em>, StatMT
  '08, pages 115--118, Stroudsburg, PA, USA, 2008. Association for
  Computational Linguistics.
[&nbsp;<a href="OMR-Related_bib.html#Agarwal2008">bib</a>&nbsp;| 
<a href="http://dl.acm.org/citation.cfm?id=1626394.1626406">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Akiyama1990">Akiyama1990</a>]
</td>
<td class="bibtexitem">
Teruo Akiyama and Norihiro Hagita.
 Automated entry system for printed documents.
 <em>Pattern Recognition</em>, 23(11):1141--1154, 1990.
[&nbsp;<a href="OMR-Related_bib.html#Akiyama1990">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/0031-3203(90)90112-X">DOI</a>&nbsp;| 
<a href="http://www.sciencedirect.com/science/article/pii/003132039090112X">http</a>&nbsp;]
<blockquote><font size="-1">
This paper proposes a system for automatically reading either Japanese or English documents that have complex layout structures that include graphics. First, document image segmentation and character segmentation are carried out using three basic features and the knowledge of document layout rules. Next, multi-font character recognition is performed based on feature vector matching. Recognition experiments with a prototype system for a variety of complex printed documents shows that the proposed system is capable of reading different types of printed documents at an accuracy rate of 94.8–97.2%.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Alvaro2016">Alvaro2016</a>]
</td>
<td class="bibtexitem">
Francisco &Aacute;lvaro, Joan-Andreu S&aacute;nchez, and Jos&eacute;-Miguel Bened&iacute;.
 An integrated grammar-based approach for mathematical expression
  recognition.
 <em>Pattern Recognition</em>, 51:135--147, 2016.
[&nbsp;<a href="OMR-Related_bib.html#Alvaro2016">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.patcog.2015.09.013">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Andre2014">Andre2014</a>]
</td>
<td class="bibtexitem">
Ga&euml;tan Andr&eacute;, Viviane Kostrubiec, Jean-Christophe Buisson,
  Jean-Michel Albaret, and Pier-Giorgio Zanone.
 A parsimonious oscillatory model of handwriting.
 <em>Biological Cybernetics</em>, 108(3):321--336, 2014.
[&nbsp;<a href="OMR-Related_bib.html#Andre2014">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s00422-014-0600-z">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Barate2018">Barate2018</a>]
</td>
<td class="bibtexitem">
Adriano Barat&egrave;, Goffredo Haus, and Luca&nbsp;A. Ludovico.
 Advanced experience of music through 5g technologies.
 <em>IOP Conference Series: Materials Science and Engineering</em>,
  364(1):012021, 2018.
[&nbsp;<a href="OMR-Related_bib.html#Barate2018">bib</a>&nbsp;| 
<a href="http://stacks.iop.org/1757-899X/364/i=1/a=012021">http</a>&nbsp;]
<blockquote><font size="-1">
This paper focuses on new models to enjoy music that will be implementable in a near future thanks to 5G technology. In the last two decades, our research mainly focused on the comprehensive description of music information, where multiple aspects are integrated to provide the user with an advanced multi-layer environment to experience music content. In recent times, the advancements in network technologies allowed a web implementation of this approach through W3C-compliant languages. The last obstacle to the use of personal devices is currently posed by the characteristics of mobile networks, concerning bandwidth, reliability, and the density of devices in an area. Designed to meet the requirements of future technological challenges, such as the Internet of Things and self-driving vehicles, the advent of 5G networks will solve these problems, thus paving the way also for new music-oriented applications. The possibilities described in this work range from bringing archive materials and music cultural heritage to a new life to the implementation of immersive environments for live-show remote experience.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Bezine2004">Bezine2004</a>]
</td>
<td class="bibtexitem">
H. Bezine, A.M. Alimi, and N. Sherkat.
 Generation and Analysis of Handwriting Script with the
  Beta-Elliptic Model.
 In <em>Ninth International Workshop on Frontiers in Handwriting
  Recognition</em>. Institute of Electrical &amp; Electronics Engineers (IEEE),
  2004.
[&nbsp;<a href="OMR-Related_bib.html#Bezine2004">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/iwfhr.2004.45">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Bezine2004a">Bezine2004a</a>]
</td>
<td class="bibtexitem">
H. Bezine, A.M. Alimi, and N. Sherkat.
 Generation and analysis of handwriting script with the beta-elliptic
  model.
 <em>International Journal of Simulation</em>, 8(2):45--65, 2004.
[&nbsp;<a href="OMR-Related_bib.html#Bezine2004a">bib</a>&nbsp;| 
<a href="http://ijssst.info/Vol-08/No-2/paper6.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Blostein1990">Blostein1990</a>]
</td>
<td class="bibtexitem">
D.&nbsp;Blostein and L.&nbsp;Haken.
 Template matching for rhythmic analysis of music keyboard input.
 In <em>[1990] Proceedings. 10th International Conference on Pattern
  Recognition</em>, volume&nbsp;i, pages 767--770 vol.1, June 1990.
[&nbsp;<a href="OMR-Related_bib.html#Blostein1990">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICPR.1990.118213">DOI</a>&nbsp;]
<blockquote><font size="-1">
A system that recognizes common rhythmic patterns through template matching is described. The use of template matching gives the user the unusual ability to modify the set of templates used for analysis. This modification effects a tradeoff between the temporal accuracy required of the input and the complexity of the recognizable rhythm patterns that happen to be common in a particular piece of music. The evolving implementation of this algorithm has received heavy use over a six-year period and has proven itself as a practical and reliable input method for fast music transcription. It is concluded that templates demonstrably provide the necessary temporal context for accurate rhythm recognition.&lt;&lt;ETX&gt;&gt;
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Bojar2011">Bojar2011</a>]
</td>
<td class="bibtexitem">
Ond&#X0159;ej Bojar, Milo&scaron; Ercegov&#x010D;evi&#x107;, Martin Popel, and
  Omar&nbsp;F. Zaidan.
 A grain of salt for the wmt manual evaluation.
 In <em>Sixth Workshop on Statistical Machine Translation</em>, WMT '11,
  pages 1--11, Edinburgh, Scotland, 2011. Association for Computational
  Linguistics.
[&nbsp;<a href="OMR-Related_bib.html#Bojar2011">bib</a>&nbsp;| 
<a href="http://dl.acm.org/citation.cfm?id=2132960.2132962">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Bresler2015">Bresler2015</a>]
</td>
<td class="bibtexitem">
Martin Bresler, Daniel Pr23 u&scaron;a, and V&aacute;clav
  Hlav&aacute;&#x010D;.
 Using Agglomerative Clustering of Strokes to Perform
  Symbols Over-segmentation within a Diagram Recognition System.
 In Vincent&nbsp;Lepetit Paul&nbsp;Wohlhart, editor, <em>CVWW 2015: Proceedings
  of the 20th Computer Vision Winter Workshop</em>, pages 67--74, Seggau, Austria,
  2015. Graz University of Technology.
[&nbsp;<a href="OMR-Related_bib.html#Bresler2015">bib</a>&nbsp;| 
<a href="http://cmp.felk.cvut.cz/ftp/articles/bresler/Bresler-Prusa-Hlavac-CVWW-2015.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Breuel2013">Breuel2013</a>]
</td>
<td class="bibtexitem">
T.&nbsp;M. Breuel, A.&nbsp;Ul-Hasan, M.&nbsp;A. Al-Azawi, and F.&nbsp;Shafait.
 High-performance OCR for printed english and fraktur using LSTM
  networks.
 In <em>2013 12th International Conference on Document Analysis and
  Recognition</em>, pages 683--687, 2013.
[&nbsp;<a href="OMR-Related_bib.html#Breuel2013">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICDAR.2013.140">DOI</a>&nbsp;]
<blockquote><font size="-1">
Long Short-Term Memory (LSTM) networks have yielded excellent results
	on handwriting recognition. This paper describes an application of
	bidirectional LSTM networks to the problem of machine-printed Latin
	and Fraktur recognition. Latin and Fraktur recognition differs significantly
	from handwriting recognition in both the statistical properties of
	the data, as well as in the required, much higher levels of accuracy.
	Applications of LSTM networks to handwriting recognition use two-dimensional
	recurrent networks, since the exact position and baseline of handwritten
	characters is variable. In contrast, for printed OCR, we used a one-dimensional
	recurrent network combined with a novel algorithm for baseline and
	x-height normalization. A number of databases were used for training
	and testing, including the UW3 database, artificially generated and
	degraded Fraktur text and scanned pages from a book digitization
	project. The LSTM architecture achieved 0.6	error on English text. When the artificially degraded Fraktur data
	set is divided into training and test sets, the system achieves an
	error rate of 1.64	of the training set), the system achieves error rates of 0.15	and 1.47	without using any language modelling or any other post-processing
	techniques.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Callison-Burch2007">Callison-Burch2007</a>]
</td>
<td class="bibtexitem">
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh
  Schroeder.
 (meta-) evaluation of machine translation.
 In <em>Second Workshop on Statistical Machine Translation</em>, StatMT
  '07, pages 136--158, Stroudsburg, PA, USA, 2007. Association for
  Computational Linguistics.
[&nbsp;<a href="OMR-Related_bib.html#Callison-Burch2007">bib</a>&nbsp;| 
<a href="http://dl.acm.org/citation.cfm?id=1626355.1626373">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Callison-Burch2008">Callison-Burch2008</a>]
</td>
<td class="bibtexitem">
Chris Callison Burch, Cameron Fordyce, Philipp Koehn, Christof
  Monz, and Josh Schroeder.
 Further Meta-evaluation of Machine Translation.
 In <em>Third Workshop on Statistical Machine Translation</em>, StatMT
  '08, pages 70--106, Stroudsburg, PA, USA, 2008. Association for Computational
  Linguistics.
[&nbsp;<a href="OMR-Related_bib.html#Callison-Burch2008">bib</a>&nbsp;| 
<a href="http://dl.acm.org/citation.cfm?id=1626394.1626403">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Callison-Burch2010">Callison-Burch2010</a>]
</td>
<td class="bibtexitem">
Chris Callison Burch, Philipp Koehn, Christof Monz, Kay
  Peterson, Mark Przybocki, and Omar&nbsp;F. Zaidan.
 Findings of the 2010 Joint Workshop on Statistical Machine
  Translation and Metrics for Machine Translation.
 In <em>Joint Fifth Workshop on Statistical Machine Translation and
  MetricsMATR</em>, WMT '10, pages 17--53, Stroudsburg, PA, USA, 2010. Association
  for Computational Linguistics.
[&nbsp;<a href="OMR-Related_bib.html#Callison-Burch2010">bib</a>&nbsp;| 
<a href="http://dl.acm.org/citation.cfm?id=1868850.1868853">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Calvo-Zaragoza2016b">Calvo-Zaragoza2016b</a>]
</td>
<td class="bibtexitem">
Jorge Calvo-Zaragoza and Jose Oncina.
 An efficient approach for interactive sequential pattern recognition.
 <em>Pattern Recognition</em>, 64:295--304, 2016.
[&nbsp;<a href="OMR-Related_bib.html#Calvo-Zaragoza2016b">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.patcog.2016.11.006">DOI</a>&nbsp;| 
<a href="http://www.sciencedirect.com/science/article/pii/S0031320316303569">http</a>&nbsp;]
<blockquote><font size="-1">
Abstract Interactive Pattern Recognition (IPR) is an emergent framework in which the user is involved actively in the recognition process by giving feedback to the system when an error is detected. Although this framework is expected to reduce the number of errors to correct, it may increase the time required to complete the task since the machine needs to recompute its proposal after each interaction. Therefore, a fast computation is required to make the interactive system profitable and user-friendly. This work presents an efficient approach to deal with IPR tasks when data has a sequential nature. Our approach includes some computation at the very beginning of the task but it then achieves a linear complexity after user corrections. We also show how these tasks can be effectively carried out if the solution space is defined with a Regular Language. This fact has indeed proven to be the most relevant factor to improve the efficiency of the approach. Several experiments are carried out in which our proposal is faced against a classical search. Results show a reduction in time in all experiments considered, solving efficiently some complex IPR tasks thanks to our proposals.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Calvo-Zaragoza2016a">Calvo-Zaragoza2016a</a>]
</td>
<td class="bibtexitem">
Jorge Calvo Zaragoza, Jose&nbsp;J. Valero Mas, and Juan&nbsp;R. Rico
  Juan.
 Prototype generation on structural data using dissimilarity space
  representation.
 <em>Neural Computing and Applications</em>, pages 1--10, 2016.
[&nbsp;<a href="OMR-Related_bib.html#Calvo-Zaragoza2016a">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s00521-016-2278-8">DOI</a>&nbsp;]
<blockquote><font size="-1">
Data reduction techniques play a key role in instance-based classification to lower the amount of data to be processed. Among the different existing approaches, prototype selection (PS) and prototype generation (PG) are the most representative ones. These two families differ in the way the reduced set is obtained from the initial one: While the former aims at selecting the most representative elements from the set, the latter creates new data out of it. Although PG is considered to delimit more efficiently decision boundaries, the operations required are not so well defined in scenarios involving structural data such as strings, trees, or graphs. This work studies the possibility of using dissimilarity space (DS) methods as an intermediate process for mapping the initial structural representation to a statistical one, thereby allowing the use of PG methods. A comparative experiment over string data is carried out in which our proposal is faced to PS methods on the original space. Results show that the proposed strategy is able to achieve significantly similar results to PS in the initial space, thus standing as a clear alternative to the classic approach, with some additional advantages derived from the DS representation.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Chen2017a">Chen2017a</a>]
</td>
<td class="bibtexitem">
Liang-Chieh Chen, Alexander Hermans, George Papandreou, Florian Schroff, Peng
  Wang, and Hartwig Adam.
 Masklab: Instance segmentation by refining object detection with
  semantic and direction features.
 <em>CoRR</em>, abs/1712.04837, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Chen2017a">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.04837">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Clausner2011">Clausner2011</a>]
</td>
<td class="bibtexitem">
Christian Clausner, Stefan Pletschacher, and Apostolos
  Antonacopoulos.
 Aletheia - An Advanced Document Layout and Text
  Ground-Truthing System for Production Environments.
 In <em>2011 International Conference on Document Analysis and
  Recognition, ICDAR</em>, pages 48--52, Beijing, China, 2011. IEEE Computer
  Society.
[&nbsp;<a href="OMR-Related_bib.html#Clausner2011">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICDAR.2011.19">DOI</a>&nbsp;| 
<a href="http://www.prima.cse.salford.ac.uk:8080/www/assets/papers/ICDAR2011_Clausner_Aletheia.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Clausner2014">Clausner2014</a>]
</td>
<td class="bibtexitem">
Christian Clausner, Stefan Pletschacher, and Apostolos
  Antonacopoulos.
 Efficient OCR Training Data Generation with Aletheia.
 In <em>Short Paper Booklet of the 11th International Association for
  Pattern Recognition (IAPR) Workshop on Document Analysis Systems (DAS)</em>,
  2014.
[&nbsp;<a href="OMR-Related_bib.html#Clausner2014">bib</a>&nbsp;| 
<a href="http://www.primaresearch.org/www/assets/papers/DAS2014_Clausner_OCRTrainingDataGeneration.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Cont2007">Cont2007</a>]
</td>
<td class="bibtexitem">
Arshia Cont, Diemo Schwarz, Norbert Schnell, and Christopher
  Raphael.
 Evaluation of real-time audio-to-score alignment.
 In <em>8th International Conference on Music Information Retrieval</em>,
  Vienna, Austria, 2007.
[&nbsp;<a href="OMR-Related_bib.html#Cont2007">bib</a>&nbsp;| 
<a href="https://hal.inria.fr/hal-00839068">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Cordella2000">Cordella2000</a>]
</td>
<td class="bibtexitem">
L.&nbsp;P. Cordella and M.&nbsp;Vento.
 Symbol and shape recognition.
 In Atul&nbsp;K. Chhabra and Dov Dori, editors, <em>Graphics Recognition
  Recent Advances</em>, pages 167--182, Berlin, Heidelberg, 2000. Springer Berlin
  Heidelberg.
[&nbsp;<a href="OMR-Related_bib.html#Cordella2000">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/3-540-40953-X_14">DOI</a>&nbsp;]
<blockquote><font size="-1">
The different aspects of a process for recognizing symbols in documents are considered and the techniques that have been most commonly used during the last ten years, in the different application fields, are reviewed. Methods used in the representation, description and classification phases are shortly discussed and the main recognition strategies are mentioned. Some of the problems that appear still open are proposed to the attention of the reader.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Cutter2015">Cutter2015</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;P. Cutter and Roberto Manduchi.
 Towards mobile OCR.
 In <em>2015 ACM Symposium on Document Engineering - DocEng'15</em>.
  ACM Press, 2015.
[&nbsp;<a href="OMR-Related_bib.html#Cutter2015">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/2682571.2797066">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Dawid1979">Dawid1979</a>]
</td>
<td class="bibtexitem">
Alexander&nbsp;Philip Dawid and Allan&nbsp;M Skene.
 Maximum likelihood estimation of observer error-rates using the
  EM algorithm.
 <em>Applied statistics</em>, pages 20--28, 1979.
[&nbsp;<a href="OMR-Related_bib.html#Dawid1979">bib</a>&nbsp;| 
<a href="https://www.jstor.org/stable/2346806">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Delakis2008">Delakis2008</a>]
</td>
<td class="bibtexitem">
Manolis Delakis and Christophe Garcia.
 Text detection with convolutional neural networks.
 In <em>International Conference on Computer Vision Theory and
  Application</em>, pages 290--294, 2008.
[&nbsp;<a href="OMR-Related_bib.html#Delakis2008">bib</a>&nbsp;| 
<a href="https://www.researchgate.net/profile/Christophe_Garcia2/publication/221415287_text_Detection_with_Convolutional_Neural_Networks/links/545251a30cf2bccc49087299/text-Detection-with-Convolutional-Neural-Networks.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Downie2010">Downie2010</a>]
</td>
<td class="bibtexitem">
J.&nbsp;Stephen Downie, Andreas&nbsp;F. Ehmann, Mert Bay, and M.&nbsp;Cameron Jones.
 <em>The Music Information Retrieval Evaluation eXchange: Some
  Observations and Insights</em>, pages 93--115.
 Springer Berlin Heidelberg, Berlin, Heidelberg, 2010.
[&nbsp;<a href="OMR-Related_bib.html#Downie2010">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-642-11674-2_5">DOI</a>&nbsp;| 
<a href="https://doi.org/10.1007/978-3-642-11674-2_5">http</a>&nbsp;]
<blockquote><font size="-1">
Advances in the science and technology of Music Information Retrieval (MIR) systems and algorithms are dependent on the development of rigorous measures of accuracy and performance such that meaningful comparisons among current and novel approaches can be made. This is the motivating principle driving the efforts of the International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL) and the annual Music Information Retrieval Evaluation eXchange (MIREX). Since it started in 2005, MIREX has fostered great advancements not only in many specific areas of MIR, but also in our general understanding of how MIR systems and algorithms are to be evaluated. This chapter outlines some of the major highlights of the past four years of MIREX evaluations, including its organizing principles, the selection of evaluation metrics, and the evolution of evaluation tasks. The chapter concludes with a brief introduction of how MIREX plans to expand into the future using a suite of Web 2.0 technologies to automated MIREX evaluations.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Droettboom2003">Droettboom2003</a>]
</td>
<td class="bibtexitem">
M.&nbsp;Droettboom.
 Correcting broken characters in the recognition of historical printed
  documents.
 In <em>2003 Joint Conference on Digital Libraries, 2003.
  Proceedings.</em>, pages 364--366, May 2003.
[&nbsp;<a href="OMR-Related_bib.html#Droettboom2003">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/JCDL.2003.1204889">DOI</a>&nbsp;]
<blockquote><font size="-1">
We present a new technique for dealing with broken characters, one of the major challenges in the optical character recognition (OCR) of degraded historical printed documents. A technique based on graph combinatorics is used to rejoin the appropriate connected components. It has been applied to real data with successful results.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Bahdanau2014">Bahdanau2014</a>]
</td>
<td class="bibtexitem">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
 Neural Machine Translation by Jointly Learning to Align
  and Translate.
 <em>Computing Research Repository</em>, abs/1409.0473, 2014.
[&nbsp;<a href="OMR-Related_bib.html#Bahdanau2014">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1409.0473">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Grefenstette2015">Grefenstette2015</a>]
</td>
<td class="bibtexitem">
Edward Grefenstette, Karl&nbsp;Moritz Hermann, Mustafa Suleyman, and
  Phil Blunsom.
 Learning to Transduce with Unbounded Memory.
 <em>Computing Research Repository</em>, abs/1506.02516, 2015.
[&nbsp;<a href="OMR-Related_bib.html#Grefenstette2015">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1506.02516">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Erickson1975">Erickson1975</a>]
</td>
<td class="bibtexitem">
Raymond&nbsp;F. Erickson.
 &ldquo;the darms project&rdquo;: A status report.
 <em>Computers and the Humanities</em>, 9(6):291--298, 1975.
[&nbsp;<a href="OMR-Related_bib.html#Erickson1975">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/BF02396292">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Eskenazi2017">Eskenazi2017</a>]
</td>
<td class="bibtexitem">
S.&nbsp;Eskenazi, P.&nbsp;Gomez-Kr&auml;mer, and J.-M. Ogier.
 A comprehensive survey of mostly textual document segmentation
  algorithms since 2008.
 <em>Pattern Recognition</em>, 64:1--14, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Eskenazi2017">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.patcog.2016.10.023">DOI</a>&nbsp;]
<blockquote><font size="-1">
In document image analysis, segmentation is the task that identifies the regions of a document. The increasing number of applications of document analysis requires a good knowledge of the available technologies. This survey highlights the variety of the approaches that have been proposed for document image segmentation since 2008. It provides a clear typology of documents and of document image segmentation algorithms. We also discuss the technical limitations of these algorithms, the way they are evaluated and the general trends of the community. © 2016 Elsevier Ltd
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Fahmy1992">Fahmy1992</a>]
</td>
<td class="bibtexitem">
Hoda Fahmy and Dorothea Blostein.
 A survey of graph grammars: Theory and applications.
 In <em>Pattern Recognition, 1992. Vol. II. Conference B: Pattern
  Recognition Methodology and Systems, Proceedings., 11th IAPR International
  Conference on</em>, pages 294--298. IEEE, 1992.
[&nbsp;<a href="OMR-Related_bib.html#Fahmy1992">bib</a>&nbsp;| 
<a href="https://www.researchgate.net/publication/3513859_A_survey_of_graph_grammars_theory_and_applications">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Fasanaro1990">Fasanaro1990</a>]
</td>
<td class="bibtexitem">
A.&nbsp;M. Fasanaro, D.&nbsp;L.&nbsp;A. Spitaleri, R.&nbsp;Valiani, and D.&nbsp;Grossi.
 Dissociation in musical reading: A musician affected by alexia
  without agraphia.
 <em>Music Perception: An Interdisciplinary Journal</em>, 7(3):259--272,
  1990.
[&nbsp;<a href="OMR-Related_bib.html#Fasanaro1990">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.2307/40285464">DOI</a>&nbsp;| 
<a href="http://arxiv.org/abs/http://mp.ucpress.edu/content/7/3/259.full.pdf">arXiv</a>&nbsp;| 
<a href="http://mp.ucpress.edu/content/7/3/259">http</a>&nbsp;]
<blockquote><font size="-1">
Previous works have postulated a similarity between music reading
	and text reading. Therefore it is interesting to evaluate both of
	these functions in an alexic subject. The patient investigated is
	a professional musician who had an ischemic lesion in the left temporoparieto-occipital
	region. Text reading showed pure alexia in which both the phonological
	and global routes were damaged. His ability to read correctly via
	matching tests showed that the word-form system was preserved. The
	reading of musical scores was damaged too and showed a dissociation
	between the reading of ideograms and rhythms (preserved) and the
	reading of notes (impaired). The results of note reading were analogous
	to those of word reading. Furthermore, the patient could read notes
	correctly via matching tests. On the basis of these findings, we
	propose a model of music reading where the reading of notes is based
	on a representational system analogous to that of words (the so-called
	internal language) whereas reading of ideograms and rhythms occurs
	via an internal representation unrelated to linguistic functions.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Feist2017">Feist2017</a>]
</td>
<td class="bibtexitem">
Jonathan Feist.
 <em>Berklee Contemporary Music Notation</em>.
 Berklee Press, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Feist2017">bib</a>&nbsp;]
<blockquote><font size="-1">
Learn the nuances of music notation, and create professional looking scores. This reference presents a comprehensive look at contemporary music notation. You will learn the meaning and stylistic practices for many types of notation that are currently in common use, from traditional staffs to lead sheets to guitar tablature. It discusses hundreds of notation symbols, as well as general guidelines for writing music. Berklee College of Music brings together teachers and students from all over the world, and we use notation in a great variety of ways. This book presents our perspectives on notation: what we have found to be the most commonly used practices in today's music industry, and what seems to be serving our community best. It includes a foreword by Matthew Nicholl, who was a long-time chair of Berklee's Contemporary Writing and Production Department. Whether you find yourself in a Nashville recording studio, Hollywood sound stage, grand concert hall, worship choir loft, or elementary school auditorium, this book will help you to create readable, professional, publication-quality notation. Beyond understanding the standard rules and definitions, you will learn to make appropriate choices for your own work, and generally how to achieve clarity and consistency in your notation so that it best serves your music.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Ferlaino2018">Ferlaino2018</a>]
</td>
<td class="bibtexitem">
M.&nbsp;Ferlaino, C.&nbsp;A. Glastonbury, C.&nbsp;Motta-Mejia, M.&nbsp;Vatish, I.&nbsp;Granne,
  S.&nbsp;Kennedy, C.&nbsp;M. Lindgren, and C.&nbsp;Nell&aring;ker.
 Towards Deep Cellular Phenotyping in Placental Histology.
 <em>ArXiv e-prints</em>, 2018.
[&nbsp;<a href="OMR-Related_bib.html#Ferlaino2018">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1804.03270">arXiv</a>&nbsp;| 
<a href="https://openreview.net/pdf?id=HJq5OGKsz">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Chollet2017">Chollet2017</a>]
</td>
<td class="bibtexitem">
Fran&ccedil;ois Chollet.
 Keras.
 <a href="https://github.com/fchollet/keras">https://github.com/fchollet/keras</a>, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Chollet2017">bib</a>&nbsp;| 
<a href="https://github.com/keras-team/keras">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Gao2003">Gao2003</a>]
</td>
<td class="bibtexitem">
Sheng Gao, N.&nbsp;C. Maddage, and Chin-Hui Lee.
 A hidden markov model based approach to music segmentation and
  identification.
 In <em>Fourth International Conference on Information,
  Communications and Signal Processing, 2003 and the Fourth Pacific Rim
  Conference on Multimedia. Proceedings of the 2003 Joint</em>, volume&nbsp;3, pages
  1576--1580 vol.3, Dec 2003.
[&nbsp;<a href="OMR-Related_bib.html#Gao2003">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICICS.2003.1292732">DOI</a>&nbsp;]
<blockquote><font size="-1">
Classification of musical segments is an interesting problem. It is a key technology in the development of content-based audio document indexing and retrieval. In this paper, we apply the feature extraction and modeling techniques commonly used in automatic speech recognition to solving the problem of segmentation and instrument identification of musical passages. The correlation among the different components in the feature space and the auto-correlation of each component are analyzed to demonstrate feasibility in musical signal analysis and instrument class modeling. Our experimental results are first evaluated on 3 instrument categories, i.e. vocal music, instrumental music, and their combinations. Furthermore each category is split into two individual cases to give a 6-class problem. Our results show that good performance could be obtained with simple features, such as mel-frequency cepstral coefficients and cepstral coefficients derived from linear prediction signal analysis. Even with a limited amount of training data, we could give an accuracy of 90.60% in the case of three categories. A slightly worse accuracy of 90.38% is obtained when we double the number of categories to six classes.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Garfinkle2017">Garfinkle2017</a>]
</td>
<td class="bibtexitem">
David Garfinkle, Claire Arthur, Peter Schubert, Julie Cumming, and Ichiro
  Fujinaga.
 Patternfinder: Content-based music retrieval with music21.
 In <em>4th International Workshop on Digital Libraries for
  Musicology</em>, DLfM '17, pages 5--8, New York, NY, USA, 2017. ACM.
[&nbsp;<a href="OMR-Related_bib.html#Garfinkle2017">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3144749.3144751">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Gatos2009">Gatos2009</a>]
</td>
<td class="bibtexitem">
B.&nbsp;Gatos, K.&nbsp;Ntirogiannis, and I.&nbsp;Pratikakis.
 ICDAR 2009 document image binarization contest (DIBCO 2009).
 In <em>2009 10th International Conference on Document Analysis and
  Recognition</em>, pages 1375--1382, 2009.
[&nbsp;<a href="OMR-Related_bib.html#Gatos2009">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICDAR.2009.246">DOI</a>&nbsp;]
<blockquote><font size="-1">
DIBCO 2009 is the first International Document Image Binarization
	Contest organized in the context of ICDAR 2009 conference. The general
	objective of the contest is to identify current advances in document
	image binarization using established evaluation performance measures.
	This paper describes the contest details including the evaluation
	measures used as well as the performance of the 43 submitted methods
	along with a short description of each method.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Gatos2006">Gatos2006</a>]
</td>
<td class="bibtexitem">
B.&nbsp;Gatos, I.&nbsp;Pratikakis, and S.J. Perantonis.
 Adaptive degraded document image binarization.
 <em>Pattern Recognition</em>, 39(3):317--327, 2006.
[&nbsp;<a href="OMR-Related_bib.html#Gatos2006">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.patcog.2005.09.010">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Gatos2004">Gatos2004</a>]
</td>
<td class="bibtexitem">
Basilios Gatos, Ioannis Pratikakis, and Stavros&nbsp;J. Perantonis.
 An adaptive binarization technique for low quality historical
  documents.
 In Simone Marinai and Andreas&nbsp;R. Dengel, editors, <em>Document
  Analysis Systems VI</em>, pages 102--113, Berlin, Heidelberg, 2004. Springer
  Berlin Heidelberg.
[&nbsp;<a href="OMR-Related_bib.html#Gatos2004">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-540-28640-0_10">DOI</a>&nbsp;]
<blockquote><font size="-1">
Historical document collections are a valuable resource for human history. This paper proposes a novel digital image binarization scheme for low quality historical documents allowing further content exploitation in an efficient way. The proposed scheme consists of five distinct steps: a pre-processing procedure using a low-pass Wiener filter, a rough estimation of foreground regions using Niblack's approach, a background surface calculation by interpolating neighboring background intensities, a thresholding by combining the calculated background surface with the original image and finally a post-processing step in order to improve the quality of text regions and preserve stroke connectivity. The proposed methodology works with great success even in cases of historical manuscripts with poor quality, shadows, nonuniform illumination, low contrast, large signal- dependent noise, smear and strain. After testing the proposed method on numerous low quality historical manuscripts, it has turned out that our methodology performs better compared to current state-of-the-art adaptive thresholding techniques.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="George2014">George2014</a>]
</td>
<td class="bibtexitem">
Joe George and Lior Shamir.
 Computer analysis of similarities between albums in popular music.
 <em>Pattern Recognition Letters</em>, 45:78--84, 2014.
[&nbsp;<a href="OMR-Related_bib.html#George2014">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.patrec.2014.02.021">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Gerou2009">Gerou2009</a>]
</td>
<td class="bibtexitem">
Tom Gerou and Linda Lusk.
 <em>Essential Dicionary of Music Notation</em>.
 Alfred Publishing Co., Inc., 2009.
[&nbsp;<a href="OMR-Related_bib.html#Gerou2009">bib</a>&nbsp;| 
<a href="http://www.amazon.com/Essentials-Music-Notation-Alfred-Publishing/dp/073906083X">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Giotis2017">Giotis2017</a>]
</td>
<td class="bibtexitem">
Angelos&nbsp;P. Giotis, Giorgos Sfikas, Basilis Gatos, and Christophoros Nikou.
 A survey of document image word spotting techniques.
 <em>Pattern Recognition</em>, 68:310--332, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Giotis2017">bib</a>&nbsp;| 
<a href="https://doi.org/10.1016/j.patcog.2017.02.023">DOI</a>&nbsp;| 
<a href="http://www.sciencedirect.com/science/article/pii/S0031320317300870">http</a>&nbsp;]
<blockquote><font size="-1">
Vast collections of documents available in image format need to be indexed for information retrieval purposes. In this framework, word spotting is an alternative solution to optical character recognition (OCR), which is rather inefficient for recognizing text of degraded quality and unknown fonts usually appearing in printed text, or writing style variations in handwritten documents. Over the past decade there has been a growing interest in addressing document indexing using word spotting which is reflected by the continuously increasing number of approaches. However, there exist very few comprehensive studies which analyze the various aspects of a word spotting system. This work aims to review the recent approaches as well as fill the gaps in several topics with respect to the related works. The nature of texts and inherent challenges addressed by word spotting methods are thoroughly examined. After presenting the core steps which compose a word spotting system, we investigate the use of retrieval enhancement techniques based on relevance feedback which improve the retrieved results. Finally, we present the datasets which are widely used for word spotting, we describe the evaluation standards and measures applied for performance assessment and discuss the results achieved by the state of the art.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Girshick2015">Girshick2015</a>]
</td>
<td class="bibtexitem">
R.&nbsp;Girshick.
 Fast r-cnn.
 In <em>2015 IEEE International Conference on Computer Vision
  (ICCV)</em>, pages 1440--1448, 2015.
[&nbsp;<a href="OMR-Related_bib.html#Girshick2015">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICCV.2015.169">DOI</a>&nbsp;]
<blockquote><font size="-1">
This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Girshick2014">Girshick2014</a>]
</td>
<td class="bibtexitem">
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.
 Rich feature hierarchies for accurate object detection and semantic
  segmentation.
 In <em>IEEE Conference On Computer Vision and Pattern Recognition</em>,
  pages 580--587, 2014.
[&nbsp;<a href="OMR-Related_bib.html#Girshick2014">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/CVPR.2014.81">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Good2003">Good2003</a>]
</td>
<td class="bibtexitem">
M.&nbsp;Good and G.&nbsp;Actor.
 Using musicxml for file interchange.
 In <em>Third International Conference on WEB Delivering of Music</em>,
  page 153, 2003.
[&nbsp;<a href="OMR-Related_bib.html#Good2003">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/WDM.2003.1233890">DOI</a>&nbsp;]
<blockquote><font size="-1">
The MusicXML format is designed to be a universal translator for programs that understand common Western musical notation. We have made significant progress towards this goal, with over a dozen programs supporting MusicXML as of June 2003. We describe some of the ways that MusicXML has been used for file interchange, and will demonstrate several scenarios.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Goodfellow2013">Goodfellow2013</a>]
</td>
<td class="bibtexitem">
Ian&nbsp;J Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha
  Arnoud, and Vinay Shet.
 Multi-digit Number Recognition from Street View Imagery using Deep
  Convolutional Neural Networks.
 <em>Computing Research Repository</em>, abs/1312.6:1--13, 2013.
[&nbsp;<a href="OMR-Related_bib.html#Goodfellow2013">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1312.6082">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1312.6082">http</a>&nbsp;]
<blockquote><font size="-1">
Recognizing arbitrary multi-character text in unconstrained natural photographs is a hard problem. In this paper, we address an equally hard sub-problem in this domain viz. recognizing arbitrary multi-digit numbers from Street View imagery. Traditional approaches to solve this problem typically separate out the localization, segmentation, and recognition steps. In this paper we propose a unified approach that integrates these three steps via the use of a deep convolutional neural network that operates directly on the image pixels. We employ the DistBelief implementation of deep neural networks in order to train large, distributed neural networks on high quality images. We find that the performance of this approach increases with the depth of the convolutional network, with the best performance occurring in the deepest architecture we trained, with eleven hidden layers. We evaluate this approach on the publicly available SVHN dataset and achieve over 96% accuracy in recognizing complete street numbers. We show that on a per-digit recognition task, we improve upon the state-of-the-art, achieving 97.84% accuracy. We also evaluate this approach on an even more challenging dataset generated from Street View imagery containing several tens of millions of street number annotations and achieve over 90% accuracy. To further explore the applicability of the proposed system to broader text recognition tasks, we apply it to synthetic distorted text from reCAPTCHA. reCAPTCHA is one of the most secure reverse turing tests that uses distorted text to distinguish humans from bots. We report a 99.8% accuracy on the hardest category of reCAPTCHA. Our evaluations on both tasks indicate that at specific operating thresholds, the performance of the proposed system is comparable to, and in some cases exceeds, that of human operators.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Goolsby1994">Goolsby1994</a>]
</td>
<td class="bibtexitem">
Thomas&nbsp;W. Goolsby.
 Eye movement in music reading: Effects of reading ability, notational
  complexity, and encounters.
 <em>Music Perception: An Interdisciplinary Journal</em>, 12(1):77--96,
  1994.
[&nbsp;<a href="OMR-Related_bib.html#Goolsby1994">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.2307/40285756">DOI</a>&nbsp;| 
<a href="http://arxiv.org/abs/http://mp.ucpress.edu/content/12/1/77.full.pdf">arXiv</a>&nbsp;| 
<a href="http://mp.ucpress.edu/content/12/1/77">http</a>&nbsp;]
<blockquote><font size="-1">
Six types of eye movement were measured and recorded with an SRI Eyetracker: number of progressive and regressive fixations, durations of progressive and regressive fixations and lengths of progressive and regressive saccades. Twenty-four graduate music students were selected as skilled and less- skilled music readers. Eye position was measured every millisecond with a high degree of accuracy. The factorial design was 2 Groups x 4 Melodies x 3 Encounters (including a practice period). Results indicated that patterns of eye movement in the two groups were similar across melodies and encounters, but differed with notational complexity. Eye movement was reduced when performing melodies with more-concentrated visual information than when performing melodies with less- concentrated visual information. The main effect of encounters indicated that music readers used fewer but longer fixations after practicing the melodies. Results suggest that skilled music readers look farther ahead in the notation, and then back to the point of performance, when sightreading.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Goolsby1994a">Goolsby1994a</a>]
</td>
<td class="bibtexitem">
Thomas&nbsp;W. Goolsby.
 Profiles of processing: Eye movements during sightreading.
 <em>Music Perception: An Interdisciplinary Journal</em>, 12(1):97--123,
  1994.
[&nbsp;<a href="OMR-Related_bib.html#Goolsby1994a">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.2307/40285757">DOI</a>&nbsp;| 
<a href="http://arxiv.org/abs/http://mp.ucpress.edu/content/12/1/97.full.pdf">arXiv</a>&nbsp;| 
<a href="http://mp.ucpress.edu/content/12/1/97">http</a>&nbsp;]
<blockquote><font size="-1">
Temporal and sequential components of the eye movement used by a skilled and a less-skilled sightreader were used to construct six profiles of processing. Each subject read three melodies of varying levels of concentration of visual detail. The profiles indicates the order, duration, and location of each fixation while the subjects sightread the melodies. Results indicate that music readers do not fixate on note stems or the bar lines that connect eighth notes when sightreading. The less-skilled music reader progressed through the melody virtually note-by-note using long fixations, whereas the skilled sightreader directed fixations to all areas of the notation (using more regressions than the less-skilled reader) to perform the music accurately. Results support earlier findings that skilled sightreaders look farther ahead in the notation, then back to the point of performance (Goolsby, 1994), and have a larger perceptual span than less-skilled sightreaders. Findings support Slobodans (1984) contention that music reading (i. e., sightreading) is indeed music perception, because music notation is processed before performance. Support was found for Sloboda's (1977, 1984, 1985, 1988) hypotheses on the effects of physical and structural boundaries on visual musical perception. The profiles indicate a number of differences between music perception from processing visual notation and perception resulting from language reading. These differences include: (1) opposite trends in the control of eye movement (i. e., the better music reader fixates in blank areas of the visual stimuli and not directly on each item of the information that was performed), (2) a perceptual span that is vertical as well as horizontal, (3) more eye movement associated with the better reader, and (4) greater attention used for processing language than for music, although the latter task requires an "exact realization."
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Graves2013">Graves2013</a>]
</td>
<td class="bibtexitem">
A.&nbsp;Graves, A.&nbsp;Mohamed, and G.&nbsp;Hinton.
 Speech recognition with deep recurrent neural networks.
 In <em>2013 IEEE International Conference on Acoustics, Speech and
  Signal Processing</em>, pages 6645--6649, 2013.
[&nbsp;<a href="OMR-Related_bib.html#Graves2013">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP.2013.6638947">DOI</a>&nbsp;]
<blockquote><font size="-1">
Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Graves2007">Graves2007</a>]
</td>
<td class="bibtexitem">
Alex Graves, Santiago Fern&aacute;ndez, Marcus Liwicki, Horst Bunke, and
  J&uuml;rgen Schmidhuber.
 Unconstrained online handwriting recognition with recurrent neural
  networks.
 In <em>20th International Conference on Neural Information
  Processing Systems</em>, NIPS'07, pages 577--584, USA, 2007. Curran Associates
  Inc.
[&nbsp;<a href="OMR-Related_bib.html#Graves2007">bib</a>&nbsp;| 
<a href="http://dl.acm.org/citation.cfm?id=2981562.2981635">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Graves2014">Graves2014</a>]
</td>
<td class="bibtexitem">
Alex Graves and Navdeep Jaitly.
 Towards end-to-end speech recognition with recurrent neural networks.
 In <em>31st International Conference on International Conference on
  Machine Learning</em>, volume&nbsp;32 of <em>ICML'14</em>, pages 1764--1772, Beijing,
  China, 2014. JMLR.org.
[&nbsp;<a href="OMR-Related_bib.html#Graves2014">bib</a>&nbsp;| 
<a href="http://dl.acm.org/citation.cfm?id=3044805.3045089">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Graves2009">Graves2009</a>]
</td>
<td class="bibtexitem">
Alex Graves, Marcus Liwicki, Santiago Fern&aacute;ndez, Roman Bertolami, Horst
  Bunke, and J&uuml;rgen Schmidhuber.
 A novel connectionist system for unconstrained handwriting
  recognition.
 <em>Pattern Analysis and Machine Intelligence, IEEE Transactions
  on</em>, 31(5):855--868, 2009.
[&nbsp;<a href="OMR-Related_bib.html#Graves2009">bib</a>&nbsp;| 
<a href="http://www.cs.toronto.edu/~graves/tpami_2009.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Harman2011">Harman2011</a>]
</td>
<td class="bibtexitem">
Donna Harman.
 <em>Information Retrieval Evaluation</em>.
 Morgan &amp; Claypool Publishers, 1st edition, 2011.
[&nbsp;<a href="OMR-Related_bib.html#Harman2011">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="He2016">He2016</a>]
</td>
<td class="bibtexitem">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
 Deep residual learning for image recognition.
 In <em>The IEEE Conference on Computer Vision and Pattern
  Recogntiion (CVPR)</em>, pages 770--778, 2016.
[&nbsp;<a href="OMR-Related_bib.html#He2016">bib</a>&nbsp;| 
<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Heussenstamm1987">Heussenstamm1987</a>]
</td>
<td class="bibtexitem">
George Heussenstamm.
 <em>The Norton Manual of Music Notation</em>.
 W. W. Norton &amp; Company, 1987.
[&nbsp;<a href="OMR-Related_bib.html#Heussenstamm1987">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Hosang2016">Hosang2016</a>]
</td>
<td class="bibtexitem">
J.&nbsp;Hosang, R.&nbsp;Benenson, P.&nbsp;Doll&aacute;r, and B.&nbsp;Schiele.
 What makes for effective detection proposals?
 <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>,
  38(4):814--830, 2016.
[&nbsp;<a href="OMR-Related_bib.html#Hosang2016">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/TPAMI.2015.2465908">DOI</a>&nbsp;]
<blockquote><font size="-1">
Current top performing object detectors employ detection proposals
	to guide the search for objects, thereby avoiding exhaustive sliding
	window search across images. Despite the popularity and widespread
	use of detection proposals, it is unclear which trade-offs are made
	when using them during object detection. We provide an in-depth analysis
	of twelve proposal methods along with four baselines regarding proposal
	repeatability, ground truth annotation recall on PASCAL, ImageNet,
	and MS COCO, and their impact on DPM, R-CNN, and Fast R-CNN detection
	performance. Our analysis shows that for object detection improving
	proposal localisation accuracy is as important as improving recall.
	We introduce a novel metric, the average recall (AR), which rewards
	both high recall and good localisation and correlates surprisingly
	well with detection performance. Our findings show common strengths
	and weaknesses of existing methods, and provide insights and metrics
	for selecting and tuning proposal methods.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Ann2010">Ann2010</a>]
</td>
<td class="bibtexitem">
Hsing-Yen Ann, Chang-Biau Yang, Yung-Hsing Peng, and
  Bern-Cherng Liaw.
 Efficient algorithms for the block edit problems.
 <em>Information and Computation</em>, 208(3):221--229, 2010.
[&nbsp;<a href="OMR-Related_bib.html#Ann2010">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.ic.2009.12.001">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Hu2017">Hu2017</a>]
</td>
<td class="bibtexitem">
Ronghang Hu, Piotr Doll&aacute;r, Kaiming He, Trevor Darrell, and Ross&nbsp;B.
  Girshick.
 Learning to segment every thing.
 <em>CoRR</em>, abs/1711.10370, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Hu2017">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.10370">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.10370">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Huang2016">Huang2016</a>]
</td>
<td class="bibtexitem">
Allen Huang and Raymond Wu.
 Deep Learning for Music.
 Technical report, Stanford University, 2016.
[&nbsp;<a href="OMR-Related_bib.html#Huang2016">bib</a>&nbsp;| 
<a href="https://arxiv.org/abs/1606.04930">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Huang2017">Huang2017</a>]
</td>
<td class="bibtexitem">
Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara,
  Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, and
  Kevin Murphy.
 Speed/accuracy trade-offs for modern convolutional object detectors.
 In <em>The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)</em>, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Huang2017">bib</a>&nbsp;| 
<a href="http://openaccess.thecvf.com/content_cvpr_2017/html/Huang_SpeedAccuracy_Trade-Offs_for_CVPR_2017_paper.html">.html</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Zhang2017">Zhang2017</a>]
</td>
<td class="bibtexitem">
Jianshu Zhang, Jun Du, Shiliang Zhang, Dan Liu, Yulong Hu,
  Jinshui Hu, Si Wei, and Lirong Dai.
 Watch, Attend and Parse: An End-to-end Neural Network
  Based Approach to Handwritten Mathematical Expression
  Recognition.
 <em>Pattern Recognition</em>, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Zhang2017">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.patcog.2017.06.017">DOI</a>&nbsp;| 
<a href="http://www.sciencedirect.com/science/article/pii/S0031320317302376">http</a>&nbsp;]
<blockquote><font size="-1">
Abstract Machine recognition of a handwritten mathematical expression (HME) is challenging due to the ambiguities of handwritten symbols and the two-dimensional structure of mathematical expressions. Inspired by recent work in deep learning, we present Watch, Attend and Parse (WAP), a novel end-to-end approach based on neural network that learns to recognize {HMEs} in a two-dimensional layout and outputs them as one-dimensional character sequences in LaTeX format. Inherently unlike traditional methods, our proposed model avoids problems that stem from symbol segmentation, and it does not require a predefined expression grammar. Meanwhile, the problems of symbol recognition and structural analysis are handled, respectively, using a watcher and a parser. We employ a convolutional neural network encoder that takes {HME} images as input as the watcher and employ a recurrent neural network decoder equipped with an attention mechanism as the parser to generate LaTeX sequences. Moreover, the correspondence between the input expressions and the output LaTeX sequences is learned automatically by the attention mechanism. We validate the proposed approach on a benchmark published by the {CROHME} international competition. Using the official training dataset, {WAP} significantly outperformed the state-of-the-art method with an expression recognition accuracy of 46.55% on {CROHME} 2014 and 44.55% on {CROHME} 2016. 
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Journet2017">Journet2017</a>]
</td>
<td class="bibtexitem">
Nicholas Journet, Muriel Visani, Boris Mansencal, Kieu Van-Cuong, and Antoine
  Billy.
 Doccreator: A new software for creating synthetic ground-truthed
  document images.
 <em>Journal of imaging</em>, 3(4):62, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Journet2017">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.3390/jimaging3040062">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Simonyan2014">Simonyan2014</a>]
</td>
<td class="bibtexitem">
Karen Simonyan and Andrew Zisserman.
 Very Deep Convolutional Networks for Large-Scale Image
  Recognition.
 <em>Computing Research Repository</em>, abs/1409.1556, 2014.
[&nbsp;<a href="OMR-Related_bib.html#Simonyan2014">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1409.1556">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Gregor2015">Gregor2015</a>]
</td>
<td class="bibtexitem">
Karol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra.
 DRAW: A Recurrent Neural Network For Image
  Generation.
 <em>Computing Research Repository</em>, abs/1502.04623, 2015.
[&nbsp;<a href="OMR-Related_bib.html#Gregor2015">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1502.04623">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Kasturi1988">Kasturi1988</a>]
</td>
<td class="bibtexitem">
R.&nbsp;Kasturi and L.&nbsp;Fletcher.
 A robust algorithm for text string separation from mixed
  text/graphics images.
 <em>IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</em>,
  10:910--918, 11 1988.
[&nbsp;<a href="OMR-Related_bib.html#Kasturi1988">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/34.9112">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Katayose1989">Katayose1989</a>]
</td>
<td class="bibtexitem">
H.&nbsp;Katayose, H.&nbsp;Kato, M.&nbsp;Imai, and Inokuchi S.
 An approach to an artificial music expert.
 In <em>International Computer Music Conference</em>, pages 139--146,
  1989.
[&nbsp;<a href="OMR-Related_bib.html#Katayose1989">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Katayose1989a">Katayose1989a</a>]
</td>
<td class="bibtexitem">
Haruhiro Katayose and Seiji Inokuchi.
 The kansei music system.
 <em>Computer Music Journal</em>, 13(4):72--77, 1989.
[&nbsp;<a href="OMR-Related_bib.html#Katayose1989a">bib</a>&nbsp;| 
<a href="http://www.jstor.org/stable/3679555">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Zhai2012">Zhai2012</a>]
</td>
<td class="bibtexitem">
Ke Zhai, Yuening Hu, Jordan L.&nbsp;Boyd-Graber, and Sinead
  Williamson.
 Modeling Images using Transformed Indian Buffet
  Processes.
 In <em>29th International Conference on Machine Learning</em>. icml.cc /
  Omnipress, 2012.
[&nbsp;<a href="OMR-Related_bib.html#Zhai2012">bib</a>&nbsp;| 
<a href="http://icml.cc/2012/papers/750.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Keil2017">Keil2017</a>]
</td>
<td class="bibtexitem">
Klaus Keil and Jennifer&nbsp;A. Ward.
 Applications of rism data in digital libraries and digital
  musicology.
 <em>International Journal on Digital Libraries</em>, Jan 2017.
[&nbsp;<a href="OMR-Related_bib.html#Keil2017">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s00799-016-0205-3">DOI</a>&nbsp;| 
<a href="https://doi.org/10.1007/s00799-016-0205-3">http</a>&nbsp;]
<blockquote><font size="-1">
Information about manuscripts and printed music indexed in RISM (R&eacute;pertoire International des Sources Musicales), a large, international project that records and describes musical sources, was for decades available solely through book publications, CD-ROMs, or subscription services. Recent initiatives to make the data available on a wider scale have resulted in, most significantly, a freely accessible online database and the availability of its data as open data and linked open data. Previously, the task of increasing the amount of data was primarily carried out by RISM national groups and the Zentralredaktion (Central Office). The current opportunities available by linking to other freely accessible databases and importing data from other resources open new perspectives and prospects. This paper describes the RISM data and their applications for digital libraries and digital musicological projects. We discuss the possibilities and challenges in making available a large and growing quantity of data and how the data have been utilized in external library and musicological projects. Interactive functions in the RISM OPAC are planned for the future, as is closer collaboration with the projects that use RISM data. Ultimately, RISM would like to arrange a &ldquo;take and give&rdquo; system in which the RISM data are used in external projects, enhanced by the project participants, and then delivered back to the RISM Zentralredaktion.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Kingma2014">Kingma2014</a>]
</td>
<td class="bibtexitem">
Diederik&nbsp;P. Kingma and Jimmy Ba.
 Adam: A method for stochastic optimization.
 <em>Computing Research Repository</em>, abs/1412.6980, 2014.
[&nbsp;<a href="OMR-Related_bib.html#Kingma2014">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1412.6980">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Koehn2005">Koehn2005</a>]
</td>
<td class="bibtexitem">
Philipp Koehn.
 Europarl: A parallel corpus for statistical machine translation.
 In <em>MT summit</em>, volume&nbsp;5, pages 79--86, 2005.
[&nbsp;<a href="OMR-Related_bib.html#Koehn2005">bib</a>&nbsp;| 
<a href="http://courses.washington.edu/ling473/Project5.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Kurth2007">Kurth2007</a>]
</td>
<td class="bibtexitem">
F. Kurth, M. M&uuml;ller, C. Fremerey, Y. Chang, and M.
  Clausen.
 Automated synchronization of scanned sheet music with audio
  recordings.
 In <em>8th International Conference on Music Information Retrieval</em>,
  pages 261--266, Vienna, Austria, 2007.
[&nbsp;<a href="OMR-Related_bib.html#Kurth2007">bib</a>&nbsp;| 
<a href="http://ismir2007.ismir.net/proceedings/ISMIR2007_p261_kurth.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Lake2013">Lake2013</a>]
</td>
<td class="bibtexitem">
Brenden&nbsp;M Lake, Ruslan&nbsp;R Salakhutdinov, and Josh Tenenbaum.
 One-shot learning by inverting a compositional causal process.
 In C.J.C. Burges, L.&nbsp;Bottou, M.&nbsp;Welling, Z.&nbsp;Ghahramani, and K.Q.
  Weinberger, editors, <em>Advances in Neural Information Processing Systems
  26</em>, pages 2526--2534. Curran Associates, Inc., 2013.
[&nbsp;<a href="OMR-Related_bib.html#Lake2013">bib</a>&nbsp;| 
<a href="http://papers.nips.cc/paper/5128-one-shot-learning-by-inverting-a-compositional-causal-process.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Lavie2007">Lavie2007</a>]
</td>
<td class="bibtexitem">
Alon Lavie and Abhaya Agarwal.
 Meteor: An Automatic Metric for MT Evaluation with High
  Levels of Correlation with Human Judgments.
 In <em>Second Workshop on Statistical Machine Translation</em>, StatMT
  '07, pages 228--231, Stroudsburg, PA, USA, 2007. Association for
  Computational Linguistics.
[&nbsp;<a href="OMR-Related_bib.html#Lavie2007">bib</a>&nbsp;| 
<a href="http://dl.acm.org/citation.cfm?id=1626355.1626389">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="LeCun1998">LeCun1998</a>]
</td>
<td class="bibtexitem">
Yann LeCun, L&eacute;on Bottou, Yoshua Bengio, and Patrick
  Haffner.
 Gradient-based learning applied to document recognition.
 <em>Proceedings of the IEEE</em>, 86(11):2278--2324, 1998.
[&nbsp;<a href="OMR-Related_bib.html#LeCun1998">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/5.726791">DOI</a>&nbsp;]
<blockquote><font size="-1">
Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Lee2016">Lee2016</a>]
</td>
<td class="bibtexitem">
C.&nbsp;Lee, H.&nbsp;J. Kim, and K.&nbsp;W. Oh.
 Comparison of faster r-CNN models for object detection.
 In <em>2016 16th International Conference on Control, Automation and
  Systems (ICCAS)</em>, pages 107--110, 2016.
[&nbsp;<a href="OMR-Related_bib.html#Lee2016">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICCAS.2016.7832305">DOI</a>&nbsp;]
<blockquote><font size="-1">
Object detection is one of the important problems for autonomous robots.
	Faster R-CNN, one of the state-of-the-art object detection methods,
	approaches real time application; nevertheless, computational time
	lies borderline of real time application, i.e. 5fps with VGG16 model
	in K40 GPU system in [1]. Moreover, computation time depends on model
	and image crop size, but precision is also affected; usually, time
	and precision have trade-off relation. By adjusting input image size
	in spite of downgrading performance, computation time meets criteria
	for one model. Therefore, selection of a model is one of the important
	problems when faster R-CNN based object detection system for an autonomous
	robot is constructed. In this paper, we convert several state-of-the-art
	models from convolution neural network (CNN) for image classification.
	Then, we compare converted models with several image crop size in
	terms of computation time and detection precision. We will utilize
	those comparison data for selecting a proper detection model in case
	a robot needs to perform an object detection task.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Lewis2004">Lewis2004</a>]
</td>
<td class="bibtexitem">
David&nbsp;D Lewis, Yiming Yang, Tony&nbsp;G Rose, and Fan Li.
 Rcv1: A new benchmark collection for text categorization
  research.
 <em>The Journal of Machine Learning Research</em>, 5:361--397, 2004.
[&nbsp;<a href="OMR-Related_bib.html#Lewis2004">bib</a>&nbsp;| 
<a href="http://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Lin2017a">Lin2017a</a>]
</td>
<td class="bibtexitem">
Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and
  Serge Belongie.
 Feature pyramid networks for object detection.
 In <em>The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)</em>, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Lin2017a">bib</a>&nbsp;| 
<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Lin2017">Lin2017</a>]
</td>
<td class="bibtexitem">
Tsung-Yi Lin, Priya Goyal, Ross&nbsp;B. Girshick, Kaiming He, and Piotr
  Doll&aacute;r.
 Focal loss for dense object detection.
 <em>CoRR</em>, abs/1708.02002, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Lin2017">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1708.02002">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Lin2014">Lin2014</a>]
</td>
<td class="bibtexitem">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll&aacute;r, and C.&nbsp;Lawrence Zitnick.
 Microsoft coco: Common objects in context.
 In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars,
  editors, <em>Computer Vision -- ECCV 2014</em>, pages 740--755, Cham, 2014.
  Springer International Publishing.
[&nbsp;<a href="OMR-Related_bib.html#Lin2014">bib</a>&nbsp;| 
<a href="https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48">http</a>&nbsp;]
<blockquote><font size="-1">
We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Liu2016">Liu2016</a>]
</td>
<td class="bibtexitem">
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
  Cheng-Yang Fu, and Alexander&nbsp;C. Berg.
 Ssd: Single shot multibox detector.
 In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors,
  <em>Computer Vision -- ECCV 2016</em>, pages 21--37, Cham, 2016. Springer
  International Publishing.
[&nbsp;<a href="OMR-Related_bib.html#Liu2016">bib</a>&nbsp;| 
<a href="https://link.springer.com/chapter/10.1007%2F978-3-319-46448-0_2">http</a>&nbsp;]
<blockquote><font size="-1">
We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300x300 input, SSD achieves 74.3% mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512x512 input, SSD achieves 76.9% mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Mandt2017">Mandt2017</a>]
</td>
<td class="bibtexitem">
S. Mandt, M.&nbsp;D. Hoffman, and D.&nbsp;M. Blei.
 Stochastic Gradient Descent as Approximate Bayesian Inference.
 <em>ArXiv e-prints</em>, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Mandt2017">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1704.04289">arXiv</a>&nbsp;| 
<a href="https://arxiv.org/abs/1704.04289">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Manning2008">Manning2008</a>]
</td>
<td class="bibtexitem">
Chirstopher&nbsp;D. Manning, Prabhakar Raghavan, and Hinrich Sch&uuml;tze.
 <em>Introduction to Information Retrieval</em>.
 Cambridge University Press, 2008.
[&nbsp;<a href="OMR-Related_bib.html#Manning2008">bib</a>&nbsp;| 
<a href="https://nlp.stanford.edu/IR-book/">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Marr2010">Marr2010</a>]
</td>
<td class="bibtexitem">
David Marr.
 <em>Vision</em>.
 W.H. Freeman and Company San Francisco, 2010.
[&nbsp;<a href="OMR-Related_bib.html#Marr2010">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.7551/mitpress/9780262514620.001.0001">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Marsden2012">Marsden2012</a>]
</td>
<td class="bibtexitem">
Alan Marsden.
 Interrogating melodic similarity: A definitive phenomenon or the
  product of interpretation?
 <em>Journal of New Music Research</em>, 41(4):323--335, 2012.
[&nbsp;<a href="OMR-Related_bib.html#Marsden2012">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Machacek2014">Machacek2014</a>]
</td>
<td class="bibtexitem">
Matou&scaron; Mach&aacute;&#x010D;ek and Ond&#X0159;ej Bojar.
 Results of the WMT14 Metrics Shared Task.
 <em>Ninth Workshop on Statistical Machine Translation</em>, pages
  293--301, 2014.
[&nbsp;<a href="OMR-Related_bib.html#Machacek2014">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Machacek2015">Machacek2015</a>]
</td>
<td class="bibtexitem">
Matou&scaron; Mach&aacute;&#x010D;ek and Ond&#X0159;ej Bojar.
 Evaluating Machine Translation Quality Using Short
  Segments Annotations.
 <em>The Prague Bulletin of Mathematical Linguistics</em>, 103(1), 2015.
[&nbsp;<a href="OMR-Related_bib.html#Machacek2015">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1515/pralin-2015-0005">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Mattheson1739">Mattheson1739</a>]
</td>
<td class="bibtexitem">
Johann Mattheson.
 <em>Der vollkommene Capellmeister</em>.
 Herold, Christian, Hamburg, 1739.
[&nbsp;<a href="OMR-Related_bib.html#Mattheson1739">bib</a>&nbsp;| 
<a href="https://imslp.org/wiki/Der_vollkommene_Capellmeister_(Mattheson,_Johann)">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="McKay2006">McKay2006</a>]
</td>
<td class="bibtexitem">
C.&nbsp;McKay and I.&nbsp;Fujinaga.
 jsymbolic: A feature extractor for midi files.
 In <em>International Computer Music Conference</em>, pages 302--305, New
  Orleans, LA, 2006.
[&nbsp;<a href="OMR-Related_bib.html#McKay2006">bib</a>&nbsp;| 
<a href="https://www.music.mcgill.ca/~cmckay/papers/musictech/McKay_ICMC_06_jSymbolic.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="McKay2004">McKay2004</a>]
</td>
<td class="bibtexitem">
Cory McKay and Ichiro Fujinaga.
 Automatic genre classification using large high-level musical feature
  sets.
 In <em>5th International Conference on Music Information Retrieval</em>,
  Barcelona, Spain, 2004.
[&nbsp;<a href="OMR-Related_bib.html#McKay2004">bib</a>&nbsp;| 
<a href="http://ismir2004.ismir.net/proceedings/p095-page-525-paper240.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="MerchanSanchez-Jara2015">MerchanSanchez-Jara2015</a>]
</td>
<td class="bibtexitem">
Javier Merch&aacute;n S&aacute;nchez-Jara.
 e-score; impact, perception and uses in music educational
  institutions.
 In Alves&nbsp;G.R. Felgueiras&nbsp;M.C., editor, <em>3rd International
  Conference on Technological Ecosystems for Enhancing Multiculturality</em>,
  volume Part F118785, pages 449--454. Association for Computing Machinery,
  2015.
[&nbsp;<a href="OMR-Related_bib.html#MerchanSanchez-Jara2015">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/2808580.2808647">DOI</a>&nbsp;]
<blockquote><font size="-1">
Parallel to the very first music expressions as a human activity,
	developed under some kind of guidelines and precepts, there have
	arisen relentless efforts and attempts to achieve the long-lasting
	and stable representation of the musical products being created,
	in a way that would allow to recreate them as closely as posible
	to the original event.from the very beginning until the present days,
	have been many notation systems, formats, and expressions developed
	with this purpose in mind. With the incursion and implementation
	of the new digital technologies in the musical activity spheres,
	a new paradigm emerge in the representation and transmission of the
	musical work, culminating nowadays with the born of a new music document
	typology tha may be called digital music score or e-Score. The aim
	of this paper is lay down and approach to this new type of object
	for music texts reading, editing and transmission through a quantitive
	research, being the scope of the study to analyze and understand
	how electronic music scores (e-Score) are received, used and perceived
	by music students and teachers in any of the music learning centers
	in our country. The main purpose is to provide scientific evidence,
	through statistical methods, of the level of implementation and use
	of these new support type for music knowledge transmission, as well
	as the most influential aspects implied in perception of therm amongst
	musical community in Spain.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Muge2000">Muge2000</a>]
</td>
<td class="bibtexitem">
F.&nbsp;Muge, I.&nbsp;Granado, M.&nbsp;Mengucci, P.&nbsp;Pina, V.&nbsp;Ramos, N.&nbsp;Sirakov, J.&nbsp;R.
  Caldas&nbsp;Pinto, A.&nbsp;Marcolino, M&aacute;rio Ramalho, P.&nbsp;Vieira, and A.&nbsp;Maia&nbsp;do
  Amaral.
 Automatic feature extraction and recognition for digital access of
  books of the renaissance.
 In Jos&eacute; Borbinha and Thomas Baker, editors, <em>Research and
  Advanced Technology for Digital Libraries</em>, pages 1--13, Berlin, Heidelberg,
  2000. Springer Berlin Heidelberg.
[&nbsp;<a href="OMR-Related_bib.html#Muge2000">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/3-540-45268-0_1">DOI</a>&nbsp;]
<blockquote><font size="-1">
Antique printed books constitute a heritage that should be preserved and used. With novel digitising techniques is now possible to have these books stored in digital format and accessible to a wider public. However it remains the problem of how to use them. DEBORA (Digital accEss to BOoks of the RenAissance) is a European project that aims to develop a system to interact with these books through world-wide networks. The main issue is to build a database accessible through client computers. That will require to built accompanying metadata that should characterise different components of the books as illuminated letters, banners, figures and key words in order to simplify and speed up the remote access. To solve these problems, digital image analysis algorithms regarding filtering, segmentation, separation of text from non-text, lines and word segmentation and word recognition were developed. Some novel ideas are presented and illustrated through examples.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Ng2008">Ng2008</a>]
</td>
<td class="bibtexitem">
Kia Ng and Paolo Nesi.
 <em>Interactive Multimedia Music Technologies</em>.
 IGI Global, 2008.
[&nbsp;<a href="OMR-Related_bib.html#Ng2008">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.4018/978-1-59904-150-6">DOI</a>&nbsp;]
<blockquote><font size="-1">
Many multimedia music content owners and distributors are converting their archives of music scores from paper into digital images, and to machine readable symbolic notation in order to survive in the business world. Interactive Multimedia Music Technologies discusses relevant state-of-the-art technologies and consists of analysis, knowledge, and application scenarios as surveyed, analyzed, and evaluated by industry professionals. Interactive Multimedia Music Technologies exemplifies the newest functionalities of multimedia interactive music to be used for valorizing cultural heritage, content and archives that are not currently distributed due to lack of safety, suitable coding models, and conversion technologies. Interactive Multimedia Music Technologies explains new and innovative methods of promoting music and products for entertainment, distance teaching, valorizing archives, and commercial and non-commercial purposes, and provides new services for those connected via personal computers, mobile and other devices, for both sighted and print-impaired consumers.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Nielsen2009">Nielsen2009</a>]
</td>
<td class="bibtexitem">
J.&nbsp;Nielsen.
 Statistical analysis of music corpora.
 Technical report, Dept. of Computer Science, University of
  Copenhagen, Copenhagen, Denmark, 2009.
[&nbsp;<a href="OMR-Related_bib.html#Nielsen2009">bib</a>&nbsp;| 
<a href="https://johanbrinch.com/static/papers/brinchj-2008_music.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Nivre2005">Nivre2005</a>]
</td>
<td class="bibtexitem">
Joakim Nivre and Jens Nilsson.
 Pseudo-projective Dependency Parsing.
 In <em>43rd Annual Meeting on Association for Computational
  Linguistics</em>, ACL '05, pages 99--106, Stroudsburg, PA, USA, 2005. Association
  for Computational Linguistics.
[&nbsp;<a href="OMR-Related_bib.html#Nivre2005">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.3115/1219840.1219853">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Ntirogiannis2013">Ntirogiannis2013</a>]
</td>
<td class="bibtexitem">
Konstantinos Ntirogiannis, Basilis Gatos, and Ioannis Pratikakis.
 Performance evaluation methodology for historical document image
  binarization.
 <em>IEEE Transactions on Image Processing</em>, 22(2):595--609, 2013.
[&nbsp;<a href="OMR-Related_bib.html#Ntirogiannis2013">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/TIP.2012.2219550">DOI</a>&nbsp;]
<blockquote><font size="-1">
Document image binarization is of great importance in the document image analysis and recognition pipeline since it affects further stages of the recognition process. The evaluation of a binarization method aids in studying its algorithmic behavior, as well as verifying its effectiveness, by providing qualitative and quantitative indication of its performance. This paper addresses a pixel-based binarization evaluation methodology for historical handwritten/machine-printed document images. In the proposed evaluation scheme, the recall and precision evaluation measures are properly modified using a weighting scheme that diminishes any potential evaluation bias. Additional performance metrics of the proposed evaluation scheme consist of the percentage rates of broken and missed text, false alarms, background noise, character enlargement, and merging. Several experiments conducted in comparison with other pixel-based evaluation measures demonstrate the validity of the proposed evaluation scheme.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="CPDL">CPDL</a>]
</td>
<td class="bibtexitem">
Rafael Ornes.
 Choral public domain library, 1998.
[&nbsp;<a href="OMR-Related_bib.html#CPDL">bib</a>&nbsp;| 
<a href="http://cpdl.org">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Palmer1999">Palmer1999</a>]
</td>
<td class="bibtexitem">
Stephen&nbsp;E. Palmer.
 <em>Vision science: Photons to phenomenology</em>.
 MIT press, 1999.
[&nbsp;<a href="OMR-Related_bib.html#Palmer1999">bib</a>&nbsp;| 
<a href="https://mitpress.mit.edu/books/vision-science">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Papineni2002">Papineni2002</a>]
</td>
<td class="bibtexitem">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
 BLEU: A Method for Automatic Evaluation of Machine
  Translation.
 In <em>40th Annual Meeting on Association for Computational
  Linguistics</em>, ACL '02, pages 311--318, Stroudsburg, PA, USA, 2002.
  Association for Computational Linguistics.
[&nbsp;<a href="OMR-Related_bib.html#Papineni2002">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.3115/1073083.1073135">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Phon-Amnuaisuk2009">Phon-Amnuaisuk2009</a>]
</td>
<td class="bibtexitem">
Somnuk Phon-Amnuaisuk.
 Estimating hmm parameters using particle swarm optimisation.
 In Mario Giacobini, Anthony Brabazon, Stefano Cagnoni, Gianni&nbsp;A.
  Di&nbsp;Caro, Anik&oacute; Ek&aacute;rt, Anna&nbsp;Isabel Esparcia-Alc&aacute;zar, Muddassar
  Farooq, Andreas Fink, and Penousal Machado, editors, <em>Applications of
  Evolutionary Computing</em>, pages 625--634, Berlin, Heidelberg, 2009. Springer
  Berlin Heidelberg.
[&nbsp;<a href="OMR-Related_bib.html#Phon-Amnuaisuk2009">bib</a>&nbsp;]
<blockquote><font size="-1">
A Hidden Markov Model (HMM) is a powerful model in describing temporal sequences. The HMM parameters are usually estimated using Baum-Welch algorithm. However, it is well known that the Baum-Welch algorithm tends to arrive at local optimal points. In this report, we investigate the potential of the Particle Swarm Optimisation (PSO) as an alternative method for HMM parameters estimation. The domain in this study is the recognition of handwritten music notations. Three observables: (i) sequence of ink patterns, (ii) stroke information and (iii) spatial information associated with eight musical symbols were recorded. Sixteen HMM models were built from the data. Eight HMM models for eight musical symbols were built from the parameters estimated using the Baum-Welch algorithm and the other eight models were built from the parameters estimated using PSO. The experiment shows that the performances of HMM models, using parameters estimated from PSO and Baum-Welch approach, are comparable. We suggest that PSO or a combination of PSO and Baum-Welch algorithm could be alternative approaches for the HMM parameters estimation.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Bellini2005">Bellini2005</a>]
</td>
<td class="bibtexitem">
Pierfrancesco Bellini, Paolo Nesi, and Giorgio Zoia.
 Symbolic Music Representation in MPEG.
 <em>IEEE MultiMedia</em>, 12(4):42--49, 2005.
[&nbsp;<a href="OMR-Related_bib.html#Bellini2005">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/MMUL.2005.82">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Presgurvic2005">Presgurvic2005</a>]
</td>
<td class="bibtexitem">
G&eacute;rard Presgurvic.
 Songbook romeo &amp; julia, 2005.
[&nbsp;<a href="OMR-Related_bib.html#Presgurvic2005">bib</a>&nbsp;| 
<a href="https://www.musicalvienna.at/de/souvenirs/12/ANDERE-MUSICALS/10/Songbook-Romeo-und-Julia">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="IMSLP">IMSLP</a>]
</td>
<td class="bibtexitem">
Project Petrucci LLC.
 International music score library project, 2006.
[&nbsp;<a href="OMR-Related_bib.html#IMSLP">bib</a>&nbsp;| 
<a href="http://imslp.org/">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Rashid2017">Rashid2017</a>]
</td>
<td class="bibtexitem">
S.&nbsp;F. Rashid, A.&nbsp;Akmal, M.&nbsp;Adnan, A.&nbsp;A. Aslam, and A.&nbsp;Dengel.
 Table recognition in heterogeneous documents using machine learning.
 In <em>2017 14th IAPR International Conference on Document Analysis
  and Recognition (ICDAR)</em>, volume&nbsp;01, pages 777--782, Nov 2017.
[&nbsp;<a href="OMR-Related_bib.html#Rashid2017">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICDAR.2017.132">DOI</a>&nbsp;]
<blockquote><font size="-1">
Tables are an easy way to represent information in a structural form. Table recognition is important for the extraction of such information from document images. Usually, modern OCR systems provide textual information coming from tables without recognizing actual table structure. However, recognition of table structure is important to get the contextual meaning of the contents. Table structure recognition in heterogeneous documents is challenging due to a variety of table layouts. It becomes harder where no physical rulings are present in a table. This work proposes a novel learning based methodology for the recognition of table contents in heterogeneous document images. Textual contents of documents are classified as table or non-table elements using a pre-trained neural network model. The output of the neural network is further enhanced by applying a contextual post processing on each element to correct the classifications errors if any. The system is trained using a subset of UNLV and UW3 document images and depicted more than 97% accuracy on a test set in detection of table and non-table elements.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Redmon2017">Redmon2017</a>]
</td>
<td class="bibtexitem">
J.&nbsp;Redmon and A.&nbsp;Farhadi.
 Yolo9000: Better, faster, stronger.
 In <em>2017 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)</em>, pages 6517--6525, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Redmon2017">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/CVPR.2017.690">DOI</a>&nbsp;]
<blockquote><font size="-1">
We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that dont have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Ren2015">Ren2015</a>]
</td>
<td class="bibtexitem">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
 Faster r-cnn: Towards real-time object detection with region proposal
  networks.
 In C.&nbsp;Cortes, N.&nbsp;D. Lawrence, D.&nbsp;D. Lee, M.&nbsp;Sugiyama, and R.&nbsp;Garnett,
  editors, <em>Advances in Neural Information Processing Systems 28</em>, pages
  91--99. Curran Associates, Inc., 2015.
[&nbsp;<a href="OMR-Related_bib.html#Ren2015">bib</a>&nbsp;| 
<a href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Socher2011">Socher2011</a>]
</td>
<td class="bibtexitem">
Richard Socher, Cliff Chiung-Yu Lin, Andrew Y.&nbsp;Ng, and
  Christopher D.&nbsp;Manning.
 Parsing Natural Scenes and Natural Language with
  Recursive Neural Networks.
 In <em>28th International Conference on Machine Learning</em>, 2011.
[&nbsp;<a href="OMR-Related_bib.html#Socher2011">bib</a>&nbsp;| 
<a href="http://ai.stanford.edu/~ang/papers/icml11-ParsingWithRecursiveNeuralNetworks.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Roman2018">Roman2018</a>]
</td>
<td class="bibtexitem">
Miguel&nbsp;A. Rom&aacute;n, Antonio Pertusa, and Jorge Calvo-Zaragoza.
 And end-to-end framework for audio-to-score music transcription on
  monophonic excerpts.
 In <em>19th International Society for Music Information Retrieval
  Conference</em>, pages 34--41, Paris, France, 2018.
[&nbsp;<a href="OMR-Related_bib.html#Roman2018">bib</a>&nbsp;| 
<a href="http://ismir2018.ircam.fr/doc/pdfs/87_Paper.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Ronneberger2015">Ronneberger2015</a>]
</td>
<td class="bibtexitem">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
 U-Net: Convolutional Networks for Biomedical Image
  Segmentation.
 In Nassir Navab, Joachim Hornegger, William&nbsp;M. Wells, and
  Alejandro&nbsp;F. Frangi, editors, <em>Medical Image Computing and
  Computer-Assisted Intervention -- MICCAI 2015: 18th International Conference,
  Munich, Germany, October 5-9, 2015, Proceedings, Part III</em>, pages 234--241,
  Cham, 2015. Springer International Publishing.
[&nbsp;<a href="OMR-Related_bib.html#Ronneberger2015">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-319-24574-4_28">DOI</a>&nbsp;]
<blockquote><font size="-1">
There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Sampson1985">Sampson1985</a>]
</td>
<td class="bibtexitem">
Geoffrey Sampson.
 <em>Writing Systems: A Linguistic Introduction</em>.
 Stanford University Press, 1985.
[&nbsp;<a href="OMR-Related_bib.html#Sampson1985">bib</a>&nbsp;| 
<a href="https://books.google.cz/books?id=tVcdNRvwoDkC">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Sauvola2000">Sauvola2000</a>]
</td>
<td class="bibtexitem">
J.&nbsp;Sauvola and M.&nbsp;Pietik&auml;inen.
 Adaptive document image binarization.
 <em>Pattern Recognition</em>, 33(2):225--236, 2000.
[&nbsp;<a href="OMR-Related_bib.html#Sauvola2000">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/S0031-3203(99)00055-2">DOI</a>&nbsp;]
<blockquote><font size="-1">
A new method is presented for adaptive document image binarization, where the page is considered as a collection of subcomponents such as text, background and picture. The problems caused by noise, illumination and many source type-related degradations are addressed. Two new algorithms are applied to determine a local threshold for each pixel. The performance evaluation of the algorithm utilizes test images with ground-truth, evaluation metrics for binarization of textual and synthetic images, and a weight-based ranking procedure for the final result presentation. The proposed algorithms were tested with images including different types of document components and degradations. The results were compared with a number of known techniques in the literature. The benchmarking results show that the method adapts and performs well in each case qualitatively and quantitatively.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Saxena2017">Saxena2017</a>]
</td>
<td class="bibtexitem">
Lalit&nbsp;Prakash Saxena.
 Niblack's binarization method and its modifications to real-time
  applications: a review.
 <em>Artificial Intelligence Review</em>, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Saxena2017">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s10462-017-9574-2">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Schmidhuber2015">Schmidhuber2015</a>]
</td>
<td class="bibtexitem">
J&uuml;rgen Schmidhuber.
 Deep learning in neural networks: An overview.
 <em>Neural networks</em>, 61:85--117, 2015.
[&nbsp;<a href="OMR-Related_bib.html#Schmidhuber2015">bib</a>&nbsp;| 
<a href="https://arxiv.org/pdf/1404.7828.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Ewert2014">Ewert2014</a>]
</td>
<td class="bibtexitem">
Sebastian Ewert, Bryan Pardo, Meinard M&uuml;ller, and Mark
  D.&nbsp;Plumbley.
 Score-Informed Source Separation for Musical Audio
  Recordings: An overview.
 <em>IEEE Signal Process. Mag.</em>, 31(3):116--124, 2014.
[&nbsp;<a href="OMR-Related_bib.html#Ewert2014">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/MSP.2013.2296076">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Selfridge-Field1997">Selfridge-Field1997</a>]
</td>
<td class="bibtexitem">
Eleanor Selfridge-Field.
 <em>Beyond MIDI: The Handbook of Musical Codes</em>.
 MIT Press, Cambridge, MA, USA, 1997.
[&nbsp;<a href="OMR-Related_bib.html#Selfridge-Field1997">bib</a>&nbsp;| 
<a href="https://mitpress.mit.edu/books/beyond-midi">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Sermanet2013">Sermanet2013</a>]
</td>
<td class="bibtexitem">
Pierre Sermanet, David Eigen, Xiang Zhang, Micha&euml;l Mathieu, Rob Fergus, and
  Yann LeCun.
 Overfeat: Integrated recognition, localization and detection using
  convolutional networks.
 <em>CoRR</em>, abs/1312.6229, 2013.
[&nbsp;<a href="OMR-Related_bib.html#Sermanet2013">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1312.6229">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Serra2017">Serra2017</a>]
</td>
<td class="bibtexitem">
Xavier Serra.
 The computational study of a musical culture through its digital
  traces.
 <em>Acta Musicologica. 2017; 89 (1): 24-44.</em>, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Serra2017">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Shahab2010">Shahab2010</a>]
</td>
<td class="bibtexitem">
Asif Shahab, Faisal Shafait, Thomas Kieninger, and Andreas Dengel.
 An open approach towards the benchmarking of table structure
  recognition systems.
 In <em>Proceedings of the 9th IAPR International Workshop on
  Document Analysis Systems</em>, DAS '10, pages 113--120, New York, NY, USA, 2010.
  ACM.
[&nbsp;<a href="OMR-Related_bib.html#Shahab2010">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/1815330.1815345">DOI</a>&nbsp;| 
<a href="http://doi.acm.org/10.1145/1815330.1815345">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Siagian2007">Siagian2007</a>]
</td>
<td class="bibtexitem">
C.&nbsp;Siagian and L.&nbsp;Itti.
 Rapid biologically-inspired scene classification using features
  shared with visual attention.
 <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>,
  29(2):300--312, 2007.
[&nbsp;<a href="OMR-Related_bib.html#Siagian2007">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/TPAMI.2007.40">DOI</a>&nbsp;]
<blockquote><font size="-1">
We describe and validate a simple context-based scene recognition
	algorithm for mobile robotics applications. The system can differentiate
	outdoor scenes from various sites on a college campus using a multiscale
	set of early-visual features, which capture the "gist" of the scene
	into a low-dimensional signature vector. Distinct from previous approaches,
	the algorithm presents the advantage of being biologically plausible
	and of having low-computational complexity, sharing its low-level
	features with a model for visual attention that may operate concurrently
	on a robot. We compare classification accuracy using scenes filmed
	at three outdoor sites on campus (13,965 to 34,711 frames per site).
	Dividing each site into nine segments, we obtain segment classification
	rates between 84.21 percent and 88.62 percent. Combining scenes from
	all sites (75,073 frames in total) yields 86.45 percent correct classification,
	demonstrating the generalization and scalability of the approach
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Sigtia2015">Sigtia2015</a>]
</td>
<td class="bibtexitem">
Siddharth Sigtia, Emmanouil Benetos, and Simon Dixon.
 An End-to-End Neural Network for Polyphonic Piano
  Music Transcription.
 <em>Computing Research Repository</em>, abs/1508.01774, 2015.
[&nbsp;<a href="OMR-Related_bib.html#Sigtia2015">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1508.01774">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Simard2003">Simard2003</a>]
</td>
<td class="bibtexitem">
P&nbsp;Y Simard, D Steinkraus, and John&nbsp;C Platt.
 Best practices for convolutional neural networks applied to visual
  document analysis.
 <em>Document Analysis and Recognition, 2003. Proceedings. Seventh
  International Conference on</em>, pages 958--963, 2003.
[&nbsp;<a href="OMR-Related_bib.html#Simard2003">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICDAR.2003.1227801">DOI</a>&nbsp;]
<blockquote><font size="-1">
Not Available
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Sloboda2005">Sloboda2005</a>]
</td>
<td class="bibtexitem">
John Sloboda.
 <em>Exploring the musical mind</em>.
 Oxford University Press, 2005.
[&nbsp;<a href="OMR-Related_bib.html#Sloboda2005">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1093/acprof:oso/9780198530121.001.0001">DOI</a>&nbsp;| 
<a href="https://books.google.at/books?id=xNK6LTIW3D8C&amp;printsec=frontcover#v=onepage&amp;q&amp;f=false">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Sordo2015">Sordo2015</a>]
</td>
<td class="bibtexitem">
Mohamed Sordo, Mitsunori Ogihara, and Stefan Wuchty.
 Analysis of the evolution of research groups and topics in the
  ISMIR conference.
 In <em>16th International Society for Music Information Retrieval
  Conference</em>, volume 205, 2015.
[&nbsp;<a href="OMR-Related_bib.html#Sordo2015">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Stadelmann2018">Stadelmann2018</a>]
</td>
<td class="bibtexitem">
T.&nbsp;Stadelmann, M.&nbsp;Amirian, I.&nbsp;Arabaci, M.&nbsp;Arnold, G.&nbsp;Fran&ccedil;ois
  Duivesteijn, I.&nbsp;Elezi, M.&nbsp;Geiger, S.&nbsp;L&ouml;rwald, B.&nbsp;B. Meier,
  K.&nbsp;Rombach, and L.&nbsp;Tuggener.
 Deep Learning in the Wild.
 <em>ArXiv e-prints</em>, 2018.
[&nbsp;<a href="OMR-Related_bib.html#Stadelmann2018">bib</a>&nbsp;| 
<a href="https://arxiv.org/abs/1807.04950">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Stamatopoulos2013">Stamatopoulos2013</a>]
</td>
<td class="bibtexitem">
Nikolaos Stamatopoulos, Basilis Gatos, Georgios Louloudis, Umpada
  Pal, and Alireza Alaei.
 ICDAR2013 Handwriting Segmentation Contest.
 In <em>2013 12th International Conference on Document Analysis and
  Recognition</em>, 2013.
[&nbsp;<a href="OMR-Related_bib.html#Stamatopoulos2013">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICDAR.2013.283">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Stauffer2018">Stauffer2018</a>]
</td>
<td class="bibtexitem">
Michael Stauffer, Andreas Fischer, and Kaspar Riesen.
 Keyword spotting in historical handwritten documents based on graph
  matching.
 <em>Pattern Recognition</em>, 81:240--253, 2018.
[&nbsp;<a href="OMR-Related_bib.html#Stauffer2018">bib</a>&nbsp;| 
<a href="https://doi.org/10.1016/j.patcog.2018.04.001">DOI</a>&nbsp;| 
<a href="http://www.sciencedirect.com/science/article/pii/S0031320318301274">http</a>&nbsp;]
<blockquote><font size="-1">
In the last decades historical handwritten documents have become increasingly available in digital form. Yet, the accessibility to these documents with respect to browsing and searching remained limited as full automatic transcription is often not possible or not sufficiently accurate. This paper proposes a novel reliable approach for template-based keyword spotting in historical handwritten documents. In particular, our framework makes use of different graph representations for segmented word images and a sophisticated matching procedure. Moreover, we extend our method to a spotting ensemble. In an exhaustive experimental evaluation on four widely used benchmark datasets we show that the proposed approach is able to keep up or even outperform several state-of-the-art methods for template- and learning-based keyword spotting.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Downie2008">Downie2008</a>]
</td>
<td class="bibtexitem">
J. Stephen Downie.
 The music information retrieval evaluation exchange (2005--2007):
  A window into music information retrieval research.
 <em>Acoust. Sci. &amp; Tech.</em>, 29(4):247--255, 2008.
[&nbsp;<a href="OMR-Related_bib.html#Downie2008">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1250/ast.29.247">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Stone1996">Stone1996</a>]
</td>
<td class="bibtexitem">
Kurt Stone.
 <em>Music Notation in the Twentieth Century, A Practical
  Guidebook</em>.
 W. W. Norton &amp; Company, 1996.
[&nbsp;<a href="OMR-Related_bib.html#Stone1996">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Su2013">Su2013</a>]
</td>
<td class="bibtexitem">
Bolan Su, Shijian Lu, and Chew&nbsp;Lim Tan.
 Robust document image binarization technique for degraded document
  images.
 <em>IEEE Transactions on Image Processing</em>, 22(4):1408--1417, 2013.
[&nbsp;<a href="OMR-Related_bib.html#Su2013">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/TIP.2012.2231089">DOI</a>&nbsp;]
<blockquote><font size="-1">
Segmentation of text from badly degraded document images is a very challenging task due to the high inter/intra-variation between the document background and the foreground text of different document images. In this paper, we propose a novel document image binarization technique that addresses these issues by using adaptive image contrast. The adaptive image contrast is a combination of the local image contrast and the local image gradient that is tolerant to text and background variation caused by different types of document degradations. In the proposed technique, an adaptive contrast map is first constructed for an input degraded document image. The contrast map is then binarized and combined with Canny's edge map to identify the text stroke edge pixels. The document text is further segmented by a local threshold that is estimated based on the intensities of detected text stroke edge pixels within a local window. The proposed method is simple, robust, and involves minimum parameter tuning. It has been tested on three public datasets that are used in the recent document image binarization contest (DIBCO) 2009 &amp; 2011 and handwritten-DIBCO 2010 and achieves accuracies of 93.5%, 87.8%, and 92.03%, respectively, that are significantly higher than or close to that of the best-performing methods reported in the three contests. Experiments on the Bickley diary dataset that consists of several challenging bad quality document images also show the superior performance of our proposed method, compared with other techniques.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Szegedy2017">Szegedy2017</a>]
</td>
<td class="bibtexitem">
Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander&nbsp;A Alemi.
 Inception-v4, inception-resnet and the impact of residual connections
  on learning.
 In <em>Thirty-First AAAI Conference on Artificial Intelligence
  (AAAI-17)</em>, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Szegedy2017">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Lilypond2014">Lilypond2014</a>]
</td>
<td class="bibtexitem">
The LilyPond Developement Team.
 <em>LilyPond - Essay on automated music engraving</em>, 2014.
[&nbsp;<a href="OMR-Related_bib.html#Lilypond2014">bib</a>&nbsp;| 
<a href="http://www.lilypond.org/">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Toiviainen2016">Toiviainen2016</a>]
</td>
<td class="bibtexitem">
P.&nbsp;Toiviainen and T.&nbsp;Eerola.
 MIDI toolbox 1.1.
 https://github.com/miditoolbox/, 2016.
[&nbsp;<a href="OMR-Related_bib.html#Toiviainen2016">bib</a>&nbsp;| 
<a href="https://github.com/miditoolbox/">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Trier1995">Trier1995</a>]
</td>
<td class="bibtexitem">
O.&nbsp;D. Trier and A.&nbsp;K. Jain.
 Goal-directed evaluation of binarization methods.
 <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>,
  17(12):1191--1201, Dec 1995.
[&nbsp;<a href="OMR-Related_bib.html#Trier1995">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/34.476511">DOI</a>&nbsp;]
<blockquote><font size="-1">
This paper presents a methodology for evaluation of low-level image analysis methods, using binarization (two-level thresholding) as an example. Binarization of scanned gray scale images is the first step in most document image analysis systems. Selection of an appropriate binarization method for an input image domain is a difficult problem. Typically, a human expert evaluates the binarized images according to his/her visual criteria. However, to conduct an objective evaluation, one needs to investigate how well the subsequent image analysis steps will perform on the binarized image. We call this approach goal-directed evaluation, and it can be used to evaluate other low-level image processing methods as well. Our evaluation of binarization methods is in the context of digit recognition, so we define the performance of the character recognition module as the objective measure. Eleven different locally adaptive binarization methods were evaluated, and Niblack's method gave the best performance.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Trier1995a">Trier1995a</a>]
</td>
<td class="bibtexitem">
O.D. Trier and T. Taxt.
 Evaluation of binarization methods for utility map images.
 <em>Proceedings of 1st International Conference on Image
  Processing</em>, 2:31--36, 1995.
[&nbsp;<a href="OMR-Related_bib.html#Trier1995a">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICIP.1994.413515">DOI</a>&nbsp;]
<blockquote><font size="-1">
This paper presents an evaluation of locally adaptive binarization methods for gray scale images with low contrast, variable background intensity and noise. Such low quality occurs frequently in utility maps and excludes the use of global binarization methods. Only robust locally adaptive binarization methods with no need for on-line tuning of the parameters were considered since the gray scale images of utility maps often consist of a billion (10&lt;sup&gt;9&lt;/sup&gt;) pixels or more. Eight locally adaptive binarization methods were tested on five different images. The postprocessing step (PS) of Yanowitz and Bruckstein's (1989) method improved all the other best binarization methods. Niblack's (1986) method with PS gave the best performance. Eikvil, Taxt and Moen's (1991) method with PS, and Yanowitz and Bruckstein's method did almost as well. Comparison was also made on the CPU requirement
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Uijlings2013">Uijlings2013</a>]
</td>
<td class="bibtexitem">
J.&nbsp;R.&nbsp;R. Uijlings, K.&nbsp;E.&nbsp;A. van&nbsp;de Sande, T.&nbsp;Gevers, and A.&nbsp;W.&nbsp;M. Smeulders.
 Selective search for object recognition.
 <em>International Journal of Computer Vision</em>, 104(2):154--171,
  2013.
[&nbsp;<a href="OMR-Related_bib.html#Uijlings2013">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s11263-013-0620-5">DOI</a>&nbsp;| 
<a href="https://doi.org/10.1007/s11263-013-0620-5">http</a>&nbsp;]
<blockquote><font size="-1">
This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 % recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software:  http://disi.unitn.it/&nbsp;uijlings/SelectiveSearch.html ).
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Urbano2013">Urbano2013</a>]
</td>
<td class="bibtexitem">
Juli&aacute;n Urbano.
 MIREX 2013 Symbolic Melodic Similarity: A Geometric Model supported
  with Hybrid Sequence Alignment.
 Technical report, Music Information Retrieval Evaluation eXchange,
  2013.
[&nbsp;<a href="OMR-Related_bib.html#Urbano2013">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Vajda2015">Vajda2015</a>]
</td>
<td class="bibtexitem">
Szil&aacute;rd Vajda, Yves Rangoni, and Hubert Cecotti.
 Semi-automatic ground truth generation using unsupervised clustering
  and limited manual labeling: Application to handwritten character
  recognition.
 <em>Pattern Recognition Letters</em>, 58:23--28, 2015.
[&nbsp;<a href="OMR-Related_bib.html#Vajda2015">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.patrec.2015.02.001">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Villegas2015">Villegas2015</a>]
</td>
<td class="bibtexitem">
M.&nbsp;Villegas, J.&nbsp;A. S&aacute;nchez, and E.&nbsp;Vidal.
 Optical modelling and language modelling trade-off for handwritten
  text recognition.
 In <em>2015 13th International Conference on Document Analysis and
  Recognition (ICDAR)</em>, pages 831--835, 2015.
[&nbsp;<a href="OMR-Related_bib.html#Villegas2015">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICDAR.2015.7333878">DOI</a>&nbsp;]
<blockquote><font size="-1">
Training the models needed for Automatic Handwritten Text Recognition of historical documents generally requires a significant amount of human effort. This is mainly due to the great differences that often exist between collections and to the lack of linguistic resources from the period when the documents were written, which results in a need of manual data labelling effort. This paper presents a study on the reuse of models trained with data from a different collection, focusing on the contribution that the language model and the optical models have on the performance. An empirical evaluation is performed using data from Jeremy Bentham manuscripts with the aim of recognising a manuscript about a very different topic written by Jane Austen.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Mnih2014">Mnih2014</a>]
</td>
<td class="bibtexitem">
Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray
  Kavukcuoglu.
 Recurrent Models of Visual Attention.
 <em>Computing Research Repository</em>, abs/1406.6247, 2014.
[&nbsp;<a href="OMR-Related_bib.html#Mnih2014">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1406.6247">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Vonikakis2011">Vonikakis2011</a>]
</td>
<td class="bibtexitem">
V. Vonikakis, I. Andreadis, and N. Papamarkos.
 Robust document binarization with OFF center-surround cells.
 <em>Pattern Analysis and Applications</em>, 14(3):219--234, 2011.
[&nbsp;<a href="OMR-Related_bib.html#Vonikakis2011">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s10044-011-0214-1">DOI</a>&nbsp;]
<blockquote><font size="-1">
This paper presents a new method for degraded-document binarization, inspired by the attributes of the Human Visual System (HVS). It can deal with various types of degradations, such as uneven illumination, shadows, low contrast, smears, and heavy noise densities. The proposed algorithm combines the characteristics of the OFF center-surround cells of the HVS with the classic Otsu binarization technique. Cells of two different scales are combined, increasing the efficiency of the algorithm and reducing the extracted noise in the final output. A new response function, which regulates the output of the cell according to the local contrast and the local lighting conditions is also introduced. The Otsu technique is used to binarize the outputs of the OFF center-surround cells. Quantitative experiments performed on a set of various computer-generated degradations, such as noise, shadow, and low contrast demonstrate the superior performance of the proposed method against six other well-established techniques. Qualitative and OCR comparisons also confirm these results.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Vos1989">Vos1989</a>]
</td>
<td class="bibtexitem">
Piet&nbsp;G. Vos and Jim&nbsp;M. Troost.
 Ascending and descending melodic intervals: Statistical findings and
  their perceptual relevance.
 <em>Music Perception</em>, 6(4):383--396, 1989.
[&nbsp;<a href="OMR-Related_bib.html#Vos1989">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.2307/40285439">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Wauthier2013">Wauthier2013</a>]
</td>
<td class="bibtexitem">
Fabian Wauthier, Michael Jordan, and Nebojsa Jojic.
 Efficient ranking from pairwise comparisons.
 In <em>30th International Conference on Machine Learning</em>, pages
  109--117, 2013.
[&nbsp;<a href="OMR-Related_bib.html#Wauthier2013">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Wilson2003">Wilson2003</a>]
</td>
<td class="bibtexitem">
D.Randall Wilson and Tony&nbsp;R. Martinez.
 The general inefficiency of batch training for gradient descent
  learning.
 <em>Neural Networks</em>, 16(10):1429--1451, 2003.
[&nbsp;<a href="OMR-Related_bib.html#Wilson2003">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/S0893-6080(03)00138-2">DOI</a>&nbsp;| 
<a href="http://www.sciencedirect.com/science/article/pii/S0893608003001382">http</a>&nbsp;]
<blockquote><font size="-1">
Abstract Gradient descent training of neural networks can be done
	in either a batch or on-line manner. A widely held myth in the neural
	network community is that batch training is as fast or faster and/or
	more ‘correct’ than on-line training because it supposedly uses a
	better approximation of the true gradient for its weight updates.
	This paper explains why batch training is almost always slower than
	on-line training—often orders of magnitude slower—especially on large
	training sets. The main reason is due to the ability of on-line training
	to follow curves in the error surface throughout each epoch, which
	allows it to safely use a larger learning rate and thus converge
	with less iterations through the training data. Empirical results
	on a large (20,000-instance) speech recognition task and on 26 other
	learning tasks demonstrate that convergence can be reached significantly
	faster using on-line training than batch training, with no apparent
	difference in accuracy.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Wittlich1978">Wittlich1978</a>]
</td>
<td class="bibtexitem">
Gary Wittlich, Donald Byrd, and Rosalee Nerheim.
 A system for interactive encoding of music scores under computer
  control.
 <em>Computers and the Humanities</em>, 12(4):309--319, Dec 1978.
[&nbsp;<a href="OMR-Related_bib.html#Wittlich1978">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/BF02400103">DOI</a>&nbsp;| 
<a href="https://doi.org/10.1007/BF02400103">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Xia2017">Xia2017</a>]
</td>
<td class="bibtexitem">
Gus&nbsp;G. Xia and Roger&nbsp;B. Dannenberg.
 Improvised duet interaction: Learning improvisation techniques for
  automatic accompaniment.
 In Cumhur Erkut, editor, <em>New Interfaces for Musical Expression</em>,
  Aalborg University Copenhagen, Denmark, 2017.
[&nbsp;<a href="OMR-Related_bib.html#Xia2017">bib</a>&nbsp;| 
<a href="http://homes.create.aau.dk/dano/nime17/papers/0022/paper0022.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Zhai2010">Zhai2010</a>]
</td>
<td class="bibtexitem">
Yun Zhai.
 <em>Non-Numerical Ranking Based on Pairwise
  Comparisons</em>.
 PhD thesis, McMaster University, Hamilton, Ontario, 2010.
[&nbsp;<a href="OMR-Related_bib.html#Zhai2010">bib</a>&nbsp;| 
<a href="http://hdl.handle.net/11375/19476">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Zhang2016">Zhang2016</a>]
</td>
<td class="bibtexitem">
Xingxing Zhang, Liang Lu, and Mirella Lapata.
 Top-down Tree Long Short-Term Memory Networks.
 In <em>2016 Conference of the North American Chapter of the
  Association for Computational Linguistics: Human Language Technologies</em>,
  pages 310--320, San Diego, California, 2016. Association for Computational
  Linguistics.
[&nbsp;<a href="OMR-Related_bib.html#Zhang2016">bib</a>&nbsp;| 
<a href="http://www.aclweb.org/anthology/N16-1035">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Zitnick2014">Zitnick2014</a>]
</td>
<td class="bibtexitem">
Larry Zitnick and Piotr Dollar.
 Edge boxes: Locating object proposals from edges.
 In <em>ECCV</em>. European Conference on Computer Vision, 2014.
[&nbsp;<a href="OMR-Related_bib.html#Zitnick2014">bib</a>&nbsp;| 
<a href="https://www.microsoft.com/en-us/research/publication/edge-boxes-locating-object-proposals-from-edges/">http</a>&nbsp;]
<blockquote><font size="-1">
The use of object proposals is an effective recent approach for increasing
	the computational efficiency of object detection. We propose a novel
	method for generating object bounding box proposals using edges.
	Edges provide a sparse yet informative representation of an image.
	Our main observation is that the number of contours that are wholly
	contained in a bounding box is indicative of the likelihood of the
	box containing an object. We propose a simple box objectness score
	that measures the number of edges that exist in the box minus those
	that are members of contours that overlap the box's boundary. Using
	efficient data structures, millions of candidate boxes can be evaluated
	in a fraction of a second, returning a ranked set of a few thousand
	top-scoring proposals. Using standard metrics, we show results that
	are significantly more accurate than the current state-of-the-art
	while being faster to compute. In particular, given just 1000 proposals
	we achieve over 96	over 75	runs in 0.25 seconds and we additionally demonstrate a near real-time
	variant with only minor loss in accuracy.
</font></blockquote>
<p>
</td>
</tr>
</table><hr><p><em>This file was generated by
<a href="http://www.lri.fr/~filliatr/bibtex2html/">bibtex2html</a> 1.98.</em></p>
</body>
</html>
