<h1>OMR-Related-Research.bib</h1><a name="Agarwal2008"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Agarwal2008">Agarwal2008</a>,
  author = {{A}garwal, {A}bhaya and {L}avie, {A}lon},
  title = {{METEOR}, {M}-{BLEU} and {M}-{TER}: {E}valuation {M}etrics for {H}igh-correlation with {H}uman {R}ankings of {M}achine {T}ranslation {O}utput},
  booktitle = {Third Workshop on Statistical Machine Translation},
  year = {2008},
  series = {StatMT '08},
  pages = {115--118},
  address = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid = {1626406},
  file = {:pdfs/2008 - METEOR, M-BLEU and M-TER - Evaluation Metrics for High-Correlation with Human Rankings of Machine Translation Output.pdf:PDF},
  isbn = {978-1-932432-09-1},
  location = {Columbus, Ohio},
  numpages = {4},
  url = {<a href="http://dl.acm.org/citation.cfm?id=1626394.1626406">http://dl.acm.org/citation.cfm?id=1626394.1626406</a>}
}
</pre>

<a name="Akiyama1990"></a><pre>
@article{<a href="docs\OMR-Related.html#Akiyama1990">Akiyama1990</a>,
  author = {Teruo Akiyama and Norihiro Hagita},
  title = {Automated entry system for printed documents},
  journal = {Pattern Recognition},
  year = {1990},
  volume = {23},
  number = {11},
  pages = {1141--1154},
  issn = {0031-3203},
  abstract = {This paper proposes a system for automatically reading either Japanese or English documents that have complex layout structures that include graphics. First, document image segmentation and character segmentation are carried out using three basic features and the knowledge of document layout rules. Next, multi-font character recognition is performed based on feature vector matching. Recognition experiments with a prototype system for a variety of complex printed documents shows that the proposed system is capable of reading different types of printed documents at an accuracy rate of 94.8â€“97.2%.},
  doi = {10.1016/0031-3203(90)90112-X},
  keywords = {Document entry system, Image processing, Document processing, Layout structure recognition, Character recognition, Feature extraction, Character segmentation},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/003132039090112X">http://www.sciencedirect.com/science/article/pii/003132039090112X</a>}
}
</pre>

<a name="Alvaro2016"></a><pre>
@article{<a href="docs\OMR-Related.html#Alvaro2016">Alvaro2016</a>,
  author = {Francisco \'{A}lvaro and Joan-Andreu S\'{a}nchez and Jos\'{e}-Miguel Bened\'{i}},
  title = {An integrated grammar-based approach for mathematical expression recognition},
  journal = {Pattern Recognition},
  year = {2016},
  volume = {51},
  pages = {135--147},
  doi = {10.1016/j.patcog.2015.09.013},
  file = {:pdfs/2016 - An Integrated Grammar Based Approach for Mathematical Expression Recognition.pdf:PDF}
}
</pre>

<a name="Andre2014"></a><pre>
@article{<a href="docs\OMR-Related.html#Andre2014">Andre2014</a>,
  author = {Ga{\"{e}}tan Andr{\'{e}} and Viviane Kostrubiec and Jean-Christophe Buisson and Jean-Michel Albaret and Pier-Giorgio Zanone},
  title = {A parsimonious oscillatory model of handwriting},
  journal = {Biological Cybernetics},
  year = {2014},
  volume = {108},
  number = {3},
  pages = {321--336},
  doi = {10.1007/s00422-014-0600-z},
  groups = {handwriting},
  file = {:pdfs/2014 - A parsimonious oscillatory model of handwriting.pdf:PDF},
  publisher = {Springer Science Business Media}
}
</pre>

<a name="Ann2010"></a><pre>
@article{<a href="docs\OMR-Related.html#Ann2010">Ann2010</a>,
  author = {{H}sing-{Y}en {A}nn and {C}hang-{B}iau {Y}ang and {Y}ung-{H}sing {P}eng and {B}ern-{C}herng {L}iaw},
  title = {{E}fficient algorithms for the block edit problems},
  journal = {Information and Computation},
  year = {2010},
  volume = {208},
  number = {3},
  pages = {221--229},
  doi = {10.1016/j.ic.2009.12.001},
  groups = {evaluation},
  file = {:pdfs/2010 - Efficient algorithms for the block edit problems.pdf:PDF},
  publisher = {Elsevier {BV}}
}
</pre>

<a name="Bahdanau2014"></a><pre>
@article{<a href="docs\OMR-Related.html#Bahdanau2014">Bahdanau2014</a>,
  author = {{D}zmitry {B}ahdanau and {K}yunghyun {C}ho and {Y}oshua {B}engio},
  title = {{N}eural {M}achine {T}ranslation by {J}ointly {L}earning to {A}lign and {T}ranslate},
  journal = {{Computing Research Repository}},
  year = {2014},
  volume = {abs/1409.0473},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {<a href="http://dblp.uni-trier.de/rec/bib/journals/corr/BahdanauCB14">http://dblp.uni-trier.de/rec/bib/journals/corr/BahdanauCB14</a>},
  file = {:pdfs/2014 - Neural Machine Translation by Jountly Learning To Align and Translate.pdf:PDF},
  url = {<a href="http://arxiv.org/abs/1409.0473">http://arxiv.org/abs/1409.0473</a>}
}
</pre>

<a name="Barate2018"></a><pre>
@article{<a href="docs\OMR-Related.html#Barate2018">Barate2018</a>,
  author = {Adriano Barat{\`{e}} and Goffredo Haus and Luca A. Ludovico},
  title = {Advanced Experience of Music through 5G Technologies},
  journal = {IOP Conference Series: Materials Science and Engineering},
  year = {2018},
  volume = {364},
  number = {1},
  pages = {012021},
  abstract = {This paper focuses on new models to enjoy music that will be implementable in a near future thanks to 5G technology. In the last two decades, our research mainly focused on the comprehensive description of music information, where multiple aspects are integrated to provide the user with an advanced multi-layer environment to experience music content. In recent times, the advancements in network technologies allowed a web implementation of this approach through W3C-compliant languages. The last obstacle to the use of personal devices is currently posed by the characteristics of mobile networks, concerning bandwidth, reliability, and the density of devices in an area. Designed to meet the requirements of future technological challenges, such as the Internet of Things and self-driving vehicles, the advent of 5G networks will solve these problems, thus paving the way also for new music-oriented applications. The possibilities described in this work range from bringing archive materials and music cultural heritage to a new life to the implementation of immersive environments for live-show remote experience.},
  file = {:pdfs/2018 - Advanced Experience of Music through 5G Technologies.pdf:PDF},
  url = {<a href="http://stacks.iop.org/1757-899X/364/i=1/a=012021">http://stacks.iop.org/1757-899X/364/i=1/a=012021</a>}
}
</pre>

<a name="Bellini2005"></a><pre>
@article{<a href="docs\OMR-Related.html#Bellini2005">Bellini2005</a>,
  author = {{P}ierfrancesco {B}ellini and {P}aolo {N}esi and {G}iorgio {Z}oia},
  title = {{S}ymbolic {M}usic {R}epresentation in {MPEG}},
  journal = {{IEEE} MultiMedia},
  year = {2005},
  volume = {12},
  number = {4},
  pages = {42--49},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {<a href="http://dblp.uni-trier.de/rec/bib/journals/ieeemm/BelliniNZ05">http://dblp.uni-trier.de/rec/bib/journals/ieeemm/BelliniNZ05</a>},
  doi = {10.1109/MMUL.2005.82},
  groups = {datasets},
  file = {:pdfs/2005 - Symbolic Music Representation in MPEG.pdf:PDF}
}
</pre>

<a name="Bezine2004"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Bezine2004">Bezine2004</a>,
  author = {{H}. {B}ezine and {A}.{M}. {A}limi and {N}. {S}herkat},
  title = {{G}eneration and {A}nalysis of {H}andwriting {S}cript with the {B}eta-{E}lliptic {M}odel},
  booktitle = {Ninth International Workshop on Frontiers in Handwriting Recognition},
  year = {2004},
  publisher = {Institute of Electrical {\&} Electronics Engineers ({IEEE})},
  doi = {10.1109/iwfhr.2004.45},
  groups = {handwriting},
  file = {:pdfs/2004 - Generation and Analysis of Handwriting Script With the Beta-Elliptic Model.pdf:PDF}
}
</pre>

<a name="Bezine2004a"></a><pre>
@article{<a href="docs\OMR-Related.html#Bezine2004a">Bezine2004a</a>,
  author = {{H}. {B}ezine and {A}.{M}. {A}limi and {N}. {S}herkat},
  title = {Generation and Analysis of Handwriting Script With the Beta-Elliptic Model},
  journal = {International Journal of Simulation},
  year = {2004},
  volume = {8},
  number = {2},
  pages = {45--65},
  issn = {1473-8031},
  file = {:pdfs/2004 - Generation and Analysis of Handwriting Script With the Beta-Elliptic Model - 2.pdf:PDF},
  url = {<a href="http://ijssst.info/Vol-08/No-2/paper6.pdf">http://ijssst.info/Vol-08/No-2/paper6.pdf</a>}
}
</pre>

<a name="Blostein1990"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Blostein1990">Blostein1990</a>,
  author = {D. Blostein and L. Haken},
  title = {Template matching for rhythmic analysis of music keyboard input},
  booktitle = {[1990] Proceedings. 10th International Conference on Pattern Recognition},
  year = {1990},
  volume = {i},
  pages = {767-770 vol.1},
  month = {June},
  abstract = {A system that recognizes common rhythmic patterns through template matching is described. The use of template matching gives the user the unusual ability to modify the set of templates used for analysis. This modification effects a tradeoff between the temporal accuracy required of the input and the complexity of the recognizable rhythm patterns that happen to be common in a particular piece of music. The evolving implementation of this algorithm has received heavy use over a six-year period and has proven itself as a practical and reliable input method for fast music transcription. It is concluded that templates demonstrably provide the necessary temporal context for accurate rhythm recognition.<<ETX>>},
  doi = {10.1109/ICPR.1990.118213},
  file = {:pdfs/1990 - Template Matching for Rhythmic Analysis of Music Keyboard Input.pdf:PDF},
  keywords = {acoustic signal processing;computerised pattern recognition;computerised signal processing;music;rhythmic pattern recognition;music keyboard input;template matching;music transcription;Multiple signal classification;Keyboards;Timing;Pattern recognition;Music;Rhythm;Computer errors;Councils;Laboratories;Information science}
}
</pre>

<a name="Bojar2011"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Bojar2011">Bojar2011</a>,
  author = {Bojar, Ond{{\v{r}}}ej and Ercegov{\v{c}}evi{\'{c}}, Milo{\v{s}} and Popel, Martin and Zaidan, Omar F.},
  title = {A Grain of Salt for the WMT Manual Evaluation},
  booktitle = {Sixth Workshop on Statistical Machine Translation},
  year = {2011},
  series = {WMT '11},
  pages = {1--11},
  address = {Edinburgh, Scotland},
  publisher = {Association for Computational Linguistics},
  acmid = {2132962},
  file = {:pdfs/2011 - A Grain of Salt for the WMT Manual Evaluation.pdf:PDF},
  isbn = {978-1-937284-12-1},
  numpages = {11},
  url = {<a href="http://dl.acm.org/citation.cfm?id=2132960.2132962">http://dl.acm.org/citation.cfm?id=2132960.2132962</a>}
}
</pre>

<a name="Bresler2015"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Bresler2015">Bresler2015</a>,
  author = {{B}resler, {M}artin and {P}r{\accent23 u}{\v s}a, {D}aniel and {H}lav{\'a}{\v c}, {V}{\' a}clav},
  title = {{U}sing {A}gglomerative {C}lustering of {S}trokes to {P}erform {S}ymbols {O}ver-segmentation within a {D}iagram {R}ecognition {S}ystem},
  booktitle = {CVWW 2015: Proceedings of the 20th Computer Vision Winter Workshop},
  year = {2015},
  editor = {Paul Wohlhart, Vincent Lepetit},
  pages = {67--74},
  address = {Seggau, Austria},
  publisher = {Graz University of Technology},
  file = {:pdfs/2015 - Using Agglomerative Clustering of Strokes to Perform Symbols Over-segmentation within a Diagram Recognition System.pdf:PDF},
  isbn = {978-3-85125-388-7},
  keywords = {Clustering, Diagram recognition, Flowcharts, Finite automata, Artificial samples},
  url = {<a href="http://cmp.felk.cvut.cz/ftp/articles/bresler/Bresler-Prusa-Hlavac-CVWW-2015.pdf">http://cmp.felk.cvut.cz/ftp/articles/bresler/Bresler-Prusa-Hlavac-CVWW-2015.pdf</a>}
}
</pre>

<a name="Breuel2013"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Breuel2013">Breuel2013</a>,
  author = {T. M. Breuel and A. Ul-Hasan and M. A. Al-Azawi and F. Shafait},
  title = {High-Performance {OCR} for Printed English and Fraktur Using {LSTM} Networks},
  booktitle = {2013 12th International Conference on Document Analysis and Recognition},
  year = {2013},
  pages = {683--687},
  abstract = {Long Short-Term Memory (LSTM) networks have yielded excellent results
	on handwriting recognition. This paper describes an application of
	bidirectional LSTM networks to the problem of machine-printed Latin
	and Fraktur recognition. Latin and Fraktur recognition differs significantly
	from handwriting recognition in both the statistical properties of
	the data, as well as in the required, much higher levels of accuracy.
	Applications of LSTM networks to handwriting recognition use two-dimensional
	recurrent networks, since the exact position and baseline of handwritten
	characters is variable. In contrast, for printed OCR, we used a one-dimensional
	recurrent network combined with a novel algorithm for baseline and
	x-height normalization. A number of databases were used for training
	and testing, including the UW3 database, artificially generated and
	degraded Fraktur text and scanned pages from a book digitization
	project. The LSTM architecture achieved 0.6% character-level test-set
	error on English text. When the artificially degraded Fraktur data
	set is divided into training and test sets, the system achieves an
	error rate of 1.64%. On specific books printed in Fraktur (not part
	of the training set), the system achieves error rates of 0.15% (Fontane)
	and 1.47% (Ersch-Gruber). These recognition accuracies were found
	without using any language modelling or any other post-processing
	techniques.},
  doi = {10.1109/ICDAR.2013.140},
  issn = {1520-5363},
  keywords = {handwriting recognition;natural language processing;optical character recognition;statistical analysis;text analysis;English text;Fraktur;Fraktur text;LSTM networks;UW3 database;book digitization project;handwriting recognition;handwritten characters;high-performance OCR;long short term memory networks;machine printed Fraktur recognition;machine printed Latin recognition;printed English;printed OCR;recurrent networks;scanned pages;statistical properties;Error analysis;Handwriting recognition;Hidden Markov models;Optical character recognition software;Recurrent neural networks;Training;LSTM Networks;OCR;RNN},
  file = {:pdfs/2013 - High-Performance OCR for Printed English and Fraktur using LSTM Networks.pdf:PDF}
}
</pre>

<a name="Callison-Burch2007"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Callison-Burch2007">Callison-Burch2007</a>,
  author = {Callison-Burch, Chris and Fordyce, Cameron and Koehn, Philipp and Monz, Christof and Schroeder, Josh},
  title = {(Meta-) Evaluation of Machine Translation},
  booktitle = {Second Workshop on Statistical Machine Translation},
  year = {2007},
  series = {StatMT '07},
  pages = {136--158},
  address = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid = {1626373},
  file = {:pdfs/2007 - (Meta-) Evaluation of Machine Translation.pdf:PDF},
  location = {Prague, Czech Republic},
  numpages = {23},
  url = {<a href="http://dl.acm.org/citation.cfm?id=1626355.1626373">http://dl.acm.org/citation.cfm?id=1626355.1626373</a>}
}
</pre>

<a name="Callison-Burch2008"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Callison-Burch2008">Callison-Burch2008</a>,
  author = {{C}allison-{B}urch, {C}hris and {F}ordyce, {C}ameron and {K}oehn, {P}hilipp and {M}onz, {C}hristof and {S}chroeder, {J}osh},
  title = {{F}urther {M}eta-evaluation of {M}achine {T}ranslation},
  booktitle = {Third Workshop on Statistical Machine Translation},
  year = {2008},
  series = {StatMT '08},
  pages = {70--106},
  address = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid = {1626403},
  isbn = {978-1-932432-09-1},
  location = {Columbus, Ohio},
  numpages = {37},
  file = {:pdfs/2008 - Further Meta-Evaluation of Machine Translation.pdf:PDF},
  url = {<a href="http://dl.acm.org/citation.cfm?id=1626394.1626403">http://dl.acm.org/citation.cfm?id=1626394.1626403</a>}
}
</pre>

<a name="Callison-Burch2010"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Callison-Burch2010">Callison-Burch2010</a>,
  author = {{C}allison-{B}urch, {C}hris and {K}oehn, {P}hilipp and {M}onz, {C}hristof and {P}eterson, {K}ay and {P}rzybocki, {M}ark and {Z}aidan, {O}mar {F}.},
  title = {{F}indings of the 2010 {J}oint {W}orkshop on {S}tatistical {M}achine {T}ranslation and {M}etrics for {M}achine {T}ranslation},
  booktitle = {Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR},
  year = {2010},
  series = {WMT '10},
  pages = {17--53},
  address = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid = {1868853},
  file = {:pdfs/2010 - Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation.pdf:PDF},
  isbn = {978-1-932432-71-8},
  location = {Uppsala, Sweden},
  numpages = {37},
  url = {<a href="http://dl.acm.org/citation.cfm?id=1868850.1868853">http://dl.acm.org/citation.cfm?id=1868850.1868853</a>}
}
</pre>

<a name="Calvo-Zaragoza2016a"></a><pre>
@article{<a href="docs\OMR-Related.html#Calvo-Zaragoza2016a">Calvo-Zaragoza2016a</a>,
  author = {{C}alvo-{Z}aragoza, {J}orge and {V}alero-{M}as, {J}ose {J}. and {R}ico-{J}uan, {J}uan {R}.},
  title = {{P}rototype generation on structural data using dissimilarity space representation},
  journal = {Neural Computing and Applications},
  year = {2016},
  pages = {1--10},
  issn = {1433-3058},
  abstract = {{D}ata reduction techniques play a key role in instance-based classification to lower the amount of data to be processed. {A}mong the different existing approaches, prototype selection ({PS}) and prototype generation ({PG}) are the most representative ones. {T}hese two families differ in the way the reduced set is obtained from the initial one: {W}hile the former aims at selecting the most representative elements from the set, the latter creates new data out of it. {A}lthough {PG} is considered to delimit more efficiently decision boundaries, the operations required are not so well defined in scenarios involving structural data such as strings, trees, or graphs. {T}his work studies the possibility of using dissimilarity space ({DS}) methods as an intermediate process for mapping the initial structural representation to a statistical one, thereby allowing the use of {PG} methods. {A} comparative experiment over string data is carried out in which our proposal is faced to {PS} methods on the original space. {R}esults show that the proposed strategy is able to achieve significantly similar results to {PS} in the initial space, thus standing as a clear alternative to the classic approach, with some additional advantages derived from the {DS} representation.},
  doi = {10.1007/s00521-016-2278-8},
  file = {:pdfs/2016 - Prototype Generation on Structural Data Using Dissimilarity Space Representation.pdf:PDF}
}
</pre>

<a name="Calvo-Zaragoza2016b"></a><pre>
@article{<a href="docs\OMR-Related.html#Calvo-Zaragoza2016b">Calvo-Zaragoza2016b</a>,
  author = {Jorge Calvo-Zaragoza and Jose Oncina},
  title = {An efficient approach for Interactive Sequential Pattern Recognition},
  journal = {Pattern Recognition},
  year = {2016},
  volume = {64},
  pages = {295--304},
  issn = {0031-3203},
  abstract = {Abstract Interactive Pattern Recognition (IPR) is an emergent framework in which the user is involved actively in the recognition process by giving feedback to the system when an error is detected. Although this framework is expected to reduce the number of errors to correct, it may increase the time required to complete the task since the machine needs to recompute its proposal after each interaction. Therefore, a fast computation is required to make the interactive system profitable and user-friendly. This work presents an efficient approach to deal with {IPR} tasks when data has a sequential nature. Our approach includes some computation at the very beginning of the task but it then achieves a linear complexity after user corrections. We also show how these tasks can be effectively carried out if the solution space is defined with a Regular Language. This fact has indeed proven to be the most relevant factor to improve the efficiency of the approach. Several experiments are carried out in which our proposal is faced against a classical search. Results show a reduction in time in all experiments considered, solving efficiently some complex {IPR} tasks thanks to our proposals.},
  doi = {10.1016/j.patcog.2016.11.006},
  keywords = {Interactive Pattern Recognition},
  file = {:pdfs/2017 - An efficient approach for Interactive Sequential Pattern Recognition.pdf:PDF},
  publisher = {Elsevier},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0031320316303569">http://www.sciencedirect.com/science/article/pii/S0031320316303569</a>}
}
</pre>

<a name="Chen2017a"></a><pre>
@article{<a href="docs\OMR-Related.html#Chen2017a">Chen2017a</a>,
  author = {Liang{-}Chieh Chen and Alexander Hermans and George Papandreou and Florian Schroff and Peng Wang and Hartwig Adam},
  title = {MaskLab: Instance Segmentation by Refining Object Detection with Semantic and Direction Features},
  journal = {CoRR},
  year = {2017},
  volume = {abs/1712.04837},
  archiveprefix = {arXiv},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/bib/journals/corr/abs-1712-04837},
  file = {:pdfs/2017 - MaskLab - Instance Segmentation by Refining Object Detection with Semantic and Direction Features.pdf:PDF},
  url = {<a href="http://arxiv.org/abs/1712.04837">http://arxiv.org/abs/1712.04837</a>}
}
</pre>

<a name="Chollet2017"></a><pre>
@misc{<a href="docs\OMR-Related.html#Chollet2017">Chollet2017</a>,
  author = {{F}ran{\c{c}}ois {C}hollet},
  title = {{K}eras},
  howpublished = {\url{https://github.com/fchollet/keras}},
  year = {2017},
  journal = {GitHub repository},
  publisher = {GitHub},
  url = {https://github.com/keras-team/keras}
}
</pre>

<a name="Clausner2011"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Clausner2011">Clausner2011</a>,
  author = {{C}hristian {C}lausner and {S}tefan {P}letschacher and {A}postolos {A}ntonacopoulos},
  title = {{A}letheia - {A}n {A}dvanced {D}ocument {L}ayout and {T}ext {G}round-{T}ruthing {S}ystem for {P}roduction {E}nvironments},
  booktitle = {2011 International Conference on Document Analysis and Recognition, {ICDAR}},
  year = {2011},
  pages = {48--52},
  address = {Beijing, China},
  publisher = {{IEEE} Computer Society},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {<a href="http://dblp.uni-trier.de/rec/bib/conf/icdar/ClausnerPA11">http://dblp.uni-trier.de/rec/bib/conf/icdar/ClausnerPA11</a>},
  doi = {10.1109/ICDAR.2011.19},
  file = {:pdfs/2011 - Aletheia an Advanced Document Layout and Text Ground Truthing System for Production Environments.pdf:PDF},
  url = {<a href="http://www.prima.cse.salford.ac.uk:8080/www/assets/papers/ICDAR2011_Clausner_Aletheia.pdf">http://www.prima.cse.salford.ac.uk:8080/www/assets/papers/ICDAR2011_Clausner_Aletheia.pdf</a>}
}
</pre>

<a name="Clausner2014"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Clausner2014">Clausner2014</a>,
  author = {{C}lausner, {C}hristian and {P}letschacher, {S}tefan and {A}ntonacopoulos, {A}postolos},
  title = {{E}fficient {OCR} {T}raining {D}ata {G}eneration with {A}letheia},
  booktitle = {Short Paper Booklet of the 11th International Association for Pattern Recognition (IAPR) Workshop on Document Analysis Systems (DAS)},
  year = {2014},
  file = {:pdfs/2014 - Efficient OCR Training Data Generation with Aletheia.pdf:PDF},
  url = {<a href="http://www.primaresearch.org/www/assets/papers/DAS2014_Clausner_OCRTrainingDataGeneration.pdf">http://www.primaresearch.org/www/assets/papers/DAS2014_Clausner_OCRTrainingDataGeneration.pdf</a>}
}
</pre>

<a name="Cont2007"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Cont2007">Cont2007</a>,
  author = {{C}ont, {A}rshia and {S}chwarz, {D}iemo and {S}chnell, {N}orbert and {R}aphael, {C}hristopher},
  title = {Evaluation of Real-Time Audio-to-Score Alignment},
  booktitle = {8th International Conference on Music Information Retrieval},
  year = {2007},
  address = {Vienna, Austria},
  file = {:pdfs/2007 - Evaluation of Real Time Audio to Score Alignment.pdf:PDF},
  hal_id = {hal-00839068},
  hal_version = {v1},
  url = {https://hal.inria.fr/hal-00839068}
}
</pre>

<a name="Cordella2000"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Cordella2000">Cordella2000</a>,
  author = {Cordella, L. P. and Vento, M.},
  title = {Symbol and Shape Recognition},
  booktitle = {Graphics Recognition Recent Advances},
  year = {2000},
  editor = {Chhabra, Atul K. and Dori, Dov},
  pages = {167--182},
  address = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract = {The different aspects of a process for recognizing symbols in documents are considered and the techniques that have been most commonly used during the last ten years, in the different application fields, are reviewed. Methods used in the representation, description and classification phases are shortly discussed and the main recognition strategies are mentioned. Some of the problems that appear still open are proposed to the attention of the reader.},
  doi = {10.1007/3-540-40953-X_14},
  file = {:pdfs/2000 - Symbol and Shape Recognition.pdf:PDF},
  isbn = {978-3-540-40953-3}
}
</pre>

<a name="CPDL"></a><pre>
@misc{<a href="docs\OMR-Related.html#CPDL">CPDL</a>,
  author = {Rafael Ornes},
  title = {Choral Public Domain Library},
  year = {1998},
  url = {<a href="http://cpdl.org">http://cpdl.org</a>}
}
</pre>

<a name="Cutter2015"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Cutter2015">Cutter2015</a>,
  author = {Michael P. Cutter and Roberto Manduchi},
  title = {Towards Mobile {OCR}},
  booktitle = {2015 {ACM} Symposium on Document Engineering - {DocEng}'15},
  year = {2015},
  publisher = {{ACM} Press},
  doi = {10.1145/2682571.2797066},
  file = {:pdfs/2015 - Towards Mobile OCR - How To Take a Good Picture of a Document Without Sight.pdf:PDF}
}
</pre>

<a name="Dawid1979"></a><pre>
@article{<a href="docs\OMR-Related.html#Dawid1979">Dawid1979</a>,
  author = {{D}awid, {A}lexander {P}hilip and {S}kene, {A}llan {M}},
  title = {{M}aximum likelihood estimation of observer error-rates using the {EM} algorithm},
  journal = {Applied statistics},
  year = {1979},
  pages = {20--28},
  file = {:pdfs/1979 - Maximum Likelihood Estimation of Observer Error Rates Using the EM Algorithm.pdf:PDF},
  publisher = {JSTOR},
  url = {https://www.jstor.org/stable/2346806}
}
</pre>

<a name="Delakis2008"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Delakis2008">Delakis2008</a>,
  author = {{D}elakis, {M}anolis and {G}arcia, {C}hristophe},
  title = {{Text detection with convolutional neural networks}},
  booktitle = {International Conference on Computer Vision Theory and Application},
  year = {2008},
  pages = {290--294},
  file = {:pdfs/2008 - Text Detection with Convolutional Neural Networks.pdf:PDF},
  keywords = {CNN},
  url = {https://www.researchgate.net/profile/Christophe_Garcia2/publication/221415287_text_Detection_with_Convolutional_Neural_Networks/links/545251a30cf2bccc49087299/text-Detection-with-Convolutional-Neural-Networks.pdf}
}
</pre>

<a name="Downie2008"></a><pre>
@article{<a href="docs\OMR-Related.html#Downie2008">Downie2008</a>,
  author = {{J}. {S}tephen {D}ownie},
  title = {{T}he music information retrieval evaluation exchange (2005--2007): {A} window into music information retrieval research},
  journal = {Acoust. Sci. {\&} Tech.},
  year = {2008},
  volume = {29},
  number = {4},
  pages = {247--255},
  doi = {10.1250/ast.29.247},
  file = {:pdfs/2008 - The Music Information Retrieval Evaluation Exchange (2005-2007) - a Window into Music Information Retrieval Research.pdf:PDF},
  publisher = {Acoustical Society of Japan}
}
</pre>

<a name="Downie2010"></a><pre>
@inbook{<a href="docs\OMR-Related.html#Downie2010">Downie2010</a>,
  pages = {93--115},
  title = {The Music Information Retrieval Evaluation eXchange: Some Observations and Insights},
  publisher = {Springer Berlin Heidelberg},
  year = {2010},
  author = {Downie, J. Stephen and Ehmann, Andreas F. and Bay, Mert and Jones, M. Cameron},
  address = {Berlin, Heidelberg},
  isbn = {978-3-642-11674-2},
  abstract = {Advances in the science and technology of Music Information Retrieval (MIR) systems and algorithms are dependent on the development of rigorous measures of accuracy and performance such that meaningful comparisons among current and novel approaches can be made. This is the motivating principle driving the efforts of the International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL) and the annual Music Information Retrieval Evaluation eXchange (MIREX). Since it started in 2005, MIREX has fostered great advancements not only in many specific areas of MIR, but also in our general understanding of how MIR systems and algorithms are to be evaluated. This chapter outlines some of the major highlights of the past four years of MIREX evaluations, including its organizing principles, the selection of evaluation metrics, and the evolution of evaluation tasks. The chapter concludes with a brief introduction of how MIREX plans to expand into the future using a suite of Web 2.0 technologies to automated MIREX evaluations.},
  booktitle = {Advances in Music Information Retrieval},
  doi = {10.1007/978-3-642-11674-2_5},
  file = {:pdfs/2010 - The Music Information Retrieval Evaluation Exchange - Some Observations and Insights.pdf:PDF},
  url = {https://doi.org/10.1007/978-3-642-11674-2_5}
}
</pre>

<a name="Droettboom2003"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Droettboom2003">Droettboom2003</a>,
  author = {M. Droettboom},
  title = {Correcting broken characters in the recognition of historical printed documents},
  booktitle = {2003 Joint Conference on Digital Libraries, 2003. Proceedings.},
  year = {2003},
  pages = {364--366},
  month = {May},
  abstract = {We present a new technique for dealing with broken characters, one of the major challenges in the optical character recognition (OCR) of degraded historical printed documents. A technique based on graph combinatorics is used to rejoin the appropriate connected components. It has been applied to real data with successful results.},
  doi = {10.1109/JCDL.2003.1204889},
  file = {:pdfs/2003 - Correcting Broken Characters in the Recognition of Historical Printed Documents.pdf:PDF},
  keywords = {optical character recognition;document image processing;history;graph theory;character sets;broken character correction;optical character recognition;OCR;historical printed document;graph combinatorics;connected component;Character recognition;Carbon capture and storage;Optical character recognition software;Degradation;Printing;Shape;Combinatorial mathematics;Optical design;Business;Robustness}
}
</pre>

<a name="Erickson1975"></a><pre>
@article{<a href="docs\OMR-Related.html#Erickson1975">Erickson1975</a>,
  author = {Erickson, Raymond F.},
  title = {``The Darms project'': A status report},
  journal = {Computers and the Humanities},
  year = {1975},
  volume = {9},
  number = {6},
  pages = {291--298},
  issn = {1572-8412},
  doi = {10.1007/BF02396292}
}
</pre>

<a name="Eskenazi2017"></a><pre>
@article{<a href="docs\OMR-Related.html#Eskenazi2017">Eskenazi2017</a>,
  author = {Eskenazi, S. and Gomez-Kr{\"{a}}mer, P. and Ogier, J.-M.},
  title = {A comprehensive survey of mostly textual document segmentation algorithms since 2008},
  journal = {Pattern Recognition},
  year = {2017},
  volume = {64},
  pages = {1--14},
  issn = {0031-3203},
  abbrev_source_title = {Pattern Recogn.},
  abstract = {In document image analysis, segmentation is the task that identifies the regions of a document. The increasing number of applications of document analysis requires a good knowledge of the available technologies. This survey highlights the variety of the approaches that have been proposed for document image segmentation since 2008. It provides a clear typology of documents and of document image segmentation algorithms. We also discuss the technical limitations of these algorithms, the way they are evaluated and the general trends of the community. Â© 2016 Elsevier Ltd},
  affiliation = {L3i laboratory - La Rochelle University Avenue Michel CrÃ©peau, La Rochelle, France},
  author_keywords = {Document; Evaluation; Segmentation; Survey; Trends; Typology},
  coden = {PTNRA},
  correspondence_address1 = {Gomez-KrÃ¤mer, P.France; email: petra.gomez@univ-lr.fr},
  document_type = {Article},
  doi = {10.1016/j.patcog.2016.10.023},
  funding_details = {ANR-14-CE28-0022, ANR, Agence Nationale de la Recherche},
  funding_text = {This work is financed by the French National Research Agency (ANR) project SHADES referenced under ANR-14-CE28-0022 and by the Town community of La Rochelle.},
  keywords = {Digital image storage; Surveying; Surveys, Document; Document image analysis; Document image segmentation; Evaluation; Technical limitations; Textual documents; Trends; Typology, Image segmentation},
  language = {English},
  file = {:pdfs/2017 - A comprehensive survey of mostly textual document segmentation algorithms since 2008.pdf:PDF},
  publisher = {Elsevier Ltd}
}
</pre>

<a name="Ewert2014"></a><pre>
@article{<a href="docs\OMR-Related.html#Ewert2014">Ewert2014</a>,
  author = {{S}ebastian {E}wert and {B}ryan {P}ardo and {M}einard {M}{\"{u}}ller and {M}ark {D}. {P}lumbley},
  title = {{S}core-{I}nformed {S}ource {S}eparation for {M}usical {A}udio {R}ecordings: {A}n overview},
  journal = {{IEEE} Signal Process. Mag.},
  year = {2014},
  volume = {31},
  number = {3},
  pages = {116--124},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {<a href="http://dblp.uni-trier.de/rec/bib/journals/spm/EwertPMP14">http://dblp.uni-trier.de/rec/bib/journals/spm/EwertPMP14</a>},
  doi = {10.1109/MSP.2013.2296076},
  file = {:pdfs/2014 - Score Informed Source Separation for Musical Audio Recordings_ an Overview.pdf:PDF}
}
</pre>

<a name="Fahmy1992"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Fahmy1992">Fahmy1992</a>,
  author = {Fahmy, Hoda and Blostein, Dorothea},
  title = {A survey of graph grammars: Theory and applications},
  booktitle = {Pattern Recognition, 1992. Vol. II. Conference B: Pattern Recognition Methodology and Systems, Proceedings., 11th IAPR International Conference on},
  year = {1992},
  pages = {294--298},
  organization = {IEEE},
  file = {:pdfs/1992 - A Survey of Graph Grammars_ Theory and Applications.pdf:PDF},
  url = {https://www.researchgate.net/publication/3513859_A_survey_of_graph_grammars_theory_and_applications}
}
</pre>

<a name="Fasanaro1990"></a><pre>
@article{<a href="docs\OMR-Related.html#Fasanaro1990">Fasanaro1990</a>,
  author = {Fasanaro, A. M. and Spitaleri, D. L. A. and Valiani, R. and Grossi, D.},
  title = {Dissociation in Musical Reading: A Musician Affected by Alexia without Agraphia},
  journal = {Music Perception: An Interdisciplinary Journal},
  year = {1990},
  volume = {7},
  number = {3},
  pages = {259--272},
  issn = {0730-7829},
  abstract = {Previous works have postulated a similarity between music reading
	and text reading. Therefore it is interesting to evaluate both of
	these functions in an alexic subject. The patient investigated is
	a professional musician who had an ischemic lesion in the left temporoparieto-occipital
	region. Text reading showed pure alexia in which both the phonological
	and global routes were damaged. His ability to read correctly via
	matching tests showed that the word-form system was preserved. The
	reading of musical scores was damaged too and showed a dissociation
	between the reading of ideograms and rhythms (preserved) and the
	reading of notes (impaired). The results of note reading were analogous
	to those of word reading. Furthermore, the patient could read notes
	correctly via matching tests. On the basis of these findings, we
	propose a model of music reading where the reading of notes is based
	on a representational system analogous to that of words (the so-called
	internal language) whereas reading of ideograms and rhythms occurs
	via an internal representation unrelated to linguistic functions.},
  doi = {10.2307/40285464},
  eprint = {<a href="http://mp.ucpress.edu/content/7/3/259.full.pdf">http://mp.ucpress.edu/content/7/3/259.full.pdf</a>},
  publisher = {University of California Press Journals},
  url = {<a href="http://mp.ucpress.edu/content/7/3/259">http://mp.ucpress.edu/content/7/3/259</a>}
}
</pre>

<a name="Feist2017"></a><pre>
@book{<a href="docs\OMR-Related.html#Feist2017">Feist2017</a>,
  title = {Berklee Contemporary Music Notation},
  publisher = {Berklee Press},
  year = {2017},
  author = {Jonathan Feist},
  isbn = {978-0876391785},
  abstract = {Learn the nuances of music notation, and create professional looking scores. This reference presents a comprehensive look at contemporary music notation. You will learn the meaning and stylistic practices for many types of notation that are currently in common use, from traditional staffs to lead sheets to guitar tablature. It discusses hundreds of notation symbols, as well as general guidelines for writing music. Berklee College of Music brings together teachers and students from all over the world, and we use notation in a great variety of ways. This book presents our perspectives on notation: what we have found to be the most commonly used practices in today's music industry, and what seems to be serving our community best. It includes a foreword by Matthew Nicholl, who was a long-time chair of Berklee's Contemporary Writing and Production Department. Whether you find yourself in a Nashville recording studio, Hollywood sound stage, grand concert hall, worship choir loft, or elementary school auditorium, this book will help you to create readable, professional, publication-quality notation. Beyond understanding the standard rules and definitions, you will learn to make appropriate choices for your own work, and generally how to achieve clarity and consistency in your notation so that it best serves your music.}
}
</pre>

<a name="Ferlaino2018"></a><pre>
@article{<a href="docs\OMR-Related.html#Ferlaino2018">Ferlaino2018</a>,
  author = {{Ferlaino}, M. and {Glastonbury}, C.~A. and {Motta-Mejia}, C. and {Vatish}, M. and {Granne}, I. and {Kennedy}, S. and {Lindgren}, C.~M. and {Nell{\aa}ker}, C.},
  title = {{Towards Deep Cellular Phenotyping in Placental Histology}},
  journal = {ArXiv e-prints},
  year = {2018},
  archiveprefix = {arXiv},
  eprint = {1804.03270},
  file = {:pdfs/2018 - Towards Deep Cellular Phenotyping in Placental Histology.pdf:PDF},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs.CV},
  url = {https://openreview.net/pdf?id=HJq5OGKsz}
}
</pre>

<a name="Gao2003"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Gao2003">Gao2003</a>,
  author = {Sheng Gao and N. C. Maddage and Chin-Hui Lee},
  title = {A hidden Markov model based approach to music segmentation and identification},
  booktitle = {Fourth International Conference on Information, Communications and Signal Processing, 2003 and the Fourth Pacific Rim Conference on Multimedia. Proceedings of the 2003 Joint},
  year = {2003},
  volume = {3},
  pages = {1576-1580 vol.3},
  month = {Dec},
  abstract = {Classification of musical segments is an interesting problem. It is a key technology in the development of content-based audio document indexing and retrieval. In this paper, we apply the feature extraction and modeling techniques commonly used in automatic speech recognition to solving the problem of segmentation and instrument identification of musical passages. The correlation among the different components in the feature space and the auto-correlation of each component are analyzed to demonstrate feasibility in musical signal analysis and instrument class modeling. Our experimental results are first evaluated on 3 instrument categories, i.e. vocal music, instrumental music, and their combinations. Furthermore each category is split into two individual cases to give a 6-class problem. Our results show that good performance could be obtained with simple features, such as mel-frequency cepstral coefficients and cepstral coefficients derived from linear prediction signal analysis. Even with a limited amount of training data, we could give an accuracy of 90.60% in the case of three categories. A slightly worse accuracy of 90.38% is obtained when we double the number of categories to six classes.},
  doi = {10.1109/ICICS.2003.1292732},
  file = {:pdfs/2003 - A Hidden Markov Model Based Approach to Music Segmentation and Identification.pdf:PDF},
  keywords = {music;hidden Markov models;signal classification;speech recognition;correlation methods;audio signal processing;cepstral analysis;content-based retrieval;indexing;music segmentation;hidden Markov model;musical segment classification;audio indexing;audio retrieval;feature extraction;automatic speech recognition;musical passage identification;component correlations;musical signal analysis;vocal music;instrumental music;cepstral coefficients;linear prediction signal analysis;time-varying signals;Hidden Markov models;Instruments;Signal analysis;Space technology;Cepstral analysis;Indexing;Music information retrieval;Content based retrieval;Feature extraction;Automatic speech recognition}
}
</pre>

<a name="Garfinkle2017"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Garfinkle2017">Garfinkle2017</a>,
  author = {Garfinkle, David and Arthur, Claire and Schubert, Peter and Cumming, Julie and Fujinaga, Ichiro},
  title = {PatternFinder: Content-Based Music Retrieval with Music21},
  booktitle = {4th International Workshop on Digital Libraries for Musicology},
  year = {2017},
  series = {DLfM '17},
  pages = {5--8},
  address = {New York, NY, USA},
  publisher = {ACM},
  acmid = {3144751},
  doi = {10.1145/3144749.3144751},
  isbn = {978-1-4503-5347-2},
  keywords = {content-based music retrieval, imitation masses, music21, point-set similarity, polyphonic search, symbolic music similarity, time-scaled, time-warped, transposition-invariant},
  location = {Shanghai, China},
  numpages = {4},
  file = {:pdfs/2017 - PatternFinder - Content-Based Music Retrieval with music21.pdf:PDF}
}
</pre>

<a name="Gatos2004"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Gatos2004">Gatos2004</a>,
  author = {Gatos, Basilios and Pratikakis, Ioannis and Perantonis, Stavros J.},
  title = {An Adaptive Binarization Technique for Low Quality Historical Documents},
  booktitle = {Document Analysis Systems VI},
  year = {2004},
  editor = {Marinai, Simone and Dengel, Andreas R.},
  pages = {102--113},
  address = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract = {Historical document collections are a valuable resource for human history. This paper proposes a novel digital image binarization scheme for low quality historical documents allowing further content exploitation in an efficient way. The proposed scheme consists of five distinct steps: a pre-processing procedure using a low-pass Wiener filter, a rough estimation of foreground regions using Niblack's approach, a background surface calculation by interpolating neighboring background intensities, a thresholding by combining the calculated background surface with the original image and finally a post-processing step in order to improve the quality of text regions and preserve stroke connectivity. The proposed methodology works with great success even in cases of historical manuscripts with poor quality, shadows, nonuniform illumination, low contrast, large signal- dependent noise, smear and strain. After testing the proposed method on numerous low quality historical manuscripts, it has turned out that our methodology performs better compared to current state-of-the-art adaptive thresholding techniques.},
  doi = {10.1007/978-3-540-28640-0_10},
  file = {:pdfs/2004 - An Adaptive Binarization Technique for Low Quality Historical Documents.pdf:PDF},
  isbn = {978-3-540-28640-0}
}
</pre>

<a name="Gatos2006"></a><pre>
@article{<a href="docs\OMR-Related.html#Gatos2006">Gatos2006</a>,
  author = {Gatos, B. and Pratikakis, I. and Perantonis, S.J.},
  title = {Adaptive degraded document image binarization},
  journal = {Pattern Recognition},
  year = {2006},
  volume = {39},
  number = {3},
  pages = {317--327},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2005.09.010},
  file = {:pdfs/2006 - Adaptive Degraded Document Image Binarization.pdf:PDF},
  keywords = {binarization,degraded document images,local adaptive binarization}
}
</pre>

<a name="Gatos2009"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Gatos2009">Gatos2009</a>,
  author = {B. Gatos and K. Ntirogiannis and I. Pratikakis},
  title = {{ICDAR} 2009 Document Image Binarization Contest ({DIBCO} 2009)},
  booktitle = {2009 10th International Conference on Document Analysis and Recognition},
  year = {2009},
  pages = {1375--1382},
  abstract = {DIBCO 2009 is the first International Document Image Binarization
	Contest organized in the context of ICDAR 2009 conference. The general
	objective of the contest is to identify current advances in document
	image binarization using established evaluation performance measures.
	This paper describes the contest details including the evaluation
	measures used as well as the performance of the 43 submitted methods
	along with a short description of each method.},
  doi = {10.1109/ICDAR.2009.246},
  issn = {1520-5363},
  keywords = {document image processing;image recognition;document image analysis;image recognition;international document image binarization contest;performance measure;Adaptive filters;Computational intelligence;Current measurement;Filtering;Image analysis;Image recognition;Informatics;Iterative algorithms;Laboratories;Text analysis;document image binarization;evaluation},
  file = {:pdfs/2009 - ICDAR 2009 Document Image Binarization Contest.pdf:PDF}
}
</pre>

<a name="George2014"></a><pre>
@article{<a href="docs\OMR-Related.html#George2014">George2014</a>,
  author = {George, Joe and Shamir, Lior},
  title = {Computer analysis of similarities between albums in popular music},
  journal = {Pattern Recognition Letters},
  year = {2014},
  volume = {45},
  pages = {78--84},
  doi = {10.1016/j.patrec.2014.02.021},
  file = {:pdfs/2014 - Computer Analysis of Similarities between Albums in Popular Music.pdf:PDF},
  publisher = {Elsevier}
}
</pre>

<a name="Gerou2009"></a><pre>
@book{<a href="docs\OMR-Related.html#Gerou2009">Gerou2009</a>,
  title = {{Essential Dicionary of Music Notation}},
  publisher = {Alfred Publishing Co., Inc.},
  year = {2009},
  author = {{G}erou, {T}om and {L}usk, {L}inda},
  isbn = {0-8828284-768-6},
  file = {:pdfs/2009 - Essential Dicionary of Music Notation.pdf:PDF},
  keywords = {music notation},
  pages = {161},
  url = {<a href="http://www.amazon.com/Essentials-Music-Notation-Alfred-Publishing/dp/073906083X">http://www.amazon.com/Essentials-Music-Notation-Alfred-Publishing/dp/073906083X</a>}
}
</pre>

<a name="Giotis2017"></a><pre>
@article{<a href="docs\OMR-Related.html#Giotis2017">Giotis2017</a>,
  author = {Angelos P. Giotis and Giorgos Sfikas and Basilis Gatos and Christophoros Nikou},
  title = {A survey of document image word spotting techniques},
  journal = {Pattern Recognition},
  year = {2017},
  volume = {68},
  pages = {310--332},
  issn = {0031-3203},
  abstract = {Vast collections of documents available in image format need to be indexed for information retrieval purposes. In this framework, word spotting is an alternative solution to optical character recognition (OCR), which is rather inefficient for recognizing text of degraded quality and unknown fonts usually appearing in printed text, or writing style variations in handwritten documents. Over the past decade there has been a growing interest in addressing document indexing using word spotting which is reflected by the continuously increasing number of approaches. However, there exist very few comprehensive studies which analyze the various aspects of a word spotting system. This work aims to review the recent approaches as well as fill the gaps in several topics with respect to the related works. The nature of texts and inherent challenges addressed by word spotting methods are thoroughly examined. After presenting the core steps which compose a word spotting system, we investigate the use of retrieval enhancement techniques based on relevance feedback which improve the retrieved results. Finally, we present the datasets which are widely used for word spotting, we describe the evaluation standards and measures applied for performance assessment and discuss the results achieved by the state of the art.},
  doi = {https://doi.org/10.1016/j.patcog.2017.02.023},
  file = {:pdfs/2017 - A Survey of Document Image Word Spotting Techniques.pdf:PDF},
  keywords = {Word spotting, Retrieval, Document indexing, Features, Representation, Relevance feedback},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0031320317300870">http://www.sciencedirect.com/science/article/pii/S0031320317300870</a>}
}
</pre>

<a name="Girshick2014"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Girshick2014">Girshick2014</a>,
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
  booktitle = {IEEE Conference On Computer Vision and Pattern Recognition},
  year = {2014},
  pages = {580--587},
  doi = {10.1109/CVPR.2014.81},
  file = {:pdfs/2014 - Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.pdf:PDF}
}
</pre>

<a name="Girshick2015"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Girshick2015">Girshick2015</a>,
  author = {R. Girshick},
  title = {Fast R-CNN},
  booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
  year = {2015},
  pages = {1440--1448},
  abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  doi = {10.1109/ICCV.2015.169},
  file = {:pdfs/2015 - Fast R CNN.pdf:PDF},
  keywords = {feedforward neural nets;object detection;C++;Caffe;Python;VGG16 network;fast R-CNN;fast region-based convolutional network method;object detection;open-source MIT License;Computer architecture;Feature extraction;Object detection;Open source software;Pipelines;Proposals;Training}
}
</pre>

<a name="Good2003"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Good2003">Good2003</a>,
  author = {M. Good and G. Actor},
  title = {Using MusicXML for file interchange},
  booktitle = {Third International Conference on WEB Delivering of Music},
  year = {2003},
  pages = {153},
  abstract = {The MusicXML format is designed to be a universal translator for programs that understand common Western musical notation. We have made significant progress towards this goal, with over a dozen programs supporting MusicXML as of June 2003. We describe some of the ways that MusicXML has been used for file interchange, and will demonstrate several scenarios.},
  doi = {10.1109/WDM.2003.1233890},
  file = {:pdfs/2003 - Using MusicXML for File Interchange.pdf:PDF},
  keywords = {XML;electronic data interchange;music;program interpreters;MusicXML format;Western musical notation;file interchange;universal translator;Application software;Displays;IEEE news;Markup languages;Prototypes;Software prototyping;Technological innovation;XML}
}
</pre>

<a name="Goodfellow2013"></a><pre>
@article{<a href="docs\OMR-Related.html#Goodfellow2013">Goodfellow2013</a>,
  author = {{G}oodfellow, {I}an {J} and {B}ulatov, {Y}aroslav and {I}barz, {J}ulian and {A}rnoud, {S}acha and {S}het, {V}inay},
  title = {{Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks}},
  journal = {{Computing Research Repository}},
  year = {2013},
  volume = {abs/1312.6},
  pages = {1--13},
  abstract = {{R}ecognizing arbitrary multi-character text in unconstrained natural photographs is a hard problem. {I}n this paper, we address an equally hard sub-problem in this domain viz. recognizing arbitrary multi-digit numbers from {S}treet {V}iew imagery. {T}raditional approaches to solve this problem typically separate out the localization, segmentation, and recognition steps. {I}n this paper we propose a unified approach that integrates these three steps via the use of a deep convolutional neural network that operates directly on the image pixels. {W}e employ the {D}ist{B}elief implementation of deep neural networks in order to train large, distributed neural networks on high quality images. {W}e find that the performance of this approach increases with the depth of the convolutional network, with the best performance occurring in the deepest architecture we trained, with eleven hidden layers. {W}e evaluate this approach on the publicly available {SVHN} dataset and achieve over 96% accuracy in recognizing complete street numbers. {W}e show that on a per-digit recognition task, we improve upon the state-of-the-art, achieving 97.84% accuracy. {W}e also evaluate this approach on an even more challenging dataset generated from {S}treet {V}iew imagery containing several tens of millions of street number annotations and achieve over 90% accuracy. {T}o further explore the applicability of the proposed system to broader text recognition tasks, we apply it to synthetic distorted text from re{CAPTCHA}. re{CAPTCHA} is one of the most secure reverse turing tests that uses distorted text to distinguish humans from bots. {W}e report a 99.8% accuracy on the hardest category of re{CAPTCHA}. {O}ur evaluations on both tasks indicate that at specific operating thresholds, the performance of the proposed system is comparable to, and in some cases exceeds, that of human operators.},
  archiveprefix = {arXiv},
  arxivid = {1312.6082},
  eprint = {1312.6082},
  file = {:pdfs/2013 - Multi Digit Number Recognition from Street View Imagery Using Deep Convolutional Neural Networks.pdf:PDF},
  keywords = {CNN},
  url = {<a href="http://arxiv.org/abs/1312.6082">http://arxiv.org/abs/1312.6082</a>}
}
</pre>

<a name="Goolsby1994"></a><pre>
@article{<a href="docs\OMR-Related.html#Goolsby1994">Goolsby1994</a>,
  author = {Goolsby, Thomas W.},
  title = {Eye Movement in Music Reading: Effects of Reading Ability, Notational Complexity, and Encounters},
  journal = {Music Perception: An Interdisciplinary Journal},
  year = {1994},
  volume = {12},
  number = {1},
  pages = {77--96},
  issn = {0730-7829},
  abstract = {Six types of eye movement were measured and recorded with an SRI Eyetracker: number of progressive and regressive fixations, durations of progressive and regressive fixations and lengths of progressive and regressive saccades. Twenty-four graduate music students were selected as skilled and less- skilled music readers. Eye position was measured every millisecond with a high degree of accuracy. The factorial design was 2 Groups x 4 Melodies x 3 Encounters (including a practice period). Results indicated that patterns of eye movement in the two groups were similar across melodies and encounters, but differed with notational complexity. Eye movement was reduced when performing melodies with more-concentrated visual information than when performing melodies with less- concentrated visual information. The main effect of encounters indicated that music readers used fewer but longer fixations after practicing the melodies. Results suggest that skilled music readers look farther ahead in the notation, and then back to the point of performance, when sightreading.},
  doi = {10.2307/40285756},
  eprint = {<a href="http://mp.ucpress.edu/content/12/1/77.full.pdf">http://mp.ucpress.edu/content/12/1/77.full.pdf</a>},
  publisher = {University of California Press Journals},
  url = {<a href="http://mp.ucpress.edu/content/12/1/77">http://mp.ucpress.edu/content/12/1/77</a>}
}
</pre>

<a name="Goolsby1994a"></a><pre>
@article{<a href="docs\OMR-Related.html#Goolsby1994a">Goolsby1994a</a>,
  author = {Goolsby, Thomas W.},
  title = {Profiles of Processing: Eye Movements during Sightreading},
  journal = {Music Perception: An Interdisciplinary Journal},
  year = {1994},
  volume = {12},
  number = {1},
  pages = {97--123},
  issn = {0730-7829},
  abstract = {Temporal and sequential components of the eye movement used by a skilled and a less-skilled sightreader were used to construct six profiles of processing. Each subject read three melodies of varying levels of concentration of visual detail. The profiles indicates the order, duration, and location of each fixation while the subjects sightread the melodies. Results indicate that music readers do not fixate on note stems or the bar lines that connect eighth notes when sightreading. The less-skilled music reader progressed through the melody virtually note-by-note using long fixations, whereas the skilled sightreader directed fixations to all areas of the notation (using more regressions than the less-skilled reader) to perform the music accurately. Results support earlier findings that skilled sightreaders look farther ahead in the notation, then back to the point of performance (Goolsby, 1994), and have a larger perceptual span than less-skilled sightreaders. Findings support Slobodans (1984) contention that music reading (i. e., sightreading) is indeed music perception, because music notation is processed before performance. Support was found for Sloboda's (1977, 1984, 1985, 1988) hypotheses on the effects of physical and structural boundaries on visual musical perception. The profiles indicate a number of differences between music perception from processing visual notation and perception resulting from language reading. These differences include: (1) opposite trends in the control of eye movement (i. e., the better music reader fixates in blank areas of the visual stimuli and not directly on each item of the information that was performed), (2) a perceptual span that is vertical as well as horizontal, (3) more eye movement associated with the better reader, and (4) greater attention used for processing language than for music, although the latter task requires an "exact realization."},
  doi = {10.2307/40285757},
  eprint = {<a href="http://mp.ucpress.edu/content/12/1/97.full.pdf">http://mp.ucpress.edu/content/12/1/97.full.pdf</a>},
  publisher = {University of California Press Journals},
  url = {<a href="http://mp.ucpress.edu/content/12/1/97">http://mp.ucpress.edu/content/12/1/97</a>}
}
</pre>

<a name="Graves2007"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Graves2007">Graves2007</a>,
  author = {Graves, Alex and Fern\'{a}ndez, Santiago and Liwicki, Marcus and Bunke, Horst and Schmidhuber, J\"{u}rgen},
  title = {Unconstrained Online Handwriting Recognition with Recurrent Neural Networks},
  booktitle = {20th International Conference on Neural Information Processing Systems},
  year = {2007},
  series = {NIPS'07},
  pages = {577--584},
  address = {USA},
  publisher = {Curran Associates Inc.},
  acmid = {2981635},
  file = {:pdfs/2007 - Unconstrained Online Handwriting Recognition with Recurrent Neural Networks.pdf:PDF},
  isbn = {978-1-60560-352-0},
  location = {Vancouver, British Columbia, Canada},
  numpages = {8},
  url = {<a href="http://dl.acm.org/citation.cfm?id=2981562.2981635">http://dl.acm.org/citation.cfm?id=2981562.2981635</a>}
}
</pre>

<a name="Graves2009"></a><pre>
@article{<a href="docs\OMR-Related.html#Graves2009">Graves2009</a>,
  author = {Graves, Alex and Liwicki, Marcus and Fern{\'{a}}ndez, Santiago and Bertolami, Roman and Bunke, Horst and Schmidhuber, J{\"{u}}rgen},
  title = {A novel connectionist system for unconstrained handwriting recognition},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2009},
  volume = {31},
  number = {5},
  pages = {855--868},
  file = {:pdfs/2009 - A Novel Connectionist System for Unconstrained Handwriting Recognition.pdf:PDF},
  groups = {handwriting},
  publisher = {IEEE},
  url = {<a href="http://www.cs.toronto.edu/~graves/tpami_2009.pdf">http://www.cs.toronto.edu/~graves/tpami_2009.pdf</a>}
}
</pre>

<a name="Graves2013"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Graves2013">Graves2013</a>,
  author = {A. Graves and A. Mohamed and G. Hinton},
  title = {Speech recognition with deep recurrent neural networks},
  booktitle = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
  year = {2013},
  pages = {6645--6649},
  abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
  doi = {10.1109/ICASSP.2013.6638947},
  file = {:pdfs/2013 - Speech Recognition with Deep Recurrent Neural Networks.pdf:PDF},
  issn = {1520-6149},
  keywords = {speech recognition;speech recognition;deep recurrent neural networks;sequential data;end-to-end training methods;connectionist temporal classification;long short-term memory RNN architecture;Speech recognition;Recurrent neural networks;Training;Vectors;Acoustics;Noise;recurrent neural networks;deep neural networks;speech recognition}
}
</pre>

<a name="Graves2014"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Graves2014">Graves2014</a>,
  author = {Graves, Alex and Jaitly, Navdeep},
  title = {Towards End-to-end Speech Recognition with Recurrent Neural Networks},
  booktitle = {31st International Conference on International Conference on Machine Learning},
  year = {2014},
  series = {ICML'14},
  pages = {1764--1772},
  address = {Beijing, China},
  publisher = {JMLR.org},
  acmid = {3045089},
  volume = {32},
  file = {:pdfs/2014 - Towards End to End Speech Recognition with Recurrent Neural Networks.pdf:PDF},
  url = {<a href="http://dl.acm.org/citation.cfm?id=3044805.3045089">http://dl.acm.org/citation.cfm?id=3044805.3045089</a>}
}
</pre>

<a name="Grefenstette2015"></a><pre>
@article{<a href="docs\OMR-Related.html#Grefenstette2015">Grefenstette2015</a>,
  author = {{E}dward {G}refenstette and {K}arl {M}oritz {H}ermann and {M}ustafa {S}uleyman and {P}hil {B}lunsom},
  title = {{L}earning to {T}ransduce with {U}nbounded {M}emory},
  journal = {{Computing Research Repository}},
  year = {2015},
  volume = {abs/1506.02516},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {<a href="http://dblp.uni-trier.de/rec/bib/journals/corr/GrefenstetteHSB15">http://dblp.uni-trier.de/rec/bib/journals/corr/GrefenstetteHSB15</a>},
  file = {:pdfs/2015 - Learning to Transduce with Unbounded Memory.pdf:PDF},
  url = {<a href="http://arxiv.org/abs/1506.02516">http://arxiv.org/abs/1506.02516</a>}
}
</pre>

<a name="Gregor2015"></a><pre>
@article{<a href="docs\OMR-Related.html#Gregor2015">Gregor2015</a>,
  author = {{K}arol {G}regor and {I}vo {D}anihelka and {A}lex {G}raves and {D}aan {W}ierstra},
  title = {{DRAW}: {A} {R}ecurrent {N}eural {N}etwork {F}or {I}mage {G}eneration},
  journal = {{Computing Research Repository}},
  year = {2015},
  volume = {abs/1502.04623},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {<a href="http://dblp.uni-trier.de/rec/bib/journals/corr/GregorDGW15">http://dblp.uni-trier.de/rec/bib/journals/corr/GregorDGW15</a>},
  file = {:pdfs/2015 - DRAW - a Recurrent Neural Network for Image Generation.pdf:PDF},
  groups = {handwriting},
  url = {<a href="http://arxiv.org/abs/1502.04623">http://arxiv.org/abs/1502.04623</a>}
}
</pre>

<a name="Harman2011"></a><pre>
@book{<a href="docs\OMR-Related.html#Harman2011">Harman2011</a>,
  title = {Information Retrieval Evaluation},
  publisher = {Morgan \& Claypool Publishers},
  year = {2011},
  author = {Harman, Donna},
  edition = {1st},
  isbn = {1598299719, 9781598299717}
}
</pre>

<a name="He2016"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#He2016">He2016</a>,
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title = {Deep residual learning for image recognition},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recogntiion (CVPR)},
  year = {2016},
  pages = {770--778},
  file = {:pdfs/2016 - Deep Residual Learning for Image Recognition.pdf:PDF},
  url = {<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf</a>}
}
</pre>

<a name="Heussenstamm1987"></a><pre>
@book{<a href="docs\OMR-Related.html#Heussenstamm1987">Heussenstamm1987</a>,
  title = {The Norton Manual of Music Notation},
  publisher = {W. W. Norton \& Company},
  year = {1987},
  author = {George Heussenstamm},
  isbn = {9780393955262}
}
</pre>

<a name="Hosang2016"></a><pre>
@article{<a href="docs\OMR-Related.html#Hosang2016">Hosang2016</a>,
  author = {J. Hosang and R. Benenson and P. Doll{\'{a}}r and B. Schiele},
  title = {What Makes for Effective Detection Proposals?},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2016},
  volume = {38},
  number = {4},
  pages = {814--830},
  issn = {0162-8828},
  abstract = {Current top performing object detectors employ detection proposals
	to guide the search for objects, thereby avoiding exhaustive sliding
	window search across images. Despite the popularity and widespread
	use of detection proposals, it is unclear which trade-offs are made
	when using them during object detection. We provide an in-depth analysis
	of twelve proposal methods along with four baselines regarding proposal
	repeatability, ground truth annotation recall on PASCAL, ImageNet,
	and MS COCO, and their impact on DPM, R-CNN, and Fast R-CNN detection
	performance. Our analysis shows that for object detection improving
	proposal localisation accuracy is as important as improving recall.
	We introduce a novel metric, the average recall (AR), which rewards
	both high recall and good localisation and correlates surprisingly
	well with detection performance. Our findings show common strengths
	and weaknesses of existing methods, and provide insights and metrics
	for selecting and tuning proposal methods.},
  doi = {10.1109/TPAMI.2015.2465908},
  file = {:pdfs/../Deep Learning Papers/2016 - What makes for Effective Detection Proposals.pdf:PDF;:2016 - What Makes for Effective Detection Proposals.pdf:PDF},
  keywords = {object detection;DPM detection performance;Fast R-CNN detection performance;ImageNet;MS COCO;PASCAL;average recall;ground truth annotation recall;object detection;object localisation;proposal localisation accuracy;proposal repeatability;Detectors;Image edge detection;Image segmentation;Measurement;Object detection;Proposals;Training;Computer Vision;Computer vision;detection proposals;object detection}
}
</pre>

<a name="Hu2017"></a><pre>
@article{<a href="docs\OMR-Related.html#Hu2017">Hu2017</a>,
  author = {Ronghang Hu and Piotr Doll{\'{a}}r and Kaiming He and Trevor Darrell and Ross B. Girshick},
  title = {Learning to Segment Every Thing},
  journal = {CoRR},
  year = {2017},
  volume = {abs/1711.10370},
  archiveprefix = {arXiv},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/bib/journals/corr/abs-1711-10370},
  eprint = {1711.10370},
  file = {:pdfs/2017 - Learning to Segment Every Thing.pdf:PDF},
  url = {<a href="http://arxiv.org/abs/1711.10370">http://arxiv.org/abs/1711.10370</a>}
}
</pre>

<a name="Huang2016"></a><pre>
@techreport{<a href="docs\OMR-Related.html#Huang2016">Huang2016</a>,
  author = {{H}uang, {A}llen and {W}u, {R}aymond},
  title = {{D}eep {L}earning for {M}usic},
  institution = {Stanford University},
  year = {2016},
  file = {:pdfs/2016 - Deep Learning for Music.pdf:PDF},
  school = {Stanford University},
  url = {https://arxiv.org/abs/1606.04930}
}
</pre>

<a name="Huang2017"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Huang2017">Huang2017</a>,
  author = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and Murphy, Kevin},
  title = {Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2017},
  file = {:pdfs/2017 - Speed_Accuracy Trade Offs for Modern Convolutional Object Detectors.pdf:PDF},
  url = {<a href="http://openaccess.thecvf.com/content_cvpr_2017/html/Huang_SpeedAccuracy_Trade-Offs_for_CVPR_2017_paper.html">http://openaccess.thecvf.com/content_cvpr_2017/html/Huang_SpeedAccuracy_Trade-Offs_for_CVPR_2017_paper.html</a>}
}
</pre>

<a name="IMSLP"></a><pre>
@misc{<a href="docs\OMR-Related.html#IMSLP">IMSLP</a>,
  author = {{Project Petrucci LLC}},
  title = {International Music Score Library Project},
  year = {2006},
  url = {<a href="http://imslp.org/">http://imslp.org/</a>}
}
</pre>

<a name="Journet2017"></a><pre>
@article{<a href="docs\OMR-Related.html#Journet2017">Journet2017</a>,
  author = {Journet, Nicholas and Visani, Muriel and Mansencal, Boris and Van-Cuong, Kieu and Billy, Antoine},
  title = {DocCreator: A New Software for Creating Synthetic Ground-Truthed Document Images},
  journal = {Journal of imaging},
  year = {2017},
  volume = {3},
  number = {4},
  pages = {62},
  doi = {10.3390/jimaging3040062},
  file = {:pdfs/2017 - DocCreator - a New Software for Creating Synthetic Ground Truthed Document Images.pdf:PDF},
  publisher = {Multidisciplinary Digital Publishing Institute}
}
</pre>

<a name="Kasturi1988"></a><pre>
@article{<a href="docs\OMR-Related.html#Kasturi1988">Kasturi1988</a>,
  author = {R. Kasturi and L. Fletcher},
  title = {A Robust Algorithm for Text String Separation from Mixed Text/Graphics Images},
  journal = {IEEE Transactions on Pattern Analysis \& Machine Intelligence},
  year = {1988},
  volume = {10},
  pages = {910--918},
  month = {11},
  issn = {0162-8828},
  doi = {10.1109/34.9112},
  file = {:pdfs/1988 - A Robust Algorithm for Text String Separation from Mixed Text - Graphics Images.pdf:PDF},
  keywords = {computerized picture processing; computer graphics; text string separation; mixed text/graphics images; document analysis; Hough transform; character recognition; graphics recognition; computer graphics; computerised pattern recognition; computerised picture processing; transforms}
}
</pre>

<a name="Katayose1989"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Katayose1989">Katayose1989</a>,
  author = {Katayose, H. and Kato, H. and Imai, M. and S., Inokuchi},
  title = {An approach to an artificial music expert},
  booktitle = {International Computer Music Conference},
  year = {1989},
  pages = {139--146}
}
</pre>

<a name="Katayose1989a"></a><pre>
@article{<a href="docs\OMR-Related.html#Katayose1989a">Katayose1989a</a>,
  author = {Haruhiro Katayose and Seiji Inokuchi},
  title = {The Kansei Music System},
  journal = {Computer Music Journal},
  year = {1989},
  volume = {13},
  number = {4},
  pages = {72--77},
  issn = {01489267, 15315169},
  publisher = {The MIT Press},
  url = {<a href="http://www.jstor.org/stable/3679555">http://www.jstor.org/stable/3679555</a>}
}
</pre>

<a name="Keil2017"></a><pre>
@article{<a href="docs\OMR-Related.html#Keil2017">Keil2017</a>,
  author = {Keil, Klaus and Ward, Jennifer A.},
  title = {Applications of RISM data in digital libraries and digital musicology},
  journal = {International Journal on Digital Libraries},
  year = {2017},
  month = {Jan},
  issn = {1432-1300},
  abstract = {Information about manuscripts and printed music indexed in RISM (R{\'e}pertoire International des Sources Musicales), a large, international project that records and describes musical sources, was for decades available solely through book publications, CD-ROMs, or subscription services. Recent initiatives to make the data available on a wider scale have resulted in, most significantly, a freely accessible online database and the availability of its data as open data and linked open data. Previously, the task of increasing the amount of data was primarily carried out by RISM national groups and the Zentralredaktion (Central Office). The current opportunities available by linking to other freely accessible databases and importing data from other resources open new perspectives and prospects. This paper describes the RISM data and their applications for digital libraries and digital musicological projects. We discuss the possibilities and challenges in making available a large and growing quantity of data and how the data have been utilized in external library and musicological projects. Interactive functions in the RISM OPAC are planned for the future, as is closer collaboration with the projects that use RISM data. Ultimately, RISM would like to arrange a ``take and give'' system in which the RISM data are used in external projects, enhanced by the project participants, and then delivered back to the RISM Zentralredaktion.},
  day = {06},
  doi = {10.1007/s00799-016-0205-3},
  file = {:pdfs/2017 - Applications of RISM Data in Digital Libraries and Digital Musicology.pdf:PDF},
  url = {https://doi.org/10.1007/s00799-016-0205-3}
}
</pre>

<a name="Kingma2014"></a><pre>
@article{<a href="docs\OMR-Related.html#Kingma2014">Kingma2014</a>,
  author = {Diederik P. Kingma and Jimmy Ba},
  title = {Adam: A Method for Stochastic Optimization},
  journal = {{Computing Research Repository}},
  year = {2014},
  volume = {abs/1412.6980},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {<a href="http://dblp.uni-trier.de/rec/bib/journals/corr/KingmaB14">http://dblp.uni-trier.de/rec/bib/journals/corr/KingmaB14</a>},
  file = {:pdfs/2014 - Adam - a Method for Stochastic Optimization.pdf:PDF},
  url = {<a href="http://arxiv.org/abs/1412.6980">http://arxiv.org/abs/1412.6980</a>}
}
</pre>

<a name="Koehn2005"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Koehn2005">Koehn2005</a>,
  author = {{K}oehn, {P}hilipp},
  title = {{E}uroparl: {A} parallel corpus for statistical machine translation},
  booktitle = {MT summit},
  year = {2005},
  volume = {5},
  pages = {79--86},
  file = {:pdfs/2005 - Europarl - a Parallel Corpus for Statistical Machine Translation.pdf:PDF},
  url = {<a href="http://courses.washington.edu/ling473/Project5.pdf">http://courses.washington.edu/ling473/Project5.pdf</a>}
}
</pre>

<a name="Kurth2007"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Kurth2007">Kurth2007</a>,
  author = {{K}urth, {F}. and {M}{\"{u}}ller, {M}. and {F}remerey, {C}. and {C}hang, {Y}. and {C}lausen, {M}.},
  title = {{Automated synchronization of scanned sheet music with audio recordings}},
  booktitle = {8th International Conference on Music Information Retrieval},
  year = {2007},
  pages = {261--266},
  address = {Vienna, Austria},
  file = {:pdfs/2007 - Automated Synchronization of Scanned Sheet Music with Audio Recordings.pdf:PDF},
  isbn = {978-3-85403-218},
  keywords = {others},
  url = {<a href="http://ismir2007.ismir.net/proceedings/ISMIR2007_p261_kurth.pdf">http://ismir2007.ismir.net/proceedings/ISMIR2007_p261_kurth.pdf</a>}
}
</pre>

<a name="Lake2013"></a><pre>
@incollection{<a href="docs\OMR-Related.html#Lake2013">Lake2013</a>,
  author = {{L}ake, {B}renden {M} and {S}alakhutdinov, {R}uslan {R} and {T}enenbaum, {J}osh},
  title = {{O}ne-shot learning by inverting a compositional causal process},
  booktitle = {Advances in Neural Information Processing Systems 26},
  publisher = {Curran Associates, Inc.},
  year = {2013},
  editor = {C.J.C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
  pages = {2526--2534},
  file = {:pdfs/2013 - One Shot Learning by Inverting a Compositional Causal Process.pdf:PDF},
  groups = {handwriting},
  url = {<a href="http://papers.nips.cc/paper/5128-one-shot-learning-by-inverting-a-compositional-causal-process.pdf">http://papers.nips.cc/paper/5128-one-shot-learning-by-inverting-a-compositional-causal-process.pdf</a>}
}
</pre>

<a name="Lavie2007"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Lavie2007">Lavie2007</a>,
  author = {{L}avie, {A}lon and {A}garwal, {A}bhaya},
  title = {{M}eteor: {A}n {A}utomatic {M}etric for {MT} {E}valuation with {H}igh {L}evels of {C}orrelation with {H}uman {J}udgments},
  booktitle = {Second Workshop on Statistical Machine Translation},
  year = {2007},
  series = {StatMT '07},
  pages = {228--231},
  address = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid = {1626389},
  file = {:pdfs/2007 - Meteor - an Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments.pdf:PDF},
  location = {Prague, Czech Republic},
  numpages = {4},
  url = {<a href="http://dl.acm.org/citation.cfm?id=1626355.1626389">http://dl.acm.org/citation.cfm?id=1626355.1626389</a>}
}
</pre>

<a name="LeCun1998"></a><pre>
@article{<a href="docs\OMR-Related.html#LeCun1998">LeCun1998</a>,
  author = {{L}e{C}un, {Y}ann and {B}ottou, {L}{\'e}on and {B}engio, {Y}oshua and {H}affner, {P}atrick},
  title = {Gradient-based learning applied to document recognition},
  journal = {Proceedings of the IEEE},
  year = {1998},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {0018-9219},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  doi = {10.1109/5.726791},
  file = {:pdfs/1998 - Gradient Based Learning Applied to Document Recognition.pdf:PDF},
  keywords = {optical character recognition;multilayer perceptrons;backpropagation;convolution;gradient-based learning;document recognition;multilayer neural networks;back-propagation;gradient based learning technique;complex decision surface synthesis;high-dimensional patterns;handwritten character recognition;handwritten digit recognition task;2D shape variability;document recognition systems;field extraction;segmentation recognition;language modeling;graph transformer networks;GTN;multimodule systems;performance measure minimization;cheque reading;convolutional neural network character recognizers;Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis}
}
</pre>

<a name="Lee2016"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Lee2016">Lee2016</a>,
  author = {C. Lee and H. J. Kim and K. W. Oh},
  title = {Comparison of faster R-{CNN} models for object detection},
  booktitle = {2016 16th International Conference on Control, Automation and Systems (ICCAS)},
  year = {2016},
  pages = {107--110},
  abstract = {Object detection is one of the important problems for autonomous robots.
	Faster R-CNN, one of the state-of-the-art object detection methods,
	approaches real time application; nevertheless, computational time
	lies borderline of real time application, i.e. 5fps with VGG16 model
	in K40 GPU system in [1]. Moreover, computation time depends on model
	and image crop size, but precision is also affected; usually, time
	and precision have trade-off relation. By adjusting input image size
	in spite of downgrading performance, computation time meets criteria
	for one model. Therefore, selection of a model is one of the important
	problems when faster R-CNN based object detection system for an autonomous
	robot is constructed. In this paper, we convert several state-of-the-art
	models from convolution neural network (CNN) for image classification.
	Then, we compare converted models with several image crop size in
	terms of computation time and detection precision. We will utilize
	those comparison data for selecting a proper detection model in case
	a robot needs to perform an object detection task.},
  doi = {10.1109/ICCAS.2016.7832305},
  file = {:pdfs/2016 - Comparison of Faster R CNN Models for Object Detection.pdf:PDF},
  keywords = {convolution;image classification;mobile robots;neural nets;object detection;R-CNN model;autonomous robot;convolution neural network;image classification;object detection;Agriculture;Computational modeling;Convolution;Data models;Feature extraction;Object detection;Robots;Convolution neural network;Faster R-CNN;Object detection}
}
</pre>

<a name="Lewis2004"></a><pre>
@article{<a href="docs\OMR-Related.html#Lewis2004">Lewis2004</a>,
  author = {{L}ewis, {D}avid {D} and {Y}ang, {Y}iming and {R}ose, {T}ony {G} and {L}i, {F}an},
  title = {{R}cv1: {A} new benchmark collection for text categorization research},
  journal = {The Journal of Machine Learning Research},
  year = {2004},
  volume = {5},
  pages = {361--397},
  file = {:pdfs/2004 - Rcv1 - a New Benchmark Collection for Text Categorization Research.pdf:PDF},
  publisher = {JMLR. org},
  url = {<a href="http://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf">http://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf</a>}
}
</pre>

<a name="Lilypond2014"></a><pre>
@manual{<a href="docs\OMR-Related.html#Lilypond2014">Lilypond2014</a>,
  title = {LilyPond - Essay on automated music engraving},
  author = {{The LilyPond Developement Team}},
  year = {2014},
  file = {:pdfs/2014 - LilyPond Essay on Automated Music Engraving.pdf:PDF},
  keywords = {music notation},
  url = {<a href="http://www.lilypond.org/">http://www.lilypond.org/</a>}
}
</pre>

<a name="Lin2014"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Lin2014">Lin2014</a>,
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C. Lawrence},
  title = {Microsoft COCO: Common Objects in Context},
  booktitle = {Computer Vision -- ECCV 2014},
  year = {2014},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  pages = {740--755},
  address = {Cham},
  publisher = {Springer International Publishing},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  file = {:pdfs/2014 - Microsoft COCO - Common Objects in Context.pdf:PDF},
  isbn = {978-3-319-10602-1},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48}
}
</pre>

<a name="Lin2017"></a><pre>
@article{<a href="docs\OMR-Related.html#Lin2017">Lin2017</a>,
  author = {Tsung{-}Yi Lin and Priya Goyal and Ross B. Girshick and Kaiming He and Piotr Doll{\'{a}}r},
  title = {Focal Loss for Dense Object Detection},
  journal = {CoRR},
  year = {2017},
  volume = {abs/1708.02002},
  archiveprefix = {arXiv},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {<a href="http://dblp.org/rec/bib/journals/corr/abs-1708-02002">http://dblp.org/rec/bib/journals/corr/abs-1708-02002</a>},
  file = {:pdfs/2017 - Focal Loss for Dense Object Detection.pdf:PDF},
  url = {<a href="http://arxiv.org/abs/1708.02002">http://arxiv.org/abs/1708.02002</a>}
}
</pre>

<a name="Lin2017a"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Lin2017a">Lin2017a</a>,
  author = {Lin, Tsung-Yi and Dollar, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  title = {Feature Pyramid Networks for Object Detection},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2017},
  file = {:pdfs/2017 - Feature Pyramid Networks for Object Detection.pdf:PDF},
  url = {<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf</a>}
}
</pre>

<a name="Liu2016"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Liu2016">Liu2016</a>,
  author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  title = {SSD: Single Shot MultiBox Detector},
  booktitle = {Computer Vision -- ECCV 2016},
  year = {2016},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  pages = {21--37},
  address = {Cham},
  publisher = {Springer International Publishing},
  abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300x300 input, SSD achieves 74.3% mAP on VOC2007 test at 59Â FPS on a Nvidia Titan X and for 512x512 input, SSD achieves 76.9% mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.},
  file = {:pdfs/2016 - SSD - Single Shot MultiBox Detector.pdf:PDF},
  isbn = {978-3-319-46448-0},
  url = {https://link.springer.com/chapter/10.1007%2F978-3-319-46448-0_2}
}
</pre>

<a name="Machacek2014"></a><pre>
@article{<a href="docs\OMR-Related.html#Machacek2014">Machacek2014</a>,
  author = {{M}atou{\v{s}} {M}ach{\'{a}}{\v{c}}ek and {O}nd{\v{r}}ej {B}ojar},
  title = {{R}esults of the {WMT}14 {M}etrics {S}hared {T}ask},
  journal = {Ninth Workshop on Statistical Machine Translation},
  year = {2014},
  pages = {293--301},
  address = {Baltimore, {MD}, {USA}},
  file = {:pdfs/2014 - Results of the WMT14 Metrics Shared Task.pdf:PDF},
  isbn = {978-1-941643-17-4},
  publisher = {Association for Computational Linguistics},
  venue = {Baltimore Marriott Waterfront}
}
</pre>

<a name="Machacek2015"></a><pre>
@article{<a href="docs\OMR-Related.html#Machacek2015">Machacek2015</a>,
  author = {{M}atou{\v{s}} {M}ach{\'{a}}{\v{c}}ek and {O}nd{\v{r}}ej {B}ojar},
  title = {{E}valuating {M}achine {T}ranslation {Q}uality {U}sing {S}hort {S}egments {A}nnotations},
  journal = {The Prague Bulletin of Mathematical Linguistics},
  year = {2015},
  volume = {103},
  number = {1},
  doi = {10.1515/pralin-2015-0005},
  file = {:pdfs/2015 - Evaluating Machine Translation Quality Using Short Segments Annotations.pdf:PDF},
  publisher = {Walter de Gruyter {GmbH}}
}
</pre>

<a name="Mandt2017"></a><pre>
@article{<a href="docs\OMR-Related.html#Mandt2017">Mandt2017</a>,
  author = {{Mandt}, {S}. and {Hoffman}, {M}.~{D}. and {Blei}, {D}.~{M}.},
  title = {{Stochastic Gradient Descent as Approximate Bayesian Inference}},
  journal = {ArXiv e-prints},
  year = {2017},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl = {<a href="http://adsabs.harvard.edu/abs/2017arXiv170404289M">http://adsabs.harvard.edu/abs/2017arXiv170404289M</a>},
  archiveprefix = {arXiv},
  eprint = {1704.04289},
  keywords = {Statistics - Machine Learning, Computer Science - Learning},
  file = {:pdfs/./nnets/HoffmanBlei1704.04289.pdf:pdf},
  url = {https://arxiv.org/abs/1704.04289}
}
</pre>

<a name="Manning2008"></a><pre>
@book{<a href="docs\OMR-Related.html#Manning2008">Manning2008</a>,
  title = {Introduction to Information Retrieval},
  publisher = {Cambridge University Press},
  year = {2008},
  author = {Manning, Chirstopher D. and Raghavan, Prabhakar and Sch{\"{u}}tze, Hinrich},
  isbn = {978-0-521-86571-5},
  file = {:pdfs/2008 - Introduction to Information Retrieval.pdf:PDF},
  url = {https://nlp.stanford.edu/IR-book/}
}
</pre>

<a name="Marr2010"></a><pre>
@book{<a href="docs\OMR-Related.html#Marr2010">Marr2010</a>,
  title = {Vision},
  publisher = {W.H. Freeman and Company San Francisco},
  year = {2010},
  author = {David Marr},
  doi = {10.7551/mitpress/9780262514620.001.0001}
}
</pre>

<a name="Marsden2012"></a><pre>
@article{<a href="docs\OMR-Related.html#Marsden2012">Marsden2012</a>,
  author = {Alan Marsden},
  title = {Interrogating Melodic Similarity: A Definitive Phenomenon or the Product of Interpretation?},
  journal = {Journal of New Music Research},
  year = {2012},
  volume = {41},
  number = {4},
  pages = {323--335},
  file = {:pdfs/2012 - Interrogating Melodic Similarity - A Definitive Phenomenon or the Product of Interpretation.pdf:PDF},
  publisher = {Routledge}
}
</pre>

<a name="Mattheson1739"></a><pre>
@book{<a href="docs\OMR-Related.html#Mattheson1739">Mattheson1739</a>,
  title = {Der vollkommene Capellmeister},
  publisher = {Herold, Christian, Hamburg},
  year = {1739},
  author = {Mattheson, Johann},
  isbn = {9783761814130},
  url = {https://imslp.org/wiki/Der_vollkommene_Capellmeister_(Mattheson,_Johann)}
}
</pre>

<a name="McKay2004"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#McKay2004">McKay2004</a>,
  author = {McKay, Cory and Fujinaga, Ichiro},
  title = {Automatic genre classification using large high-level musical feature sets},
  booktitle = {5th International Conference on Music Information Retrieval},
  year = {2004},
  address = {Barcelona, Spain},
  file = {:pdfs/2004 - Automatic Genre Classification Using Large High Level Musical Feature Sets.pdf:PDF},
  url = {<a href="http://ismir2004.ismir.net/proceedings/p095-page-525-paper240.pdf">http://ismir2004.ismir.net/proceedings/p095-page-525-paper240.pdf</a>}
}
</pre>

<a name="McKay2006"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#McKay2006">McKay2006</a>,
  author = {McKay, C. and Fujinaga, I.},
  title = {jSymbolic: A feature extractor for MIDI files},
  booktitle = {International Computer Music Conference},
  year = {2006},
  pages = {302--305},
  address = {New Orleans, LA},
  file = {:pdfs/2006 - JSymbolic - a Feature Extractor for MIDI Files.pdf:PDF},
  url = {https://www.music.mcgill.ca/~cmckay/papers/musictech/McKay_ICMC_06_jSymbolic.pdf}
}
</pre>

<a name="MerchanSanchez-Jara2015"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#MerchanSanchez-Jara2015">MerchanSanchez-Jara2015</a>,
  author = {Merch\'{a}n S\'{a}nchez-Jara, Javier},
  title = {e-Score; Impact, Perception and Uses in Music Educational Institutions},
  booktitle = {3rd International Conference on Technological Ecosystems for Enhancing Multiculturality},
  year = {2015},
  editor = {Felgueiras M.C., Alves G.R.},
  volume = {Part F118785},
  pages = {449--454},
  publisher = {Association for Computing Machinery},
  abbrev_source_title = {ACM Int. Conf. Proc. Ser.},
  abstract = {Parallel to the very first music expressions as a human activity,
	developed under some kind of guidelines and precepts, there have
	arisen relentless efforts and attempts to achieve the long-lasting
	and stable representation of the musical products being created,
	in a way that would allow to recreate them as closely as posible
	to the original event.from the very beginning until the present days,
	have been many notation systems, formats, and expressions developed
	with this purpose in mind. With the incursion and implementation
	of the new digital technologies in the musical activity spheres,
	a new paradigm emerge in the representation and transmission of the
	musical work, culminating nowadays with the born of a new music document
	typology tha may be called digital music score or e-Score. The aim
	of this paper is lay down and approach to this new type of object
	for music texts reading, editing and transmission through a quantitive
	research, being the scope of the study to analyze and understand
	how electronic music scores (e-Score) are received, used and perceived
	by music students and teachers in any of the music learning centers
	in our country. The main purpose is to provide scientific evidence,
	through statistical methods, of the level of implementation and use
	of these new support type for music knowledge transmission, as well
	as the most influential aspects implied in perception of therm amongst
	musical community in Spain.},
  affiliation = {E-Lectra (Universidad de Salamanca, Spain},
  author_keywords = {Digital e-learning; Digital sheet music; E-Score; Electronic edition; Music education research; Music readers},
  correspondence_address1 = {SÃ¡nchez-Jara, J.M.; E-Lectra (Universidad de SalamancaSpain; email: javiermerchan@usal.es},
  document_type = {Conference Paper},
  doi = {10.1145/2808580.2808647},
  file = {:pdfs/2015 - e-Score - Impact, perception and uses in music educational institutions.pdf:PDF},
  isbn = {9781450334426},
  journal = {ACM International Conference Proceeding Series},
  keywords = {Computer music; Ecology; Ecosystems; Education; Teaching, Digital sheet music; Digital technologies; Educational institutions; Electronic editions; Human activities; Music education; Music readers; Scientific evidence, E-learning},
  language = {English}
}
</pre>

<a name="Mnih2014"></a><pre>
@article{<a href="docs\OMR-Related.html#Mnih2014">Mnih2014</a>,
  author = {{V}olodymyr {M}nih and {N}icolas {H}eess and {A}lex {G}raves and {K}oray {K}avukcuoglu},
  title = {{R}ecurrent {M}odels of {V}isual {A}ttention},
  journal = {{Computing Research Repository}},
  year = {2014},
  volume = {abs/1406.6247},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {<a href="http://dblp.uni-trier.de/rec/bib/journals/corr/MnihHGK14">http://dblp.uni-trier.de/rec/bib/journals/corr/MnihHGK14</a>},
  file = {:pdfs/2014 - Recurrent Models of Visual Attention.pdf:PDF},
  groups = {handwriting},
  url = {<a href="http://arxiv.org/abs/1406.6247">http://arxiv.org/abs/1406.6247</a>}
}
</pre>

<a name="Muge2000"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Muge2000">Muge2000</a>,
  author = {Muge, F. and Granado, I. and Mengucci, M. and Pina, P. and Ramos, V. and Sirakov, N. and Caldas Pinto, J. R. and Marcolino, A. and Ramalho, M{\'a}rio and Vieira, P. and Maia do Amaral, A.},
  title = {Automatic Feature Extraction and Recognition for Digital Access of Books of the Renaissance},
  booktitle = {Research and Advanced Technology for Digital Libraries},
  year = {2000},
  editor = {Borbinha, Jos{\'e} and Baker, Thomas},
  pages = {1--13},
  address = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract = {Antique printed books constitute a heritage that should be preserved and used. With novel digitising techniques is now possible to have these books stored in digital format and accessible to a wider public. However it remains the problem of how to use them. DEBORA (Digital accEss to BOoks of the RenAissance) is a European project that aims to develop a system to interact with these books through world-wide networks. The main issue is to build a database accessible through client computers. That will require to built accompanying metadata that should characterise different components of the books as illuminated letters, banners, figures and key words in order to simplify and speed up the remote access. To solve these problems, digital image analysis algorithms regarding filtering, segmentation, separation of text from non-text, lines and word segmentation and word recognition were developed. Some novel ideas are presented and illustrated through examples.},
  doi = {10.1007/3-540-45268-0_1},
  file = {:pdfs/2000 - Automatic Feature Extraction and Recognition for Digital Access of Books of the Renaissance.pdf:PDF},
  isbn = {978-3-540-45268-3}
}
</pre>

<a name="Ng2008"></a><pre>
@book{<a href="docs\OMR-Related.html#Ng2008">Ng2008</a>,
  title = {{Interactive Multimedia Music Technologies}},
  publisher = {IGI Global},
  year = {2008},
  author = {Ng, Kia and Nesi, Paolo},
  isbn = {1599041529},
  abstract = {{M}any multimedia music content owners and distributors are converting their archives of music scores from paper into digital images, and to machine readable symbolic notation in order to survive in the business world. {I}nteractive {M}ultimedia {M}usic {T}echnologies discusses relevant state-of-the-art technologies and consists of analysis, knowledge, and application scenarios as surveyed, analyzed, and evaluated by industry professionals. {I}nteractive {M}ultimedia {M}usic {T}echnologies exemplifies the newest functionalities of multimedia interactive music to be used for valorizing cultural heritage, content and archives that are not currently distributed due to lack of safety, suitable coding models, and conversion technologies. {I}nteractive {M}ultimedia {M}usic {T}echnologies explains new and innovative methods of promoting music and products for entertainment, distance teaching, valorizing archives, and commercial and non-commercial purposes, and provides new services for those connected via personal computers, mobile and other devices, for both sighted and print-impaired consumers.},
  doi = {10.4018/978-1-59904-150-6},
  file = {:pdfs/2008 - Interactive Multimedia Music Technologies.pdf:PDF},
  keywords = {others},
  pages = {394}
}
</pre>

<a name="Nielsen2009"></a><pre>
@techreport{<a href="docs\OMR-Related.html#Nielsen2009">Nielsen2009</a>,
  author = {Nielsen, J.},
  title = {Statistical Analysis of Music Corpora},
  institution = {Dept. of Computer Science, University of Copenhagen},
  year = {2009},
  address = {Copenhagen, Denmark},
  comment = {Bsc. Thesis (?)
See https://johanbrinch.com/sections/projects/music.html
Wrote his Master's Thesis in 2012},
  file = {:pdfs/2009 - Statistical Analysis of Music Corpora.pdf:PDF},
  school = {University of Copenhagen},
  url = {https://johanbrinch.com/static/papers/brinchj-2008_music.pdf}
}
</pre>

<a name="Nivre2005"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Nivre2005">Nivre2005</a>,
  author = {{N}ivre, {J}oakim and {N}ilsson, {J}ens},
  title = {{P}seudo-projective {D}ependency {P}arsing},
  booktitle = {43rd Annual Meeting on Association for Computational Linguistics},
  year = {2005},
  series = {ACL '05},
  pages = {99--106},
  address = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid = {1219853},
  doi = {10.3115/1219840.1219853},
  file = {:pdfs/2005 - Pseudo Projective Dependency Parsing.pdf:PDF},
  location = {Ann Arbor, Michigan},
  numpages = {8}
}
</pre>

<a name="Ntirogiannis2013"></a><pre>
@article{<a href="docs\OMR-Related.html#Ntirogiannis2013">Ntirogiannis2013</a>,
  author = {{N}tirogiannis, {K}onstantinos and {G}atos, {B}asilis and {P}ratikakis, {I}oannis},
  title = {{Performance evaluation methodology for historical document image binarization}},
  journal = {IEEE Transactions on Image Processing},
  year = {2013},
  volume = {22},
  number = {2},
  pages = {595--609},
  issn = {1057-7149},
  abstract = {{D}ocument image binarization is of great importance in the document image analysis and recognition pipeline since it affects further stages of the recognition process. {T}he evaluation of a binarization method aids in studying its algorithmic behavior, as well as verifying its effectiveness, by providing qualitative and quantitative indication of its performance. {T}his paper addresses a pixel-based binarization evaluation methodology for historical handwritten/machine-printed document images. {I}n the proposed evaluation scheme, the recall and precision evaluation measures are properly modified using a weighting scheme that diminishes any potential evaluation bias. {A}dditional performance metrics of the proposed evaluation scheme consist of the percentage rates of broken and missed text, false alarms, background noise, character enlargement, and merging. {S}everal experiments conducted in comparison with other pixel-based evaluation measures demonstrate the validity of the proposed evaluation scheme.},
  doi = {10.1109/TIP.2012.2219550},
  file = {:pdfs/2013 - Performance Evaluation Methodology for Historical Document Image Binarization.pdf:PDF},
  keywords = {Document image binarization,binarization,ground truth,performance evaluation},
  pmid = {23008259}
}
</pre>

<a name="Palmer1999"></a><pre>
@book{<a href="docs\OMR-Related.html#Palmer1999">Palmer1999</a>,
  title = {Vision science: Photons to phenomenology},
  publisher = {MIT press},
  year = {1999},
  author = {Palmer, Stephen E.},
  url = {https://mitpress.mit.edu/books/vision-science}
}
</pre>

<a name="Papineni2002"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Papineni2002">Papineni2002</a>,
  author = {{P}apineni, {K}ishore and {R}oukos, {S}alim and {W}ard, {T}odd and {Z}hu, {W}ei-{J}ing},
  title = {{BLEU}: {A} {M}ethod for {A}utomatic {E}valuation of {M}achine {T}ranslation},
  booktitle = {40th Annual Meeting on Association for Computational Linguistics},
  year = {2002},
  series = {ACL '02},
  pages = {311--318},
  address = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid = {1073135},
  doi = {10.3115/1073083.1073135},
  file = {:pdfs/2002 - BLEU - a Method for Automatic Evaluation of Machine Translation.pdf:PDF},
  groups = {evaluation},
  location = {Philadelphia, Pennsylvania},
  numpages = {8}
}
</pre>

<a name="Phon-Amnuaisuk2009"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Phon-Amnuaisuk2009">Phon-Amnuaisuk2009</a>,
  author = {Phon-Amnuaisuk, Somnuk},
  title = {Estimating HMM Parameters Using Particle Swarm Optimisation},
  booktitle = {Applications of Evolutionary Computing},
  year = {2009},
  editor = {Giacobini, Mario and Brabazon, Anthony and Cagnoni, Stefano and Di Caro, Gianni A. and Ek{\'a}rt, Anik{\'o} and Esparcia-Alc{\'a}zar, Anna Isabel and Farooq, Muddassar and Fink, Andreas and Machado, Penousal},
  pages = {625--634},
  address = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract = {A Hidden Markov Model (HMM) is a powerful model in describing temporal sequences. The HMM parameters are usually estimated using Baum-Welch algorithm. However, it is well known that the Baum-Welch algorithm tends to arrive at local optimal points. In this report, we investigate the potential of the Particle Swarm Optimisation (PSO) as an alternative method for HMM parameters estimation. The domain in this study is the recognition of handwritten music notations. Three observables: (i) sequence of ink patterns, (ii) stroke information and (iii) spatial information associated with eight musical symbols were recorded. Sixteen HMM models were built from the data. Eight HMM models for eight musical symbols were built from the parameters estimated using the Baum-Welch algorithm and the other eight models were built from the parameters estimated using PSO. The experiment shows that the performances of HMM models, using parameters estimated from PSO and Baum-Welch approach, are comparable. We suggest that PSO or a combination of PSO and Baum-Welch algorithm could be alternative approaches for the HMM parameters estimation.},
  isbn = {978-3-642-01129-0}
}
</pre>

<a name="Presgurvic2005"></a><pre>
@misc{<a href="docs\OMR-Related.html#Presgurvic2005">Presgurvic2005</a>,
  author = {G{\'{e}}rard Presgurvic},
  title = {Songbook Romeo \& Julia},
  year = {2005},
  address = {Vienna, Austria},
  editor = {German translation by Michaela Ronzoni; Arrangement Christian Kolonovits},
  institution = {Vereinigte B{\"{u}}hnen Wien},
  publisher = {Universal Music Publishing Limited, London},
  url = {https://www.musicalvienna.at/de/souvenirs/12/ANDERE-MUSICALS/10/Songbook-Romeo-und-Julia}
}
</pre>

<a name="Rashid2017"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Rashid2017">Rashid2017</a>,
  author = {S. F. Rashid and A. Akmal and M. Adnan and A. A. Aslam and A. Dengel},
  title = {Table Recognition in Heterogeneous Documents Using Machine Learning},
  booktitle = {2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)},
  year = {2017},
  volume = {01},
  pages = {777--782},
  month = {Nov},
  abstract = {Tables are an easy way to represent information in a structural form. Table recognition is important for the extraction of such information from document images. Usually, modern OCR systems provide textual information coming from tables without recognizing actual table structure. However, recognition of table structure is important to get the contextual meaning of the contents. Table structure recognition in heterogeneous documents is challenging due to a variety of table layouts. It becomes harder where no physical rulings are present in a table. This work proposes a novel learning based methodology for the recognition of table contents in heterogeneous document images. Textual contents of documents are classified as table or non-table elements using a pre-trained neural network model. The output of the neural network is further enhanced by applying a contextual post processing on each element to correct the classifications errors if any. The system is trained using a subset of UNLV and UW3 document images and depicted more than 97% accuracy on a test set in detection of table and non-table elements.},
  doi = {10.1109/ICDAR.2017.132},
  file = {:pdfs/2017 - Table Recognition in Heterogeneous Documents Using Machine Learning.pdf:PDF},
  issn = {2379-2140},
  keywords = {document image processing;learning (artificial intelligence);neural nets;optical character recognition;table layouts;table contents;heterogeneous document images;nontable elements;table recognition;heterogeneous documents;table structure recognition;modern OCR systems;non-table elements;Feature extraction;Optical character recognition software;Layout;Neural networks;Training;Image recognition;Character recognition}
}
</pre>

<a name="Redmon2017"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Redmon2017">Redmon2017</a>,
  author = {J. Redmon and A. Farhadi},
  title = {YOLO9000: Better, Faster, Stronger},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2017},
  pages = {6517--6525},
  abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that dont have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time.},
  doi = {10.1109/CVPR.2017.690},
  file = {:pdfs/2017 - YOLO9000 - Better, Faster, Stronger.pdf:PDF},
  issn = {1063-6919},
  keywords = {image classification;object detection;COCO detection dataset;ImageNet classification dataset;ImageNet detection task;PASCAL VOC;YOLO detection method;YOLO9000;YOLOv2 model;object classification;object detection system;Detectors;Feature extraction;Image resolution;Object detection;Real-time systems;Training}
}
</pre>

<a name="Ren2015"></a><pre>
@incollection{<a href="docs\OMR-Related.html#Ren2015">Ren2015</a>,
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  booktitle = {Advances in Neural Information Processing Systems 28},
  publisher = {Curran Associates, Inc.},
  year = {2015},
  editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  pages = {91--99},
  file = {:pdfs/2015 - Faster R-CNN - Towards Real Time Object Detection with Region Proposal Networks.pdf:PDF;:2015 - Faster R CNN - Towards Real Time Object Detection with Region Proposal Networks - extended.pdf:PDF},
  url = {<a href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf">http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf</a>}
}
</pre>

<a name="Roman2018"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Roman2018">Roman2018</a>,
  author = {Miguel A. Rom{\'{a}}n and Antonio Pertusa and Jorge Calvo-Zaragoza},
  title = {And End-To-End Framework for Audio-to-Score Music Transcription on Monophonic Excerpts},
  booktitle = {19th International Society for Music Information Retrieval Conference},
  year = {2018},
  pages = {34--41},
  address = {Paris, France},
  file = {:pdfs/2018 - And End to End Framework for Audio to Score Music Transcription on Monophonic Excerpts.pdf:PDF},
  isbn = {978-2-9540351-2-3},
  url = {<a href="http://ismir2018.ircam.fr/doc/pdfs/87_Paper.pdf">http://ismir2018.ircam.fr/doc/pdfs/87_Paper.pdf</a>}
}
</pre>

<a name="Ronneberger2015"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Ronneberger2015">Ronneberger2015</a>,
  author = {{R}onneberger, {O}laf and {F}ischer, {P}hilipp and {B}rox, {T}homas},
  title = {{U}-{N}et: {C}onvolutional {N}etworks for {B}iomedical {I}mage {S}egmentation},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III},
  year = {2015},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  pages = {234--241},
  address = {Cham},
  publisher = {Springer International Publishing},
  abstract = {{T}here is large consent that successful training of deep networks requires many thousand annotated training samples. {I}n this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. {T}he architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. {W}e show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the {ISBI} challenge for segmentation of neuronal structures in electron microscopic stacks. {U}sing the same network trained on transmitted light microscopy images (phase contrast and {DIC}) we won the {ISBI} cell tracking challenge 2015 in these categories by a large margin. {M}oreover, the network is fast. {S}egmentation of a 512x512 image takes less than a second on a recent {GPU}. {T}he full implementation (based on {C}affe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  doi = {10.1007/978-3-319-24574-4_28},
  file = {:pdfs/2015 - U Net - Convolutional Networks for Biomedical Image Segmentation.pdf:PDF},
  isbn = {978-3-319-24574-4}
}
</pre>

<a name="Sampson1985"></a><pre>
@book{<a href="docs\OMR-Related.html#Sampson1985">Sampson1985</a>,
  title = {{W}riting {S}ystems: {A} {L}inguistic {I}ntroduction},
  publisher = {Stanford University Press},
  year = {1985},
  author = {{S}ampson, {G}eoffrey},
  isbn = {9780804717564},
  lccn = {84040708},
  url = {https://books.google.cz/books?id=tVcdNRvwoDkC}
}
</pre>

<a name="Sauvola2000"></a><pre>
@article{<a href="docs\OMR-Related.html#Sauvola2000">Sauvola2000</a>,
  author = {Sauvola, J. and Pietik{\"{a}}inen, M.},
  title = {Adaptive document image binarization},
  journal = {Pattern Recognition},
  year = {2000},
  volume = {33},
  number = {2},
  pages = {225--236},
  issn = {0031-3203},
  abstract = {A new method is presented for adaptive document image binarization, where the page is considered as a collection of subcomponents such as text, background and picture. The problems caused by noise, illumination and many source type-related degradations are addressed. Two new algorithms are applied to determine a local threshold for each pixel. The performance evaluation of the algorithm utilizes test images with ground-truth, evaluation metrics for binarization of textual and synthetic images, and a weight-based ranking procedure for the final result presentation. The proposed algorithms were tested with images including different types of document components and degradations. The results were compared with a number of known techniques in the literature. The benchmarking results show that the method adapts and performs well in each case qualitatively and quantitatively.},
  doi = {10.1016/S0031-3203(99)00055-2},
  file = {:pdfs/2000 - Adaptive Document Image Binarization.pdf:PDF},
  isbn = {0818678984},
  keywords = {adaptive binarization,binarization,document analysis,document segmentation,document understanding,soft decision},
  pmid = {84262800005}
}
</pre>

<a name="Saxena2017"></a><pre>
@article{<a href="docs\OMR-Related.html#Saxena2017">Saxena2017</a>,
  author = {Lalit Prakash Saxena},
  title = {Niblack's binarization method and its modifications to real-time applications: a review},
  journal = {Artificial Intelligence Review},
  year = {2017},
  doi = {10.1007/s10462-017-9574-2},
  file = {:pdfs/2017 - Niblack's binarization method and its modifications to real-time applications - A Review.pdf:PDF},
  publisher = {Springer Nature}
}
</pre>

<a name="Schmidhuber2015"></a><pre>
@article{<a href="docs\OMR-Related.html#Schmidhuber2015">Schmidhuber2015</a>,
  author = {{S}chmidhuber, {J}{\"u}rgen},
  title = {{D}eep learning in neural networks: {A}n overview},
  journal = {Neural networks},
  year = {2015},
  volume = {61},
  pages = {85--117},
  file = {:pdfs/2015 - Deep Learning in Neural Networks - an Overview.pdf:PDF},
  publisher = {Elsevier},
  url = {https://arxiv.org/pdf/1404.7828.pdf}
}
</pre>

<a name="Selfridge-Field1997"></a><pre>
@book{<a href="docs\OMR-Related.html#Selfridge-Field1997">Selfridge-Field1997</a>,
  title = {Beyond MIDI: The Handbook of Musical Codes},
  publisher = {MIT Press},
  year = {1997},
  author = {Selfridge-Field, Eleanor},
  address = {Cambridge, MA, USA},
  isbn = {0-262-19394-9},
  url = {https://mitpress.mit.edu/books/beyond-midi}
}
</pre>

<a name="Sermanet2013"></a><pre>
@article{<a href="docs\OMR-Related.html#Sermanet2013">Sermanet2013</a>,
  author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Micha{\"e}l and Fergus, Rob and LeCun, Yann},
  title = {Overfeat: Integrated recognition, localization and detection using convolutional networks},
  journal = {CoRR},
  year = {2013},
  volume = {abs/1312.6229},
  file = {:pdfs/2013 - Overfeat - Integrated Recognition, Localization and Detection Using Convolutional Networks.pdf:PDF},
  url = {<a href="http://arxiv.org/abs/1312.6229">http://arxiv.org/abs/1312.6229</a>}
}
</pre>

<a name="Serra2017"></a><pre>
@article{<a href="docs\OMR-Related.html#Serra2017">Serra2017</a>,
  author = {Serra, Xavier},
  title = {The computational study of a musical culture through its digital traces},
  journal = {Acta Musicologica. 2017; 89 (1): 24-44.},
  year = {2017},
  comment = {Overview paper that describes different musicological analyses that the authors have carried out on digital traces of music (machine-readable music scores, audio recording, other textual information).},
  file = {:pdfs/2017 - The Computational Study of a Musical Culture Through its Digital Traces.pdf:PDF},
  publisher = {International Musicological Society}
}
</pre>

<a name="Shahab2010"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Shahab2010">Shahab2010</a>,
  author = {Shahab, Asif and Shafait, Faisal and Kieninger, Thomas and Dengel, Andreas},
  title = {An Open Approach Towards the Benchmarking of Table Structure Recognition Systems},
  booktitle = {Proceedings of the 9th IAPR International Workshop on Document Analysis Systems},
  year = {2010},
  series = {DAS '10},
  pages = {113--120},
  address = {New York, NY, USA},
  publisher = {ACM},
  acmid = {1815345},
  doi = {10.1145/1815330.1815345},
  file = {:pdfs/2010 - An Open Approach Towards the Benchmarking of Table Structure Recognition Systems.pdf:PDF},
  isbn = {978-1-60558-773-8},
  keywords = {benchmarking, ground truth preparation, image segmentation, performance evaluation, performance measures, table structure recognition},
  location = {Boston, Massachusetts, USA},
  numpages = {8},
  url = {<a href="http://doi.acm.org/10.1145/1815330.1815345">http://doi.acm.org/10.1145/1815330.1815345</a>}
}
</pre>

<a name="Siagian2007"></a><pre>
@article{<a href="docs\OMR-Related.html#Siagian2007">Siagian2007</a>,
  author = {C. Siagian and L. Itti},
  title = {Rapid Biologically-Inspired Scene Classification Using Features Shared with Visual Attention},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2007},
  volume = {29},
  number = {2},
  pages = {300--312},
  issn = {0162-8828},
  abstract = {We describe and validate a simple context-based scene recognition
	algorithm for mobile robotics applications. The system can differentiate
	outdoor scenes from various sites on a college campus using a multiscale
	set of early-visual features, which capture the "gist" of the scene
	into a low-dimensional signature vector. Distinct from previous approaches,
	the algorithm presents the advantage of being biologically plausible
	and of having low-computational complexity, sharing its low-level
	features with a model for visual attention that may operate concurrently
	on a robot. We compare classification accuracy using scenes filmed
	at three outdoor sites on campus (13,965 to 34,711 frames per site).
	Dividing each site into nine segments, we obtain segment classification
	rates between 84.21 percent and 88.62 percent. Combining scenes from
	all sites (75,073 frames in total) yields 86.45 percent correct classification,
	demonstrating the generalization and scalability of the approach},
  doi = {10.1109/TPAMI.2007.40},
  file = {:pdfs/2007 - Rapid Biologically Inspired Scene Classification Using Features Shared with Visual Attention.pdf:PDF},
  keywords = {computational complexity;image recognition;mobile robots;robot vision;biologically-inspired scene classification;college campus;computational complexity;context-based scene recognition;mobile robotics;outdoor scenes;Computer vision;Educational institutions;Image segmentation;Layout;Mobile robots;Object recognition;Robot sensing systems;Robot vision systems;Robustness;Sonar navigation;Gist of a scene;computational neuroscience;image classification;image statistics;robot localization.;robot vision;saliency;scene recognition;Algorithms;Artificial Intelligence;Attention;Biomimetics;Computer Systems;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Information Storage and Retrieval;Pattern Recognition, Automated;Robotics;Visual Perception}
}
</pre>

<a name="Sigtia2015"></a><pre>
@article{<a href="docs\OMR-Related.html#Sigtia2015">Sigtia2015</a>,
  author = {{S}iddharth {S}igtia and {E}mmanouil {B}enetos and {S}imon {D}ixon},
  title = {{A}n {E}nd-to-{E}nd {N}eural {N}etwork for {P}olyphonic {P}iano {M}usic {T}ranscription},
  journal = {{Computing Research Repository}},
  year = {2015},
  volume = {abs/1508.01774},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {<a href="http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1508-01774">http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1508-01774</a>},
  file = {:pdfs/2015 - An End to End Neural Network for Polyphonic Piano Music Transcription.pdf:PDF},
  url = {<a href="http://arxiv.org/abs/1508.01774">http://arxiv.org/abs/1508.01774</a>}
}
</pre>

<a name="Simard2003"></a><pre>
@article{<a href="docs\OMR-Related.html#Simard2003">Simard2003</a>,
  author = {{S}imard, {P} {Y} and {S}teinkraus, {D} and {P}latt, {J}ohn {C}},
  title = {{Best practices for convolutional neural networks applied to visual document analysis}},
  journal = {Document Analysis and Recognition, 2003. Proceedings. Seventh International Conference on},
  year = {2003},
  pages = {958--963},
  abstract = {{N}ot {A}vailable},
  doi = {10.1109/ICDAR.2003.1227801},
  file = {:pdfs/2003 - Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis.pdf:PDF},
  isbn = {0-7695-1960-1},
  keywords = {Best practices,CNN,Concrete,Convolution,Handwriting recognition,Industrial training,Information processing,Neural networks,Performance analysis,Support vector machines,Text analysis}
}
</pre>

<a name="Simonyan2014"></a><pre>
@article{<a href="docs\OMR-Related.html#Simonyan2014">Simonyan2014</a>,
  author = {{K}aren {S}imonyan and {A}ndrew {Z}isserman},
  title = {{V}ery {D}eep {C}onvolutional {N}etworks for {L}arge-{S}cale {I}mage {R}ecognition},
  journal = {{Computing Research Repository}},
  year = {2014},
  volume = {abs/1409.1556},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {<a href="http://dblp.uni-trier.de/rec/bib/journals/corr/SimonyanZ14a">http://dblp.uni-trier.de/rec/bib/journals/corr/SimonyanZ14a</a>},
  file = {:pdfs/2014 - Very Deep Convolutional Networks for Large Scale Image Recognition.pdf:PDF},
  url = {<a href="http://arxiv.org/abs/1409.1556">http://arxiv.org/abs/1409.1556</a>}
}
</pre>

<a name="Sloboda2005"></a><pre>
@book{<a href="docs\OMR-Related.html#Sloboda2005">Sloboda2005</a>,
  title = {Exploring the musical mind},
  publisher = {Oxford University Press},
  year = {2005},
  author = {Sloboda, John},
  comment = {https://books.google.at/books?hl=en&lr=&id=xNK6LTIW3D8C&oi=fnd&pg=PR27&dq=Exploring+the+Musical+Mind:+Cognition,+emotion,+ability,+function&ots=1PPm6DlcYJ&sig=-aROR4P0enmLMIJ4xkZD8y-HJPg\#v=onepage&q&f=true},
  doi = {10.1093/acprof:oso/9780198530121.001.0001},
  journal = {Cognition, emotion, ability, function},
  pages = {102--104},
  url = {https://books.google.at/books?id=xNK6LTIW3D8C&printsec=frontcover#v=onepage&q&f=false}
}
</pre>

<a name="Socher2011"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Socher2011">Socher2011</a>,
  author = {{R}ichard {S}ocher and {C}liff {C}hiung-{Y}u {L}in and {A}ndrew {Y}. {N}g and {C}hristopher {D}. {M}anning},
  title = {{P}arsing {N}atural {S}cenes and {N}atural {L}anguage with {R}ecursive {N}eural {N}etworks},
  booktitle = {28th International Conference on Machine Learning},
  year = {2011},
  file = {:pdfs/2011 - Parsing Natural Scenes and Natural Language with Recursive Neural Networks.pdf:PDF},
  groups = {recognition},
  url = {<a href="http://ai.stanford.edu/~ang/papers/icml11-ParsingWithRecursiveNeuralNetworks.pdf">http://ai.stanford.edu/~ang/papers/icml11-ParsingWithRecursiveNeuralNetworks.pdf</a>}
}
</pre>

<a name="Sordo2015"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Sordo2015">Sordo2015</a>,
  author = {{S}ordo, {M}ohamed and {O}gihara, {M}itsunori and {W}uchty, {S}tefan},
  title = {Analysis of the Evolution of Research Groups and Topics in the {ISMIR} Conference},
  booktitle = {16th International Society for Music Information Retrieval Conference},
  year = {2015},
  volume = {205},
  file = {:pdfs/2015 - Analysis of the Evolution of Research Groups and Topics in the ISMIR Conference.pdf:PDF},
  isbn = {978-84-606-8853-2}
}
</pre>

<a name="Stadelmann2018"></a><pre>
@article{<a href="docs\OMR-Related.html#Stadelmann2018">Stadelmann2018</a>,
  author = {{Stadelmann}, T. and {Amirian}, M. and {Arabaci}, I. and {Arnold}, M. and {Fran{\c c}ois Duivesteijn}, G. and {Elezi}, I. and {Geiger}, M. and {L{\"o}rwald}, S. and {Meier}, B.~B. and {Rombach}, K. and {Tuggener}, L.},
  title = {{Deep Learning in the Wild}},
  journal = {ArXiv e-prints},
  year = {2018},
  file = {:pdfs/2018 - Deep Learning in the Wild.pdf:PDF},
  keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
  url = {https://arxiv.org/abs/1807.04950}
}
</pre>

<a name="Stamatopoulos2013"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Stamatopoulos2013">Stamatopoulos2013</a>,
  author = {{S}tamatopoulos, {N}ikolaos and {G}atos, {B}asilis and {L}ouloudis, {G}eorgios and {P}al, {U}mpada and {A}laei, {A}lireza},
  title = {{ICDAR}2013 {H}andwriting {S}egmentation {C}ontest},
  booktitle = {2013 12th International Conference on Document Analysis and Recognition},
  year = {2013},
  doi = {10.1109/ICDAR.2013.283},
  file = {:pdfs/2013 - ICDAR2013 Handwriting Segmentation Contest.pdf:PDF}
}
</pre>

<a name="Stauffer2018"></a><pre>
@article{<a href="docs\OMR-Related.html#Stauffer2018">Stauffer2018</a>,
  author = {Michael Stauffer and Andreas Fischer and Kaspar Riesen},
  title = {Keyword spotting in historical handwritten documents based on graph matching},
  journal = {Pattern Recognition},
  year = {2018},
  volume = {81},
  pages = {240--253},
  issn = {0031-3203},
  abstract = {In the last decades historical handwritten documents have become increasingly available in digital form. Yet, the accessibility to these documents with respect to browsing and searching remained limited as full automatic transcription is often not possible or not sufficiently accurate. This paper proposes a novel reliable approach for template-based keyword spotting in historical handwritten documents. In particular, our framework makes use of different graph representations for segmented word images and a sophisticated matching procedure. Moreover, we extend our method to a spotting ensemble. In an exhaustive experimental evaluation on four widely used benchmark datasets we show that the proposed approach is able to keep up or even outperform several state-of-the-art methods for template- and learning-based keyword spotting.},
  doi = {https://doi.org/10.1016/j.patcog.2018.04.001},
  file = {:pdfs/2018 - Keyword Spotting in Historical Handwritten Documents Based on Graph Matching.pdf:PDF},
  keywords = {Handwritten keyword spotting, Graph representation, Bipartite graph matching, Ensemble methods},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0031320318301274">http://www.sciencedirect.com/science/article/pii/S0031320318301274</a>}
}
</pre>

<a name="Stone1996"></a><pre>
@book{<a href="docs\OMR-Related.html#Stone1996">Stone1996</a>,
  title = {{Music Notation in the Twentieth Century, A Practical Guidebook}},
  publisher = {W. W. Norton \& Company},
  year = {1996},
  author = {{S}tone, {K}urt},
  file = {:pdfs/1996 - Music Notation in the Twentieth Century, a Practical Guidebook.pdf:PDF},
  keywords = {music notation}
}
</pre>

<a name="Su2013"></a><pre>
@article{<a href="docs\OMR-Related.html#Su2013">Su2013</a>,
  author = {{S}u, {B}olan and {L}u, {S}hijian and {T}an, {C}hew {L}im},
  title = {{Robust document image binarization technique for degraded document images}},
  journal = {IEEE Transactions on Image Processing},
  year = {2013},
  volume = {22},
  number = {4},
  pages = {1408--1417},
  issn = {1057-7149},
  abstract = {{S}egmentation of text from badly degraded document images is a very challenging task due to the high inter/intra-variation between the document background and the foreground text of different document images. {I}n this paper, we propose a novel document image binarization technique that addresses these issues by using adaptive image contrast. {T}he adaptive image contrast is a combination of the local image contrast and the local image gradient that is tolerant to text and background variation caused by different types of document degradations. {I}n the proposed technique, an adaptive contrast map is first constructed for an input degraded document image. {T}he contrast map is then binarized and combined with {C}anny's edge map to identify the text stroke edge pixels. {T}he document text is further segmented by a local threshold that is estimated based on the intensities of detected text stroke edge pixels within a local window. {T}he proposed method is simple, robust, and involves minimum parameter tuning. {I}t has been tested on three public datasets that are used in the recent document image binarization contest ({DIBCO}) 2009 {\&} 2011 and handwritten-{DIBCO} 2010 and achieves accuracies of 93.5{\%}, 87.8{\%}, and 92.03{\%}, respectively, that are significantly higher than or close to that of the best-performing methods reported in the three contests. {E}xperiments on the {B}ickley diary dataset that consists of several challenging bad quality document images also show the superior performance of our proposed method, compared with other techniques.},
  doi = {10.1109/TIP.2012.2231089},
  file = {:pdfs/2013 - Robust Document Image Binarization Technique for Degraded Document Images.pdf:PDF},
  keywords = {Adaptive image contrast,binarization,degraded document image binarization,document analysis,document image processing,pixel classification},
  pmid = {23221822}
}
</pre>

<a name="Szegedy2017"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Szegedy2017">Szegedy2017</a>,
  author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A},
  title = {Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning},
  booktitle = {Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)},
  year = {2017},
  file = {:pdfs/2017 - Inception V4, Inception ResNet and the Impact of Residual Connections on Learning.pdf:PDF}
}
</pre>

<a name="Toiviainen2016"></a><pre>
@misc{<a href="docs\OMR-Related.html#Toiviainen2016">Toiviainen2016</a>,
  author = {Toiviainen, P. and Eerola, T.},
  title = {{MIDI} toolbox 1.1},
  howpublished = {https://github.com/miditoolbox/},
  year = {2016},
  date-modified = {2016-05-18 09:15:01 +0000},
  journal = {GitHub repository},
  publisher = {GitHub},
  url = {https://github.com/miditoolbox/}
}
</pre>

<a name="Trier1995"></a><pre>
@article{<a href="docs\OMR-Related.html#Trier1995">Trier1995</a>,
  author = {O. D. Trier and A. K. Jain},
  title = {Goal-directed evaluation of binarization methods},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {1995},
  volume = {17},
  number = {12},
  pages = {1191--1201},
  month = {Dec},
  issn = {0162-8828},
  abstract = {This paper presents a methodology for evaluation of low-level image analysis methods, using binarization (two-level thresholding) as an example. Binarization of scanned gray scale images is the first step in most document image analysis systems. Selection of an appropriate binarization method for an input image domain is a difficult problem. Typically, a human expert evaluates the binarized images according to his/her visual criteria. However, to conduct an objective evaluation, one needs to investigate how well the subsequent image analysis steps will perform on the binarized image. We call this approach goal-directed evaluation, and it can be used to evaluate other low-level image processing methods as well. Our evaluation of binarization methods is in the context of digit recognition, so we define the performance of the character recognition module as the objective measure. Eleven different locally adaptive binarization methods were evaluated, and Niblack's method gave the best performance.},
  doi = {10.1109/34.476511},
  file = {:pdfs/1995 - Goal Directed Evaluation of Binarization Methods.pdf:PDF},
  keywords = {document image processing;document handling;character recognition;image segmentation;performance evaluation;goal-directed evaluation;binarization;low-level image analysis;two-level thresholding;scanned gray scale images;document image processing;digit image recognition;character recognition module;Niblack's method;segmentation;Image analysis;Text analysis;Image processing;Humans;Performance evaluation;Character recognition;Image segmentation;Machine vision;Computer vision;Data mining}
}
</pre>

<a name="Trier1995a"></a><pre>
@article{<a href="docs\OMR-Related.html#Trier1995a">Trier1995a</a>,
  author = {{T}rier, {O}.{D}. and {T}axt, {T}.},
  title = {{Evaluation of binarization methods for utility map images}},
  journal = {Proceedings of 1st International Conference on Image Processing},
  year = {1995},
  volume = {2},
  pages = {31--36},
  issn = {0162-8828},
  abstract = {{T}his paper presents an evaluation of locally adaptive binarization methods for gray scale images with low contrast, variable background intensity and noise. {S}uch low quality occurs frequently in utility maps and excludes the use of global binarization methods. {O}nly robust locally adaptive binarization methods with no need for on-line tuning of the parameters were considered since the gray scale images of utility maps often consist of a billion (10<sup>9</sup>) pixels or more. {E}ight locally adaptive binarization methods were tested on five different images. {T}he postprocessing step ({PS}) of {Y}anowitz and {B}ruckstein's (1989) method improved all the other best binarization methods. {N}iblack's (1986) method with {PS} gave the best performance. {E}ikvil, {T}axt and {M}oen's (1991) method with {PS}, and {Y}anowitz and {B}ruckstein's method did almost as well. {C}omparison was also made on the {CPU} requirement},
  doi = {10.1109/ICIP.1994.413515},
  file = {:pdfs/1995 - Evaluation of Binarization Methods for Utility Map Images.pdf:PDF},
  isbn = {0-8186-6952-7},
  keywords = {binarization,document images,evaluation,locally adaptive binarization,thresholding,utility maps}
}
</pre>

<a name="Uijlings2013"></a><pre>
@article{<a href="docs\OMR-Related.html#Uijlings2013">Uijlings2013</a>,
  author = {Uijlings, J. R. R. and van de Sande, K. E. A. and Gevers, T. and Smeulders, A. W. M.},
  title = {Selective Search for Object Recognition},
  journal = {International Journal of Computer Vision},
  year = {2013},
  volume = {104},
  number = {2},
  pages = {154--171},
  issn = {1573-1405},
  abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99Â {\%} recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software:  http://disi.unitn.it/~uijlings/SelectiveSearch.html ).},
  day = {01},
  doi = {10.1007/s11263-013-0620-5},
  file = {:pdfs/2013 - Selective Search for Object Recognition.pdf:PDF},
  url = {https://doi.org/10.1007/s11263-013-0620-5}
}
</pre>

<a name="Urbano2013"></a><pre>
@techreport{<a href="docs\OMR-Related.html#Urbano2013">Urbano2013</a>,
  author = {Juli\'{a}n Urbano},
  title = {{MIREX 2013 Symbolic Melodic Similarity: A Geometric Model supported with Hybrid Sequence Alignment}},
  institution = {Music Information Retrieval Evaluation eXchange},
  year = {2013},
  file = {:pdfs/2013 - MIREX 2013 Symbolic Melodic Similarity - A Geometric Model supported with Hybrid Sequence Alignment.pdf:PDF}
}
</pre>

<a name="Vajda2015"></a><pre>
@article{<a href="docs\OMR-Related.html#Vajda2015">Vajda2015</a>,
  author = {Szil{\'{a}}rd Vajda and Yves Rangoni and Hubert Cecotti},
  title = {Semi-automatic ground truth generation using unsupervised clustering and limited manual labeling: Application to handwritten character recognition},
  journal = {Pattern Recognition Letters},
  year = {2015},
  volume = {58},
  pages = {23--28},
  doi = {10.1016/j.patrec.2015.02.001},
  file = {:pdfs/2015 - Semi-automatic ground truth generation using unsupervised clustering and limited manual labeling - Application to handwritten character recognition.pdf:PDF},
  publisher = {Elsevier {BV}}
}
</pre>

<a name="Villegas2015"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Villegas2015">Villegas2015</a>,
  author = {M. Villegas and J. A. S{\'{a}}nchez and E. Vidal},
  title = {Optical modelling and language modelling trade-off for Handwritten Text Recognition},
  booktitle = {2015 13th International Conference on Document Analysis and Recognition (ICDAR)},
  year = {2015},
  pages = {831--835},
  abstract = {Training the models needed for Automatic Handwritten Text Recognition of historical documents generally requires a significant amount of human effort. This is mainly due to the great differences that often exist between collections and to the lack of linguistic resources from the period when the documents were written, which results in a need of manual data labelling effort. This paper presents a study on the reuse of models trained with data from a different collection, focusing on the contribution that the language model and the optical models have on the performance. An empirical evaluation is performed using data from Jeremy Bentham manuscripts with the aim of recognising a manuscript about a very different topic written by Jane Austen.},
  doi = {10.1109/ICDAR.2015.7333878},
  file = {:pdfs/2015 - Optical Modelling and Language Modelling Trade-off for Handwritten Text Recognition.pdf:PDF},
  keywords = {document image processing;history;linguistics;optical character recognition;text detection;Bentham Jeremy manuscript;handwritten text recognition;language modelling;linguistic resource;manual data labelling effort;optical modelling;Adaptation models;Integrated circuits;Integrated optics;Handwritten Text Recognition;Language Models;Model Retraining;Optical Models}
}
</pre>

<a name="Vonikakis2011"></a><pre>
@article{<a href="docs\OMR-Related.html#Vonikakis2011">Vonikakis2011</a>,
  author = {{V}onikakis, {V}. and {A}ndreadis, {I}. and {P}apamarkos, {N}.},
  title = {{Robust document binarization with OFF center-surround cells}},
  journal = {Pattern Analysis and Applications},
  year = {2011},
  volume = {14},
  number = {3},
  pages = {219--234},
  issn = {1433-7541},
  abstract = {{T}his paper presents a new method for degraded-document binarization, inspired by the attributes of the {H}uman {V}isual {S}ystem ({HVS}). {I}t can deal with various types of degradations, such as uneven illumination, shadows, low contrast, smears, and heavy noise densities. {T}he proposed algorithm combines the characteristics of the {OFF} center-surround cells of the {HVS} with the classic {O}tsu binarization technique. {C}ells of two different scales are combined, increasing the efficiency of the algorithm and reducing the extracted noise in the final output. {A} new response function, which regulates the output of the cell according to the local contrast and the local lighting conditions is also introduced. {T}he {O}tsu technique is used to binarize the outputs of the {OFF} center-surround cells. {Q}uantitative experiments performed on a set of various computer-generated degradations, such as noise, shadow, and low contrast demonstrate the superior performance of the proposed method against six other well-established techniques. {Q}ualitative and {OCR} comparisons also confirm these results.},
  doi = {10.1007/s10044-011-0214-1},
  file = {:pdfs/2011 - Robust Document Binarization with Off Center Surround Cells.pdf:PDF},
  isbn = {3025410795},
  keywords = {Document binarization,Human visual system,OFF center-surround cells,Thresholding,binarization}
}
</pre>

<a name="Vos1989"></a><pre>
@article{<a href="docs\OMR-Related.html#Vos1989">Vos1989</a>,
  author = {Piet G. Vos and Jim M. Troost},
  title = {Ascending and Descending Melodic Intervals: Statistical Findings and their Perceptual Relevance},
  journal = {Music Perception},
  year = {1989},
  volume = {6},
  number = {4},
  pages = {383--396},
  doi = {10.2307/40285439}
}
</pre>

<a name="Wauthier2013"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Wauthier2013">Wauthier2013</a>,
  author = {{W}authier, {F}abian and {J}ordan, {M}ichael and {J}ojic, {N}ebojsa},
  title = {{E}fficient ranking from pairwise comparisons},
  booktitle = {30th International Conference on Machine Learning},
  year = {2013},
  pages = {109--117},
  file = {:pdfs/2013 - Efficient Ranking from Pairwise Comparisons.pdf:PDF},
  groups = {pairwise}
}
</pre>

<a name="Wilson2003"></a><pre>
@article{<a href="docs\OMR-Related.html#Wilson2003">Wilson2003</a>,
  author = {D.Randall Wilson and Tony R. Martinez},
  title = {The general inefficiency of batch training for gradient descent learning},
  journal = {Neural Networks},
  year = {2003},
  volume = {16},
  number = {10},
  pages = {1429--1451},
  issn = {0893-6080},
  abstract = {Abstract Gradient descent training of neural networks can be done
	in either a batch or on-line manner. A widely held myth in the neural
	network community is that batch training is as fast or faster and/or
	more â€˜correctâ€™ than on-line training because it supposedly uses a
	better approximation of the true gradient for its weight updates.
	This paper explains why batch training is almost always slower than
	on-line trainingâ€”often orders of magnitude slowerâ€”especially on large
	training sets. The main reason is due to the ability of on-line training
	to follow curves in the error surface throughout each epoch, which
	allows it to safely use a larger learning rate and thus converge
	with less iterations through the training data. Empirical results
	on a large (20,000-instance) speech recognition task and on 26 other
	learning tasks demonstrate that convergence can be reached significantly
	faster using on-line training than batch training, with no apparent
	difference in accuracy.},
  doi = {10.1016/S0893-6080(03)00138-2},
  file = {:pdfs/2003 - The General Inefficiency of Batch Training for Gradient Descent Learning.pdf:PDF},
  keywords = {Batch training},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0893608003001382">http://www.sciencedirect.com/science/article/pii/S0893608003001382</a>}
}
</pre>

<a name="Wittlich1978"></a><pre>
@article{<a href="docs\OMR-Related.html#Wittlich1978">Wittlich1978</a>,
  author = {Wittlich, Gary and Byrd, Donald and Nerheim, Rosalee},
  title = {A system for interactive encoding of music scores under computer control},
  journal = {Computers and the Humanities},
  year = {1978},
  volume = {12},
  number = {4},
  pages = {309--319},
  month = {Dec},
  issn = {1572-8412},
  day = {01},
  doi = {10.1007/BF02400103},
  url = {https://doi.org/10.1007/BF02400103}
}
</pre>

<a name="Xia2017"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Xia2017">Xia2017</a>,
  author = {Xia, Gus G. and Dannenberg, Roger B.},
  title = {Improvised Duet Interaction: Learning Improvisation Techniques for Automatic Accompaniment},
  booktitle = {New Interfaces for Musical Expression},
  year = {2017},
  editor = {Erkut, Cumhur},
  address = {Aalborg University Copenhagen, Denmark},
  file = {:pdfs/2017 - Improvised Duet Interaction_ Learning Improvisation Techniques for Automatic Accompaniment.pdf:PDF},
  url = {<a href="http://homes.create.aau.dk/dano/nime17/papers/0022/paper0022.pdf">http://homes.create.aau.dk/dano/nime17/papers/0022/paper0022.pdf</a>}
}
</pre>

<a name="Zhai2010"></a><pre>
@phdthesis{<a href="docs\OMR-Related.html#Zhai2010">Zhai2010</a>,
  author = {{Y}un {Z}hai},
  title = {{N}on-{N}umerical {R}anking {B}ased on {P}airwise {C}omparisons},
  school = {McMaster University},
  year = {2010},
  address = {Hamilton, Ontario},
  file = {:pdfs/2010 - Non Numerical Ranking Based on Pairwise Comparisons.pdf:PDF},
  url = {<a href="http://hdl.handle.net/11375/19476">http://hdl.handle.net/11375/19476</a>}
}
</pre>

<a name="Zhai2012"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Zhai2012">Zhai2012</a>,
  author = {{K}e {Z}hai and {Y}uening {H}u and {J}ordan {L}. {B}oyd{-}{G}raber and {S}inead {W}illiamson},
  title = {{M}odeling {I}mages using {T}ransformed {I}ndian {B}uffet {P}rocesses},
  booktitle = {29th International Conference on Machine Learning},
  year = {2012},
  publisher = {icml.cc / Omnipress},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {<a href="http://dblp.uni-trier.de/rec/bib/conf/icml/ZhaiHBW12">http://dblp.uni-trier.de/rec/bib/conf/icml/ZhaiHBW12</a>},
  file = {:pdfs/2012 - Modeling Images Using Transformed Indian Buffet Processes.pdf:PDF},
  url = {<a href="http://icml.cc/2012/papers/750.pdf">http://icml.cc/2012/papers/750.pdf</a>}
}
</pre>

<a name="Zhang2016"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Zhang2016">Zhang2016</a>,
  author = {{Z}hang, {X}ingxing and {L}u, {L}iang and {L}apata, {M}irella},
  title = {{T}op-down {T}ree {L}ong {S}hort-{T}erm {M}emory {N}etworks},
  booktitle = {2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  year = {2016},
  pages = {310--320},
  address = {San Diego, California},
  publisher = {Association for Computational Linguistics},
  file = {:pdfs/2016 - Top down Tree Long Short Term Memory Networks.pdf:PDF},
  groups = {rnn},
  url = {<a href="http://www.aclweb.org/anthology/N16-1035">http://www.aclweb.org/anthology/N16-1035</a>}
}
</pre>

<a name="Zhang2017"></a><pre>
@article{<a href="docs\OMR-Related.html#Zhang2017">Zhang2017</a>,
  author = {{J}ianshu {Z}hang and {J}un {D}u and {S}hiliang {Z}hang and {D}an {L}iu and {Y}ulong {H}u and {J}inshui {H}u and {S}i {W}ei and {L}irong {D}ai},
  title = {{W}atch, {A}ttend and {P}arse: {A}n {E}nd-to-end {N}eural {N}etwork {B}ased {A}pproach to {H}andwritten {M}athematical {E}xpression {R}ecognition},
  journal = {Pattern Recognition},
  year = {2017},
  issn = {0031-3203},
  abstract = {{A}bstract {M}achine recognition of a handwritten mathematical expression ({HME}) is challenging due to the ambiguities of handwritten symbols and the two-dimensional structure of mathematical expressions. {I}nspired by recent work in deep learning, we present {W}atch, {A}ttend and {P}arse ({WAP}), a novel end-to-end approach based on neural network that learns to recognize \{HMEs\} in a two-dimensional layout and outputs them as one-dimensional character sequences in {L}a{T}e{X} format. {I}nherently unlike traditional methods, our proposed model avoids problems that stem from symbol segmentation, and it does not require a predefined expression grammar. {M}eanwhile, the problems of symbol recognition and structural analysis are handled, respectively, using a watcher and a parser. {W}e employ a convolutional neural network encoder that takes \{HME\} images as input as the watcher and employ a recurrent neural network decoder equipped with an attention mechanism as the parser to generate {L}a{T}e{X} sequences. {M}oreover, the correspondence between the input expressions and the output {L}a{T}e{X} sequences is learned automatically by the attention mechanism. {W}e validate the proposed approach on a benchmark published by the \{CROHME\} international competition. {U}sing the official training dataset, \{WAP\} significantly outperformed the state-of-the-art method with an expression recognition accuracy of 46.55% on \{CROHME\} 2014 and 44.55% on \{CROHME\} 2016. },
  doi = {10.1016/j.patcog.2017.06.017},
  file = {:pdfs/2017 - Watch, Attend and Parse - an End to End Neural Network Based Approach to Handwritten Mathematical Expression Recognition.pdf:PDF},
  keywords = {Handwritten Mathematical Expression Recognition},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0031320317302376">http://www.sciencedirect.com/science/article/pii/S0031320317302376</a>}
}
</pre>

<a name="Zitnick2014"></a><pre>
@inproceedings{<a href="docs\OMR-Related.html#Zitnick2014">Zitnick2014</a>,
  author = {Zitnick, Larry and Dollar, Piotr},
  title = {Edge Boxes: Locating Object Proposals from Edges},
  booktitle = {ECCV},
  year = {2014},
  publisher = {European Conference on Computer Vision},
  abstract = {The use of object proposals is an effective recent approach for increasing
	the computational efficiency of object detection. We propose a novel
	method for generating object bounding box proposals using edges.
	Edges provide a sparse yet informative representation of an image.
	Our main observation is that the number of contours that are wholly
	contained in a bounding box is indicative of the likelihood of the
	box containing an object. We propose a simple box objectness score
	that measures the number of edges that exist in the box minus those
	that are members of contours that overlap the box's boundary. Using
	efficient data structures, millions of candidate boxes can be evaluated
	in a fraction of a second, returning a ranked set of a few thousand
	top-scoring proposals. Using standard metrics, we show results that
	are significantly more accurate than the current state-of-the-art
	while being faster to compute. In particular, given just 1000 proposals
	we achieve over 96% object recall at overlap threshold of 0.5 and
	over 75% recall at the more challenging overlap of 0.7. Our approach
	runs in 0.25 seconds and we additionally demonstrate a near real-time
	variant with only minor loss in accuracy.},
  file = {:pdfs/2014 - Edge Boxes - Locating Object Proposals from Edges.pdf:PDF},
  url = {https://www.microsoft.com/en-us/research/publication/edge-boxes-locating-object-proposals-from-edges/}
}
</pre>

<pre>
@comment{{jabref-meta: databaseType:bibtex;}}
</pre>

<pre>
@comment{{jabref-meta: saveActions:enabled;
date[normalize_date]
editor[unicode_to_latex]
pages[normalize_page_numbers]
journal[unicode_to_latex]
author[unicode_to_latex]
all-text-fields[identity]
title[html_to_latex,unicode_to_latex]
booktitle[unicode_to_latex]
;}}
</pre>

<pre>
@comment{{jabref-meta: saveOrderConfig:specified;bibtexkey;false;;false;;false;}}
</pre>

<hr><p><em>This file was generated by
<a href="http://www.lri.fr/~filliatr/bibtex2html/">bibtex2html</a> 1.98.</em></p>
