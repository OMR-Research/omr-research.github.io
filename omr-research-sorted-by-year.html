<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>
<head>
	<title>OMR-Research</title>
	<link rel=stylesheet type="text/css" href="css/OMR-Research.css">
</head>
<body>

<section class="page-header">
	<h1>Bibliography on Optical Music Recognition</h1>
	<p>Last updated: 10.12.2018</p>
	<a href="https://github.com/OMR-Research/omr-research.github.io" class="btn">View on GitHub</a>		
	<table class="page-header-table" id="navigation-table">
		<tr>
			<td><a href="index.html" class="btn-light">Sort by Key</a></td>		
			<td><a href="omr-research-sorted-by-year.html" class="btn-light">Sort by Year</a>	</td>			
			<td><a href="omr-research-compact.html" class="btn-light">Compact</a></td>		
			<td><a href="omr-related-research.html" class="btn-light">Related research</a></td>
			<td><a href="omr-research-unverified.html" class="btn-light">Unverified research</a></td>
		</tr>
	</table>		
</section>

<!-- This document was automatically generated with bibtex2html 1.98
     (see http://www.lri.fr/~filliatr/bibtex2html/),
     with the following command:
     BibTeX2HTML\Windows\bibtex2html.exe --use-keys --no-keywords --nodoc -d -o OMR-Research-Year .\OMR-Research.bib  -->


<table>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="RISM">RISM</a>]
</td>
<td class="bibtexitem">
R&eacute;pertoire international des sources musicales, 1952.
[&nbsp;<a href="OMR-Research-Year_bib.html#RISM">bib</a>&nbsp;| 
<a href="http://www.rism.info">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Sampson1985">Sampson1985</a>]
</td>
<td class="bibtexitem">
Geoffrey Sampson.
 <em>Writing Systems: A Linguistic Introduction</em>.
 Stanford University Press, 1985.
[&nbsp;<a href="OMR-Research-Year_bib.html#Sampson1985">bib</a>&nbsp;| 
<a href="https://books.google.cz/books?id=tVcdNRvwoDkC">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Rebelo2012a">Rebelo2012a</a>]
</td>
<td class="bibtexitem">
Ana&nbsp;Maria Rebelo.
 <em>Robust Optical Recognition of Handwritten Musical Scores based
  on Domain Knowledge</em>.
 PhD thesis, University of Porto, 2012.
[&nbsp;<a href="OMR-Research-Year_bib.html#Rebelo2012a">bib</a>&nbsp;| 
<a href="http://www.inescporto.pt/~arebelo/arebeloThesis.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Rebelo2012">Rebelo2012</a>]
</td>
<td class="bibtexitem">
Ana Rebelo, Ichiro Fujinaga, Filipe Paszkiewicz, Andre&nbsp;R.S. Marcal, Carlos
  Guedes, and Jaime&nbsp;S. Cardoso.
 Optical music recognition: state-of-the-art and open issues.
 <em>International Journal of Multimedia Information Retrieval</em>,
  1(3):173--190, 2012.
[&nbsp;<a href="OMR-Research-Year_bib.html#Rebelo2012">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s13735-012-0004-6">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Urbano2013">Urbano2013</a>]
</td>
<td class="bibtexitem">
Juli&aacute;n Urbano.
 MIREX 2013 Symbolic Melodic Similarity: A Geometric Model supported
  with Hybrid Sequence Alignment.
 Technical report, Music Information Retrieval Evaluation eXchange,
  2013.
[&nbsp;<a href="OMR-Research-Year_bib.html#Urbano2013">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Silva2013">Silva2013</a>]
</td>
<td class="bibtexitem">
Rui Miguel Filipe&nbsp;da Silva.
 Mobile framework for recognition of musical characters.
 Master's thesis, Universidade do Porto, 2013.
[&nbsp;<a href="OMR-Research-Year_bib.html#Silva2013">bib</a>&nbsp;| 
<a href="https://repositorio-aberto.up.pt/bitstream/10216/68500/2/26777.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Rhodes2016">Rhodes2016</a>]
</td>
<td class="bibtexitem">
Christophe Rhodes, Tim Crawford, and Mark d'Inverno.
 <em>Duplicate Detection in Facsimile Scans of Early Printed Music</em>,
  pages 449--459.
 Springer International Publishing, Cham, 2016.
[&nbsp;<a href="OMR-Research-Year_bib.html#Rhodes2016">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-319-25226-1_38">DOI</a>&nbsp;]
<blockquote><font size="-1">
There is a growing number of collections of readily available scanned
	musical documents, whether generated and managed by libraries, research
	projects, or volunteer efforts. They are typically digital images;
	for computational musicology we also need the musical data in machine-readable
	form. Optical Music Recognition (OMR) can be used on printed music,
	but is prone to error, depending on document condition and the quality
	of intermediate stages in the digitization process such as archival
	photographs. This work addresses the detection of one such error---duplication
	of images---and the discovery of other relationships between images
	in the process.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Wu2017">Wu2017</a>]
</td>
<td class="bibtexitem">
Fu-Hai&nbsp;Frank Wu.
 Applying machine learning in optical music recognition of numbered
  music notation.
 In <em>International Journal of Multimedia Data Engineering and
  Management (IJMDEM)</em>, page&nbsp;21. IGI Global, 2017.
[&nbsp;<a href="OMR-Research-Year_bib.html#Wu2017">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.4018/IJMDEM.2017070102">DOI</a>&nbsp;]
<blockquote><font size="-1">
Although research of optical music recognition (OMR) has existed for
	few decades, most of efforts were put in step of image processing
	to approach upmost accuracy and evaluations were not in common ground.
	And major music notations explored were the conventional western
	music notations with staff. On contrary, the authors explore the
	challenges of numbered music notation, which is popular in Asia and
	used in daily life for sight reading. The authors use different way
	to improve recognition accuracy by applying elementary image processing
	with rough tuning and supplementing with methods of machine learning.
	The major contributions of this work are the architecture of machine
	learning specified for this task, the dataset, and the evaluation
	metrics, which indicate the performance of OMR system, provide objective
	function for machine learning and highlight the challenges of the
	scores of music with the specified notation.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Pacha2017a">Pacha2017a</a>]
</td>
<td class="bibtexitem">
Alexander Pacha and Horst Eidenberger.
 Towards self-learning optical music recognition.
 In <em>2017 16th IEEE International Conference on Machine Learning
  and Applications (ICMLA)</em>, pages 795--800, 2017.
[&nbsp;<a href="OMR-Research-Year_bib.html#Pacha2017a">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICMLA.2017.00-60">DOI</a>&nbsp;]
<blockquote><font size="-1">
Optical Music Recognition (OMR) is a branch of
artificial intelligence that aims at automatically recognizing
and understanding the content of music scores in images.
Several approaches and systems have been proposed that try to
solve this problem by using expert knowledge and specialized
algorithms that tend to fail at generalization to a broader
set of scores, imperfect image scans or data of different
formatting. In this paper we propose a new approach to solve
OMR by investigating how humans read music scores and by
imitating that behavior with machine learning. To demonstrate
the power of this approach, we conduct two experiments
that teach a machine to distinguish entire music sheets from
arbitrary content through frame-by-frame classification and
distinguishing between 32 classes of handwritten music symbols
which can be a basis for object detection. Both tasks can
be performed at high rates of confidence (&gt;98comparable to the performance of humans on the same task.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Pacha2018c">Pacha2018c</a>]
</td>
<td class="bibtexitem">
Alexander Pacha, Jan&nbsp;jr. Haji&#x010D;, and Jorge Calvo-Zaragoza.
 A baseline for general music object detection with deep learning.
 <em>Applied Sciences</em>, 8(9):1488--1508, 2018.
[&nbsp;<a href="OMR-Research-Year_bib.html#Pacha2018c">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.3390/app8091488">DOI</a>&nbsp;| 
<a href="http://www.mdpi.com/2076-3417/8/9/1488">http</a>&nbsp;]
<blockquote><font size="-1">
Deep learning is bringing breakthroughs to many computer vision subfields including Optical Music Recognition (OMR), which has seen a series of improvements to musical symbol detection achieved by using generic deep learning models. However, so far, each such proposal has been based on a specific dataset and different evaluation criteria, which made it difficult to quantify the new deep learning-based state-of-the-art and assess the relative merits of these detection models on music scores. In this paper, a baseline for general detection of musical symbols with deep learning is presented. We consider three datasets of heterogeneous typology but with the same annotation format, three neural models of different nature, and establish their performance in terms of a common evaluation standard. The experimental results confirm that the direct music object detection with deep learning is indeed promising, but at the same time illustrates some of the domain-specific shortcomings of the general detectors. A qualitative comparison then suggests avenues for OMR improvement, based both on properties of the detection model and how the datasets are defined. To the best of our knowledge, this is the first time that competing music object detection systems from the machine learning paradigm are directly compared to each other. We hope that this work will serve as a reference to measure the progress of future developments of OMR in music object detection.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Pacha2018">Pacha2018</a>]
</td>
<td class="bibtexitem">
Alexander Pacha, Kwon-Young Choi, Bertrand Co&uuml;asnon, Yann Ricquebourg,
  Richard Zanibbi, and Horst Eidenberger.
 Handwritten music object detection: Open issues and baseline results.
 In <em>2018 13th IAPR Workshop on Document Analysis Systems (DAS)</em>,
  pages 163--168, 2018.
[&nbsp;<a href="OMR-Research-Year_bib.html#Pacha2018">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/DAS.2018.51">DOI</a>&nbsp;]
<blockquote><font size="-1">
Optical Music Recognition (OMR) is the challenge of understanding the content of musical scores. Accurate detection of individual music objects is a critical step in processing musical documents because a failure at this stage corrupts any further processing. So far, all proposed methods were either limited to typeset music scores or were built to detect only a subset of the available classes of music symbols. In this work, we propose an end-to-end trainable object detector for music symbols that is capable of detecting almost the full vocabulary of modern music notation in handwritten music scores. By training deep convolutional neural networks on the recently released MUSCIMA++ dataset which has symbol-level annotations, we show that a machine learning approach can be used to accurately detect music objects with a mean average precision of over 80%.
</font></blockquote>
<p>
</td>
</tr>
</table><hr><p><em>This file was generated by
<a href="http://www.lri.fr/~filliatr/bibtex2html/">bibtex2html</a> 1.98.</em></p>
</body>
</html>
